\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\bibstyle{abbrvnat}
\citation{dengImageNetLargescaleHierarchical2009}
\citation{devlinBERTPretrainingDeep2019}
\citation{oordWaveNetGenerativeModel2016,lecunDeepLearning2015}
\citation{goodfellowDeepLearning2016}
\citation{zhouObjectDetectorsEmerge2015}
\citation{luoUnderstandingEffectiveReceptive2016}
\citation{kobayashiInterpretationResNetVisualization2020}
\citation{kimDeadPixelTest2023}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {2}Related Work}{1}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Experimental results}{2}{section.3}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{Here I never talked of how I calculated the receptive field, which I used a libary that calculates the gradient projection in a dummy input space with a projected gradient of 1 in the middle of all the fueature maps of the last convolutional layer for the two architectures. }{2}{section*.2}\protected@file@percent }
\pgfsyspdfmark {pgfid1}{20112833}{32124398}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Experimental settings}{2}{subsection.3.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Manipulating the Receptive Field}{2}{subsection.3.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Receptive Field, Accuracy and Pruning}{2}{subsection.3.3}\protected@file@percent }
\newlabel{sec:rfanda}{{3.3}{2}{Receptive Field, Accuracy and Pruning}{subsection.3.3}{}}
\newlabel{sec:rfanda@cref}{{[subsection][3][3]3.3}{[1][2][]2}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \textbf  {CIFAR10 results:} Here are summarised the results for the experiments performed in CIFAR10. It can be seen that the discrepancy in accuracy between different receptive fields is consistent for these two architectures. Also, as we increase the receptive field we can see that the gap in performance between dense and pruned models diminishes. The pruning rate used is 90\%\relax }}{3}{table.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{tab:pruned_cifar10}{{1}{3}{\textbf {CIFAR10 results:} Here are summarised the results for the experiments performed in CIFAR10. It can be seen that the discrepancy in accuracy between different receptive fields is consistent for these two architectures. Also, as we increase the receptive field we can see that the gap in performance between dense and pruned models diminishes. The pruning rate used is 90\%\relax }{table.caption.3}{}}
\newlabel{tab:pruned_cifar10@cref}{{[table][1][]1}{[1][3][]3}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces  \textbf  {Tiny ImageNet Results:} Here are summarised the results for the experiments performed in Tiny ImageNet. Similarly to CIFAR10, the trend of diminishing dense accuracy and gap between dense and pruned accuracy as we increase the receptive field is consistent in the two architectures. The pruning rate is 90\% excluding the first convolutional layer and the linear layer\relax }}{3}{table.caption.4}\protected@file@percent }
\newlabel{tab:tiny_imagenet_results}{{2}{3}{\textbf {Tiny ImageNet Results:} Here are summarised the results for the experiments performed in Tiny ImageNet. Similarly to CIFAR10, the trend of diminishing dense accuracy and gap between dense and pruned accuracy as we increase the receptive field is consistent in the two architectures. The pruning rate is 90\% excluding the first convolutional layer and the linear layer\relax }{table.caption.4}{}}
\newlabel{tab:tiny_imagenet_results@cref}{{[table][2][]2}{[1][3][]3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Fine-tuning Pruned Solutions}{3}{subsection.3.4}\protected@file@percent }
\newlabel{subsec:Fine_tuning_solutions}{{3.4}{3}{Fine-tuning Pruned Solutions}{subsection.3.4}{}}
\newlabel{subsec:Fine_tuning_solutions@cref}{{[subsection][4][3]3.4}{[1][3][]3}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces VGG and ResNet50 on CIFAR10 with fine tuning and pruning rate 0.9\relax }}{4}{table.caption.5}\protected@file@percent }
\newlabel{tab:cifar10 fine tuning pruning rate 09}{{3}{4}{VGG and ResNet50 on CIFAR10 with fine tuning and pruning rate 0.9\relax }{table.caption.5}{}}
\newlabel{tab:cifar10 fine tuning pruning rate 09@cref}{{[table][3][]3}{[1][3][]4}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces VGG and ReseNet50 on Tiny ImageNet with fine tuning and pruning rate of 0.9\relax }}{4}{table.caption.6}\protected@file@percent }
\newlabel{tab:cifar10 fine tuning pruning rate 09}{{4}{4}{VGG and ReseNet50 on Tiny ImageNet with fine tuning and pruning rate of 0.9\relax }{table.caption.6}{}}
\newlabel{tab:cifar10 fine tuning pruning rate 09@cref}{{[table][4][]4}{[1][4][]4}}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces VGG and ReseNet50 on Tiny ImageNet with fine tuning for 100 epochs and pruning rate of 0.9\relax }}{4}{table.caption.7}\protected@file@percent }
\newlabel{tab:tiny imagenet fine tuning pruning rate 09}{{5}{4}{VGG and ReseNet50 on Tiny ImageNet with fine tuning for 100 epochs and pruning rate of 0.9\relax }{table.caption.7}{}}
\newlabel{tab:tiny imagenet fine tuning pruning rate 09@cref}{{[table][5][]5}{[1][4][]4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Loss Landscape and Receptive Field}{5}{subsection.3.5}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{Talk about the training trajectory in \Cref  {fig:training_trajectories}, and mention that the figures in \Cref  {fig:hessian_vgg_cifar10,fig:hessian_resnet50_cifar10} explain such behaviour. In \Cref  {fig:images-resnet50_CIFAR10_training-png} I trained for double the epochs trying to show that even with more training the larger receptive field cannot match the smaller receptive field. The bump seen arround 200 epochs is due tu the learning rate schedule, which is a cosine schedule with Tmax=200, right now im training a model with Tmax=400 to see if it that claim is still valid. All the accuracies seen throughtout the document correspond to the best model while trainig e.g. the bump}{5}{section*.8}\protected@file@percent }
\pgfsyspdfmark {pgfid2}{20112833}{36467236}
\newlabel{subfig:Hessian_VGG_before_training}{{1a}{5}{Before Training\relax }{figure.caption.9}{}}
\newlabel{subfig:Hessian_VGG_before_training@cref}{{[subfigure][1][1]1a}{[1][5][]5}}
\newlabel{sub@subfig:Hessian_VGG_before_training}{{a}{5}{Before Training\relax }{figure.caption.9}{}}
\newlabel{sub@subfig:Hessian_VGG_before_training@cref}{{[subfigure][1][1]1a}{[1][5][]5}}
\newlabel{subfig:Hessian_VGG_after_training}{{1b}{5}{After training\relax }{figure.caption.9}{}}
\newlabel{subfig:Hessian_VGG_after_training@cref}{{[subfigure][2][1]1b}{[1][5][]5}}
\newlabel{sub@subfig:Hessian_VGG_after_training}{{b}{5}{After training\relax }{figure.caption.9}{}}
\newlabel{sub@subfig:Hessian_VGG_after_training@cref}{{[subfigure][2][1]1b}{[1][5][]5}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Largest 90 eigen values of VGG model on CIFAR10 for different Receptive Fields \todo [inline]{Here I can put labels like $\lambda $ and $P(\lambda )$. Larger font} \relax }}{5}{figure.caption.9}\protected@file@percent }
\@writefile{tdo}{\contentsline {todo}{Here I can put labels like $\lambda $ and $P(\lambda )$. Larger font}{5}{section*.11}\protected@file@percent }
\pgfsyspdfmark {pgfid4}{20112833}{20974502}
\newlabel{fig:hessian_vgg_cifar10}{{1}{5}{Largest 90 eigen values of VGG model on CIFAR10 for different Receptive Fields \todo [inline]{Here I can put labels like $\lambda $ and $P(\lambda )$. Larger font} \relax }{section*.11}{}}
\newlabel{fig:hessian_vgg_cifar10@cref}{{[figure][1][]1}{[1][5][]5}}
\newlabel{subfig:}{{2a}{5}{Before Training\relax }{figure.caption.12}{}}
\newlabel{subfig:@cref}{{[subfigure][1][2]2a}{[1][5][]5}}
\newlabel{sub@subfig:}{{a}{5}{Before Training\relax }{figure.caption.12}{}}
\newlabel{sub@subfig:@cref}{{[subfigure][1][2]2a}{[1][5][]5}}
\newlabel{subfig:}{{2b}{5}{After training\relax }{figure.caption.12}{}}
\newlabel{subfig:@cref}{{[subfigure][2][2]2b}{[1][5][]5}}
\newlabel{sub@subfig:}{{b}{5}{After training\relax }{figure.caption.12}{}}
\newlabel{sub@subfig:@cref}{{[subfigure][2][2]2b}{[1][5][]5}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Largest 90 eigen values of ResNet50 model on CIFAR10 for different Receptive Fields \relax }}{5}{figure.caption.12}\protected@file@percent }
\newlabel{fig:hessian_resnet50_cifar10}{{2}{5}{Largest 90 eigen values of ResNet50 model on CIFAR10 for different Receptive Fields \relax }{figure.caption.12}{}}
\newlabel{fig:hessian_resnet50_cifar10@cref}{{[figure][2][]2}{[1][5][]5}}
\citation{kornblithSimilarityNeuralNetwork2019}
\newlabel{fig:images-resnet50_CIFAR10_training-png}{{3a}{6}{ResNet50 training for different Receptive fields on CIFAR10\relax }{figure.caption.13}{}}
\newlabel{fig:images-resnet50_CIFAR10_training-png@cref}{{[subfigure][1][3]3a}{[1][5][]6}}
\newlabel{sub@fig:images-resnet50_CIFAR10_training-png}{{a}{6}{ResNet50 training for different Receptive fields on CIFAR10\relax }{figure.caption.13}{}}
\newlabel{sub@fig:images-resnet50_CIFAR10_training-png@cref}{{[subfigure][1][3]3a}{[1][5][]6}}
\newlabel{fig:images-resnet50_Tiny_imagenet_training-png}{{3b}{6}{ResNet50 training for different Receptive fields on Tiny ImageNet\relax }{figure.caption.13}{}}
\newlabel{fig:images-resnet50_Tiny_imagenet_training-png@cref}{{[subfigure][2][3]3b}{[1][5][]6}}
\newlabel{sub@fig:images-resnet50_Tiny_imagenet_training-png}{{b}{6}{ResNet50 training for different Receptive fields on Tiny ImageNet\relax }{figure.caption.13}{}}
\newlabel{sub@fig:images-resnet50_Tiny_imagenet_training-png@cref}{{[subfigure][2][3]3b}{[1][5][]6}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Training of ResNet50 on CIFAR10 and Tiny ImageNet\relax }}{6}{figure.caption.13}\protected@file@percent }
\newlabel{fig:training_trajectories}{{3}{6}{Training of ResNet50 on CIFAR10 and Tiny ImageNet\relax }{figure.caption.13}{}}
\newlabel{fig:training_trajectories@cref}{{[figure][3][]3}{[1][5][]6}}
\citation{yosinskiHowTransferableAre2014,roAutoLRLayerwisePruning2021}
\citation{roAutoLRLayerwisePruning2021}
\newlabel{fig:similarity_lvl1}{{4a}{7}{Network1}{figure.caption.14}{}}
\newlabel{fig:similarity_lvl1@cref}{{[subfigure][1][4]4a}{[1][6][]7}}
\newlabel{sub@fig:similarity_lvl1}{{a}{7}{Network1}{figure.caption.14}{}}
\newlabel{sub@fig:similarity_lvl1@cref}{{[subfigure][1][4]4a}{[1][6][]7}}
\newlabel{fig:similarity_lvl2}{{4b}{7}{Network2}{figure.caption.14}{}}
\newlabel{fig:similarity_lvl2@cref}{{[subfigure][2][4]4b}{[1][6][]7}}
\newlabel{sub@fig:similarity_lvl2}{{b}{7}{Network2}{figure.caption.14}{}}
\newlabel{sub@fig:similarity_lvl2@cref}{{[subfigure][2][4]4b}{[1][6][]7}}
\newlabel{fig:similarity_lvl3}{{4c}{7}{}{figure.caption.14}{}}
\newlabel{fig:similarity_lvl3@cref}{{[subfigure][3][4]4c}{[1][6][]7}}
\newlabel{sub@fig:similarity_lvl3}{{c}{7}{}{figure.caption.14}{}}
\newlabel{sub@fig:similarity_lvl3@cref}{{[subfigure][3][4]4c}{[1][6][]7}}
\newlabel{fig:similarity_lvl4}{{4d}{7}{}{figure.caption.14}{}}
\newlabel{fig:similarity_lvl4@cref}{{[subfigure][4][4]4d}{[1][6][]7}}
\newlabel{sub@fig:similarity_lvl4}{{d}{7}{}{figure.caption.14}{}}
\newlabel{sub@fig:similarity_lvl4@cref}{{[subfigure][4][4]4d}{[1][6][]7}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Representation similarity}}{7}{figure.caption.14}\protected@file@percent }
\newlabel{fig:s