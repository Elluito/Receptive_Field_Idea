@article{0ebeadddd4fd8d30011008145e1a2e3f28efa89a,
  title = {Jacobian Regularization for Mitigating Universal Adversarial Perturbations},
  author = {Co, Kenneth T. and Rego, David Mart{\'i}nez and Lupu, Emil C.},
  year = {2021},
  journal = {ArXiv},
  volume = {abs/2104.10459},
  doi = {10.1007/978-3-030-86380-7_17},
  abstract = {Universal Adversarial Perturbations (UAPs) are input perturbations that can fool a neural network on large sets of data. They are a class of attacks that represents a significant threat as they facilitate realistic, practical, and low-cost attacks on neural networks. In this work, we derive upper bounds for the effectiveness of UAPs based on norms of data-dependent Jacobians. We empirically verify that Jacobian regularization greatly increases model robustness to UAPs by up to four times whilst maintaining clean performance. Our theoretical analysis also allows us to formulate a metric for the strength of shared adversarial perturbations between pairs of inputs. We apply this metric to benchmark datasets and show that it is highly correlated with the actual observed robustness. This suggests that realistic and practical universal attacks can be reliably mitigated without sacrificing clean accuracy, which shows promise for the robustness of machine learning systems.},
  arxivid = {2104.10459}
}

@article{0fdac4f87e9fd3f7d98f65f513a3f5aa4d794392,
  title = {Improving Local Effectiveness for Global Robust Training},
  author = {Lu, Jingyue and Kumar, M. P.},
  year = {2021},
  journal = {ArXiv},
  volume = {abs/2110.14030},
  abstract = {Many successful robust training methods rely on strong adversaries, which can be prohibitively expensive to generate when the input dimension is high and the model structure is complicated. We adopt a new perspective on robustness and propose a novel training algorithm that allows a more effective use of adversaries. Our method improves the model robustness at each local ball centered around an adversary and then, by combining these local balls through a global term, achieves overall robustness. Focusing on local balls improves the efficiency of robust training. We demonstrate the performance of our method on MNIST, CIFAR-10, CIFAR-100.},
  arxivid = {2110.14030}
}

@article{18c03ca8bf432b83cd0d4148d3a171cde326c6f9,
  title = {Fighting Adversarial Images with Interpretable Gradients},
  author = {Du, Keke and Chang, Shane and Wen, Huixiang and Zhang, Hao},
  year = {2021},
  journal = {ACM Turing Award Celebration Conference - China ( ACM TURC 2021)},
  doi = {10.1145/3472634.3472644},
  abstract = {Adversarial images are specifically designed to fool neural networks into making a wrong decision about what they are looking at, which severely degrade neural network accuracy. Recently, empirical and theoretical evidence suggests that robust neural network models tend to have better interpretable gradients. Therefore, we speculate that improving the interpretability of the gradients of the neural network models may also help to improve the robustness of the models. Two methods are used to add gradient-dependent constraint terms to the loss function of neural network models and both improve the robustness of the models. The first method adds the fussed lasso penalty term of the saliency maps to the loss function of the neural network models, which makes the saliency maps arrange in a natural way to improve the interpretability of the saliency maps, and uses the gradient enhancement for relu instead of relu to strengthen the constraint of regularization term on saliency maps. In the second method, the cosine similarity penalty term between the input gradients and the image contour is added to the loss function of the model to constrain the approximation between the input gradients and the image contour. This method has a certain biological significance, because the contour information of the image is used in the human visual system to recognize the image. Both methods improve the interpretability of model`s gradients and the first method exceeds most regularization methods except adversarial training on MNIST and the second method even exceeds the adversarial training under white-box attacks on CIFAR-10 and CIFAR-100.}
}

@misc{210303570Secondorder,
  title = {[2103.03570] {{Second-order}} Step-Size Tuning of {{SGD}} for Non-Convex Optimization},
  urldate = {2021-12-30},
  howpublished = {https://arxiv.org/abs/2103.03570},
  file = {/home/luisaam/Zotero/storage/36TUPC37/2103.html}
}

@article{8add03796d5c4e345d17b9f487aabd730da9bb10,
  title = {Regression Approach to a Novel Lateral Flatness Leveling System for Smart Manufacturing},
  author = {Tsai, S. and Chang, J.},
  year = {2021},
  journal = {Applied Sciences},
  doi = {10.3390/app11146645},
  abstract = {Sheet metal coils are widely used in the steel, automotive, and electronics industries. Many of these coils are processed through metal stamping or laser cutting to form different types of shapes. Sheet metal coil leveling is an essential procedure before any metal forming process. In practice, this leveling procedure is now executed by operators and primarily relies on their experience, resulting in many trials and errors before settling on the correct machine parameters. In smart manufacturing, it is required to digitize the machine's parameters to achieve such a leveling process. Although smart manufacturing has been adopted in the manufacturing industry in recent years, it has not been implemented in steel leveling. In this paper, a novel leveling method for flatness leveling is proposed and validated with data collected by flatness sensors for measuring each roll adjustment position, which is later processed through the multi-regression method. The regression results and experienced machine operator results are compared. From this research, not only can the experience of the machine operators be digitized, but the results also indicate the feasibility of the proposed method to offer more efficient and accurate machine settings for metal leveling operations.}
}

@article{a33f8957a8a5f14011ab103187c28a6263d4a299,
  title = {Quantifying the Preferential Direction of the Model Gradient in Adversarial Training with Projected Gradient Descent},
  author = {Lanfredi, Ricardo Bigolin and Schroeder, J. and Tasdizen, T.},
  year = {2020},
  journal = {ArXiv},
  volume = {abs/2009.04709},
  abstract = {Adversarial training, especially projected gradient descent (PGD), has been the most successful approach for improving robustness against adversarial attacks. After adversarial training, gradients of models with respect to their inputs are meaningful and interpretable by humans. However, the concept of interpretability is not mathematically well established, making it difficult to evaluate it quantitatively. We define interpretability as the alignment of the model gradient with the vector pointing toward the closest point of the support of the other class. We propose a method for measuring this alignment for binary classification problems, using generative adversarial model training to produce the smallest residual needed to change the class present in the image. We show that PGD-trained models are more interpretable than the baseline according to our definition, and our metric presents higher alignment values than a competing metric formulation. We also show that enforcing this alignment increases the robustness of models without adversarial training.},
  arxivid = {2009.04709}
}

@article{a5344ca43451dd819a8b2b65eca4ad28e949b7f3,
  title = {Learning Smooth Neural Functions via Lipschitz Regularization},
  author = {Liu, Hsueh-Ti Derek and Williams, Francis and Jacobson, A. and Fidler, S. and Litany, O.},
  year = {2022},
  journal = {ArXiv},
  volume = {abs/2202.08345},
  abstract = {Neural implicit fields have recently emerged as a useful representation for 3D shapes. These fields are commonly represented as neural networks which map latent descriptors and 3D coordinates to implicit function values. The la- tent descriptor of a neural field acts as a deformation handle for the 3D shape it represents. Thus, smoothness with respect to this descriptor is paramount for performing shape-editing operations. In this work, we introduce a novel regularization designed to encourage smooth latent spaces in neural fields by penalizing the upper bound on the field's Lipschitz constant. Compared with prior Lipschitz regularized networks, ours is computationally fast, can be implemented in four lines of code, and requires minimal hyperparameter tuning for geometric applications. We demonstrate the effectiveness of our approach on shape interpolation and extrapolation as well as partial shape reconstruction from 3D point clouds, showing both qualitative and quan- titative improvements over existing state-of-the-art and non-regularized baselines.},
  arxivid = {2202.08345}
}

@misc{abadiTensorFlowLargescaleMachine2015,
  title = {{{TensorFlow}}: {{Large-scale}} Machine Learning on Heterogeneous Systems},
  author = {Abadi, Mart{\'i}n and Agarwal, Ashish and Barham, Paul and Brevdo, Eugene and Chen, Zhifeng and Citro, Craig and Corrado, Greg S. and Davis, Andy and Dean, Jeffrey and Devin, Matthieu and Ghemawat, Sanjay and Goodfellow, Ian and Harp, Andrew and Irving, Geoffrey and Isard, Michael and Jia, Yangqing and Jozefowicz, Rafal and Kaiser, Lukasz and Kudlur, Manjunath and Levenberg, Josh and Man{\'e}, Dandelion and Monga, Rajat and Moore, Sherry and Murray, Derek and Olah, Chris and Schuster, Mike and Shlens, Jonathon and Steiner, Benoit and Sutskever, Ilya and Talwar, Kunal and Tucker, Paul and Vanhoucke, Vincent and Vasudevan, Vijay and Vi{\'e}gas, Fernanda and Vinyals, Oriol and Warden, Pete and Wattenberg, Martin and Wicke, Martin and Yu, Yuan and Zheng, Xiaoqiang},
  year = {2015}
}

@article{abdelazizAdvancedMetaheuristicOptimization2021,
  title = {Advanced Metaheuristic Optimization Techniques in Applications of Deep Neural Networks: A Review},
  shorttitle = {Advanced Metaheuristic Optimization Techniques in Applications of Deep Neural Networks},
  author = {Abd Elaziz, Mohamed and Dahou, Abdelghani and Abualigah, Laith and Yu, Liyang and Alshinwan, Mohammad and Khasawneh, Ahmad M. and Lu, Songfeng},
  year = {2021},
  month = nov,
  journal = {Neural Computing and Applications},
  volume = {33},
  number = {21},
  pages = {14079--14099},
  issn = {1433-3058},
  doi = {10.1007/s00521-021-05960-5},
  urldate = {2022-03-22},
  abstract = {Deep neural networks (DNNs) have evolved as a beneficial machine learning method that has been successfully used in various applications. Currently, DNN is a superior technique of extracting information from massive sets of data in a self-organized method. DNNs have different structures and parameters, which are usually produced for particular applications. Nevertheless, the training procedures of DNNs can be protracted depending on the given application and the size of the training set. Further, determining the most precise and practical structure of a deep learning method in a reasonable time is a possible problem related to this procedure. Meta-heuristics techniques, such as swarm intelligence (SI) and evolutionary computing (EC), represent optimization frames with specific theories and objective functions. These methods are adjustable and have been demonstrated their effectiveness in various applications; hence, they can optimize the DNNs models. This paper presents a comprehensive survey of the recent optimization methods (i.e., SI and EC) employed to enhance DNNs performance on various tasks. This paper also analyzes the importance of optimization methods in generating the optimal hyper-parameters and structures of DNNs in taking into consideration massive-scale data. Finally, several potential directions that still need improvements and open problems in evolutionary DNNs are identified.},
  langid = {english},
  keywords = {metaheuristics,review},
  file = {/home/luisaam/Zotero/storage/QSZTWXNM/Abd Elaziz et al. - 2021 - Advanced metaheuristic optimization techniques in .pdf}
}

@misc{AIEfficiency2020,
  title = {{{AI}} and {{Efficiency}}},
  year = {2020},
  month = may,
  journal = {OpenAI},
  urldate = {2021-08-16},
  abstract = {We're releasing an analysis showing that since 2012 the amount of compute needed to train a neural net to the same performance on ImageNet classification has been decreasing by a factor of 2 every 16 months. Compared to 2012, it now takes 44 times less compute to train a},
  howpublished = {https://openai.com/blog/ai-and-efficiency/},
  langid = {english},
  file = {/home/luisaam/Zotero/storage/EXKDECAP/ai-and-efficiency.html}
}

@book{AITeachesItself,
  title = {{{AI Teaches Itself Diplomacy}} - {{IEEE Spectrum}}},
  urldate = {2021-03-07}
}

@article{akbariVATTTransformersMultimodal2021,
  title = {{{VATT}}: {{Transformers}} for {{Multimodal Self-Supervised Learning}} from {{Raw Video}}, {{Audio}} and {{Text}}},
  shorttitle = {{{VATT}}},
  author = {Akbari, Hassan and Yuan, Linagzhe and Qian, Rui and Chuang, Wei-Hong and Chang, Shih-Fu and Cui, Yin and Gong, Boqing},
  year = {2021},
  month = apr,
  journal = {arXiv:2104.11178 [cs, eess]},
  eprint = {2104.11178},
  primaryclass = {cs, eess},
  urldate = {2021-07-30},
  abstract = {We present a framework for learning multimodal representations from unlabeled data using convolution-free Transformer architectures. Specifically, our Video-Audio-Text Transformer (VATT) takes raw signals as inputs and extracts multimodal representations that are rich enough to benefit a variety of downstream tasks. We train VATT end-to-end from scratch using multimodal contrastive losses and evaluate its performance by the downstream tasks of video action recognition, audio event classification, image classification, and text-to-video retrieval. Furthermore, we study a modality-agnostic single-backbone Transformer by sharing weights among the three modalities. We show that the convolution-free VATT outperforms state-of-the-art ConvNet-based architectures in the downstream tasks. Especially, VATT's vision Transformer achieves the top-1 accuracy of 82.1\% on Kinetics-400, 83.6\% on Kinetics-600,and 41.1\% on Moments in Time, new records while avoiding supervised pre-training. Transferring to image classification leads to 78.7\% top-1 accuracy on ImageNet compared to 64.7\% by training the same Transformer from scratch, showing the generalizability of our model despite the domain gap between videos and images. VATT's audio Transformer also sets a new record on waveform-based audio event recognition by achieving the mAP of 39.4\% on AudioSet without any supervised pre-training.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Multimedia,Electrical Engineering and Systems Science - Image and Video Processing,small networks},
  file = {/home/luisaam/Zotero/storage/LL3YZVPG/Akbari et al. - 2021 - VATT Transformers for Multimodal Self-Supervised .pdf;/home/luisaam/Zotero/storage/FVCDWV98/2104.html}
}

@misc{alainNegativeEigenvaluesHessian2019,
  title = {Negative Eigenvalues of the {{Hessian}} in Deep Neural Networks},
  author = {Alain, Guillaume and Roux, Nicolas Le and Manzagol, Pierre-Antoine},
  year = {2019},
  month = feb,
  number = {arXiv:1902.02366},
  eprint = {1902.02366},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1902.02366},
  urldate = {2023-12-03},
  abstract = {The loss function of deep networks is known to be non-convex but the precise nature of this nonconvexity is still an active area of research. In this work, we study the loss landscape of deep networks through the eigendecompositions of their Hessian matrix. In particular, we examine how important the negative eigenvalues are and the benefits one can observe in handling them appropriately.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Second Order,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/QX2QFRCQ/Alain et al. - 2019 - Negative eigenvalues of the Hessian in deep neural.pdf;/home/luisaam/Zotero/storage/T8AVTA3F/1902.html}
}

@inproceedings{amjadRegressionDeepNeural2021,
  title = {Regression with {{Deep Neural Networks}}: {{Generalization Error Guarantees}}, {{Learning Algorithms}}, and {{Regularizers}}},
  shorttitle = {Regression with {{Deep Neural Networks}}},
  booktitle = {2021 29th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  author = {Amjad, Jaweria and Lyu, Zhaoyan and Rodrigues, Miguel R.D.},
  year = {2021},
  month = aug,
  pages = {1481--1485},
  issn = {2076-1465},
  doi = {10.23919/EUSIPCO54536.2021.9616069},
  abstract = {We present new data-dependent characterizations of the generalization capability of deep neural networks based data representations within the context of regression tasks. In particular, we propose new generalization error bounds that depend on various elements associated with the learning problem such as the complexity of the data space, the cardinality of the training set, and the input-output Jacobian of the deep neural network. Moreover, building upon our bounds, we propose new regularization strategies constraining the network Lipschitz properties through norms of the network gradient. Experimental results show that our newly proposed regularization techniques can deliver state-of-the-art performance in comparison to established weight-based regularization.},
  keywords = {Buildings,Deep learning,Europe,Jacobian matrices,Jacobian regularisation,Signal processing,Signal processing algorithms,Sobolev training,Training},
  file = {/home/luisaam/Zotero/storage/ZPPI7FQX/9616069.html}
}

@misc{amodeiAICompute2018,
  title = {{{AI}} and {{Compute}}},
  author = {Amodei, Dario and Hernandez, Danny},
  year = {2018},
  month = may,
  journal = {OpenAI},
  urldate = {2022-01-26},
  abstract = {We're releasing an analysis showing that since 2012, the amount of compute used in the largest AI training runs has been increasing exponentially with a 3.4-month doubling time (by comparison, Moore's Law had a 2-year doubling period).},
  howpublished = {https://openai.com/blog/ai-and-compute/},
  langid = {english},
  file = {/home/luisaam/Zotero/storage/XLAGM7VW/ai-and-compute.html}
}

@article{anagunDeepLearningBasedCustomer2023,
  title = {Deep {{Learning-Based Customer Complaint Management}}},
  author = {Anagun, Yildiray and Bolel, Nur Sultan and Isik, Sahin and Ozkan, Serif Ercan},
  year = {2023},
  month = jun,
  journal = {Journal of Organizational Computing and Electronic Commerce},
  volume = {0},
  number = {0},
  pages = {1--15},
  publisher = {{Taylor \& Francis}},
  issn = {1091-9392},
  doi = {10.1080/10919392.2023.2210049},
  urldate = {2023-06-08},
  abstract = {In recent years, managing customer complaints poses a problem for companies due to the increasing market and customer base. One of the most effective ways to speed up the handling of complaints is to categorize customer issues and automatically forward complaints to relevant officers or departments. This reduces the response time to complaints and ensures that specific complaints are being handled by the people with the right expertise. Also, the companies can create a strategy exclusively for certain types of problems, which will hasten the problem resolution. In this article, we propose an intelligent customer complaint management system (CCMS) for financial services organizations. We described a pre-processing technique for Turkish agglutinative language using deep learning algorithms and it was not previously considered in the literature. Furthermore, the performance of the algorithm has been significantly increased by choosing the appropriate combinations of pre-processing tasks. The proposed method not only greatly increases text classification's utility for a broader range of customer complaints, but it also yields improved overall performance, recorded with a 96\% accuracy score. The findings of the experiments show that the proposed approach is more effective than the other state-of-the-art strategies.},
  keywords = {agglutinative language,artificial intelligence,complaint management,Customer complaint,deep learning,text classification,Turkish text classification},
  file = {/home/luisaam/Zotero/storage/HGCQABAR/Anagun et al. - 2023 - Deep Learning-Based Customer Complaint Management.pdf}
}

@article{anandkumarEfficientApproachesEscaping2016,
  title = {Efficient Approaches for Escaping Higher Order Saddle Points in Non-Convex Optimization},
  author = {Anandkumar, Anima and Ge, Rong},
  year = {2016},
  month = feb,
  journal = {arXiv:1602.05908 [cs, stat]},
  eprint = {1602.05908},
  primaryclass = {cs, stat},
  urldate = {2021-06-18},
  abstract = {Local search heuristics for non-convex optimizations are popular in applied machine learning. However, in general it is hard to guarantee that such algorithms even converge to a local minimum, due to the existence of complicated saddle point structures in high dimensions. Many functions have degenerate saddle points such that the first and second order derivatives cannot distinguish them with local optima. In this paper we use higher order derivatives to escape these saddle points: we design the first efficient algorithm guaranteed to converge to a third order local optimum (while existing techniques are at most second order). We also show that it is NP-hard to extend this further to finding fourth order local optima.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Saddle Points,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/UFQSEII9/Anandkumar y Ge - 2016 - Efficient approaches for escaping higher order sad.pdf;/home/luisaam/Zotero/storage/BSJKMZTG/1602.html}
}

@article{aronskyDiagnosingCommunityacquiredPneumonia1998,
  title = {Diagnosing Community-Acquired Pneumonia with a {{Bayesian}} Network.},
  author = {Aronsky, D. and Haug, P. J.},
  year = {1998},
  journal = {Proceedings of the AMIA Symposium},
  pages = {632--636},
  issn = {1531-605X},
  urldate = {2023-06-01},
  abstract = {We present the development and the evaluation of a Bayesian network for the diagnosis of community-acquired pneumonia. The Bayesian network is intended to be part of a larger decision support system which assists emergency room physicians in the management of pneumonia patients. Minimal data entry from the nurse or the physician, timely availability of clinical parameters, and high accuracy were requirements we tried to meet. Data from more than 32,000 emergency room patients over a period of 2 years (June 1995-June 1997) were extracted from the clinical information system to train and test the Bayesian network. The network performed well in discriminating patients with pneumonia from patients with other diseases. The Bayesian network achieved a sensitivity of 95\%, a specificity of 96.5\%, an area under the receiver operating characteristic of 0.98, and a predictive value positive of 26.8\%. Our feasibility study demonstrates that the proposed Bayesian network is an appropriate method to detect pneumonia patients with high accuracy. The study suggests that the proposed Bayesian network may represent a successful component within a larger decision support system for the management of community-acquired pneumonia.},
  pmcid = {PMC2232064},
  pmid = {9929296},
  keywords = {Bayesian Networks},
  file = {/home/luisaam/Zotero/storage/HGLR9BCD/Aronsky and Haug - 1998 - Diagnosing community-acquired pneumonia with a Bay.pdf}
}

@inproceedings{aroraOptimizationDeepNetworks2018,
  title = {On the {{Optimization}} of {{Deep Networks}}: {{Implicit Acceleration}} by {{Overparameterization}}},
  shorttitle = {On the {{Optimization}} of {{Deep Networks}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Arora, Sanjeev and Cohen, Nadav and Hazan, Elad},
  year = {2018},
  month = jul,
  pages = {244--253},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2021-11-14},
  abstract = {Conventional wisdom in deep learning states that increasing depth improves expressiveness but complicates optimization. This paper suggests that, sometimes, increasing depth can speed up optimization. The effect of depth on optimization is decoupled from expressiveness by focusing on settings where additional layers amount to overparameterization {\textendash} linear neural networks, a well-studied model. Theoretical analysis, as well as experiments, show that here depth acts as a preconditioner which may accelerate convergence. Even on simple convex problems such as linear regression with {$\mathscr{l}$}p{$\mathscr{l}$}p{\textbackslash}ell\_p loss, p{$>$}2p{$>$}2p{$>$}2, gradient descent can benefit from transitioning to a non-convex overparameterized objective, more than it would from some common acceleration schemes. We also prove that it is mathematically impossible to obtain the acceleration effect of overparametrization via gradients of any regularizer.},
  langid = {english},
  keywords = {Overparametrization},
  file = {/home/luisaam/Zotero/storage/CFXI37CG/Arora et al. - 2018 - On the Optimization of Deep Networks Implicit Acc.pdf;/home/luisaam/Zotero/storage/KRTCP94P/Arora et al. - 2018 - On the Optimization of Deep Networks Implicit Acc.pdf}
}

@article{AttentionSpikingNeural,
  title = {Attention {{Spiking Neural Networks}}. ({{arXiv}}:2209.13929v1 [Cs.{{CV}}])},
  journal = {arXiv Computer Science},
  doi = {arXiv:2209.13929v1},
  abstract = {Benefiting from the event-driven and sparse spiking characteristics of the brain, spiking neural networks (SNNs) are becoming an energy-efficient alternative to artificial neural networks (ANNs). However, the performance gap between SNNs and ANNs has been a great hindrance to deploying SNNs ubiquitously for a long time. To leverage the full potential of SNNs, we study the effect of attention mechanisms in SNNs. We first present our idea of attention with a plug-and-play kit, termed the Multi-dimensional Attention (MA). Then, a new attention SNN architecture with end-to-end training called "MA-SNN" is proposed, which infers attention weights along the temporal, channel, as well as spatial dimensions separately or simultaneously. Based on the existing neuroscience theories, we exploit the attention weights to optimize membrane potentials, which in turn regulate the spiking response in a data-dependent way. At the cost of negligible additional parameters, MA facilitates vanilla SNNs to achieve sparser spiking activity, better performance, and energy efficiency concurrently. Experiments are conducted in event-based DVS128 Gesture/Gait action recognition and ImageNet-1k image classification. On Gesture/Gait, the spike counts are reduced by 84.9\%/81.6\%, and the task accuracy and energy efficiency are improved by 5.9\%/4.7\% and 3.4/3.2. On ImageNet-1K, we achieve top-1 accuracy of 75.92\% and 77.08\% on single/4-step Res-SNN-104, which are state-of-the-art results in SNNs. To our best knowledge, this is for the first time, that the SNN community achieves comparable or even better performance compared with its ANN counterpart in the large-scale dataset. Our work lights up SNN's potential as a general backbone to support various applications for SNNs, with a great balance between effectiveness and efficiency.},
  keywords = {Researcher App}
}

@article{avranasCodedResNeXtNetwork2022,
  title = {Coded {{ResNeXt}}: A Network for Designing Disentangled Information Paths},
  shorttitle = {Coded {{ResNeXt}}},
  author = {Avranas, Apostolos and Kountouris, Marios},
  year = {2022},
  month = feb,
  journal = {arXiv:2202.05343 [cs]},
  eprint = {2202.05343},
  primaryclass = {cs},
  urldate = {2022-02-14},
  abstract = {To avoid treating neural networks as highly complex black boxes, the deep learning research community has tried to build interpretable models allowing humans to understand the decisions taken by the model. Unfortunately, the focus is mostly on manipulating only the very high-level features associated with the last layers. In this work, we look at neural network architectures for classification in a more general way and introduce an algorithm which defines before the training the paths of the network through which the per-class information flows. We show that using our algorithm we can extract a lighter single-purpose binary classifier for a particular class by removing the parameters that do not participate in the predefined information path of that class, which is approximately 60\% of the total parameters. Notably, leveraging coding theory to design the information paths enables us to use intermediate network layers for making early predictions without having to evaluate the full network. We demonstrate that a slightly modified ResNeXt model, trained with our algorithm, can achieve higher classification accuracy on CIFAR-10/100 and ImageNet than the original ResNeXt, while having all the aforementioned properties.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,landscape characterization},
  file = {/home/luisaam/Zotero/storage/Q9V4PLYV/Avranas y Kountouris - 2022 - Coded ResNeXt a network for designing disentangle.pdf;/home/luisaam/Zotero/storage/MKJKGHR2/2202.html}
}

@article{avrutskiyEnhancingFunctionApproximation2021,
  title = {Enhancing {{Function Approximation Abilities}} of {{Neural Networks}} by {{Training Derivatives}}},
  author = {Avrutskiy, V. I.},
  year = {2021},
  month = feb,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {32},
  number = {2},
  pages = {916--924},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2020.2979706},
  abstract = {A method to increase the precision of feedforward networks is proposed. It requires prior knowledge of a target's function derivatives of several orders and uses this information in gradient-based training. Forward pass calculates not only the values of the output layer of a network but also their derivatives. The deviations of those derivatives from the target ones are used in an extended cost function, and then, the backward pass calculates the gradient of the extended cost with respect to weights, which is then used by a weights update algorithm. The most accurate approximation is obtained when the training starts with all available derivatives that are then step by step excluded from the extended cost function, starting with the highest orders up until only values are trained. Despite a substantial increase in arithmetic operations per pattern (compared with the conventional training), the method allows to obtain 140-1000 times more accurate approximation for simple cases if the total number of operations is equal. This precision also happens to be out of reach for the regular cost function. The method works well for solving differential equations with neural networks. The cost function is the deviation of the equation's residual from zero, and it can be extended by differentiating the equation itself so no prior information is required. This extension allows to solve 2-D nonlinear partial differential equation 13 times more accurately using seven times fewer grid points. The GPU-efficient algorithm for calculating the gradient of the extended cost function is proposed.},
  keywords = {Cost function,Function approximation,high-order derivatives,Jacobian regularisation,Learning systems,Mathematical model,neural networks,Neural networks,partial differential equations,Sobolev training,Standards,Training},
  file = {/home/luisaam/Zotero/storage/QGRLEXKL/Avrutskiy - 2021 - Enhancing Function Approximation Abilities of Neur.pdf;/home/luisaam/Zotero/storage/JGQ8GE59/9058995.html}
}

@inproceedings{baiDualLotteryTicket2022,
  title = {Dual {{Lottery Ticket Hypothesis}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Bai, Yue and Wang, Huan and Tao, Zhiqiang and Li, Kunpeng and Fu, Yun},
  year = {2022},
  month = mar,
  urldate = {2022-12-15},
  abstract = {Fully exploiting the learning capacity of neural networks requires overparameterized dense networks. On the other side, directly training sparse neural networks typically results in unsatisfactory performance. Lottery Ticket Hypothesis (LTH) provides a novel view to investigate sparse network training and maintain its capacity. Concretely, it claims there exist winning tickets from a randomly initialized network found by iterative magnitude pruning and preserving promising trainability (or we say being in trainable condition). In this work, we regard the winning ticket from LTH as the subnetwork which is in trainable condition and its performance as our benchmark, then go from a complementary direction to articulate the Dual Lottery Ticket Hypothesis (DLTH): Randomly selected subnetworks from a randomly initialized dense network can be transformed into a trainable condition and achieve admirable performance compared with LTH --- random tickets in a given lottery pool can be transformed into winning tickets. Specifically, by using uniform-randomly selected subnetworks to represent the general cases, we propose a simple sparse network training strategy, Random Sparse Network Transformation (RST), to substantiate our DLTH. Concretely, we introduce a regularization term to borrow learning capacity and realize information extrusion from the weights which will be masked. After finishing the transformation for the randomly selected subnetworks, we conduct the regular finetuning to evaluate the model using fair comparisons with LTH and other strong baselines. Extensive experiments on several public datasets and comparisons with competitive approaches validate our DLTH as well as the effectiveness of the proposed model RST. Our work is expected to pave a way for inspiring new research directions of sparse network training in the future. Our code is available at https://github.com/yueb17/DLTH.},
  langid = {english},
  keywords = {LTH,mask training,population of networks,Pruning},
  file = {/home/luisaam/Zotero/storage/92PCK424/Bai et al. - 2022 - Dual Lottery Ticket Hypothesis.pdf}
}

@article{baldassiShapingLearningLandscape2020,
  title = {Shaping the Learning Landscape in Neural Networks around Wide Flat Minima},
  author = {Baldassi, Carlo and Pittorino, Fabrizio and Zecchina, Riccardo},
  year = {2020},
  month = jan,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {1},
  pages = {161--170},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1908636117},
  urldate = {2021-12-25},
  abstract = {Learning in deep neural networks takes place by minimizing a nonconvex high-dimensional loss function, typically by a stochastic gradient descent (SGD) strategy. The learning process is observed to be able to find good minimizers without getting stuck in local critical points and such minimizers are often satisfactory at avoiding overfitting. How these 2 features can be kept under control in nonlinear devices composed of millions of tunable connections is a profound and far-reaching open question. In this paper we study basic nonconvex 1- and 2-layer neural network models that learn random patterns and derive a number of basic geometrical and algorithmic features which suggest some answers. We first show that the error loss function presents few extremely wide flat minima (WFM) which coexist with narrower minima and critical points. We then show that the minimizers of the cross-entropy loss function overlap with the WFM of the error loss. We also show examples of learning devices for which WFM do not exist. From the algorithmic perspective we derive entropy-driven greedy and message-passing algorithms that focus their search on wide flat regions of minimizers. In the case of SGD and cross-entropy loss, we show that a slow reduction of the norm of the weights along the learning process also leads to WFM. We corroborate the results by a numerical study of the correlations between the volumes of the minimizers, their Hessian, and their generalization performance on real data.},
  chapter = {PNAS Plus},
  copyright = {Copyright {\textcopyright} 2020 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by-nc-nd/4.0/This open access article is distributed under Creative Commons Attribution-NonCommercial-NoDerivatives License 4.0 (CC BY-NC-ND).},
  langid = {english},
  pmid = {31871189},
  keywords = {machine learning,neural networks,statistical physics},
  file = {/home/luisaam/Zotero/storage/43L5FGMS/Baldassi et al. - 2020 - Shaping the learning landscape in neural networks .pdf}
}

@article{baldassiUnveilingStructureWide2021,
  title = {Unveiling the {{Structure}} of {{Wide Flat Minima}} in {{Neural Networks}}},
  author = {Baldassi, Carlo and Lauditi, Clarissa and Malatesta, Enrico M. and Perugini, Gabriele and Zecchina, Riccardo},
  year = {2021},
  month = dec,
  journal = {Physical Review Letters},
  volume = {127},
  number = {27},
  pages = {278301},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevLett.127.278301},
  urldate = {2022-03-04},
  abstract = {The success of deep learning has revealed the application potential of neural networks across the sciences and opened up fundamental theoretical problems. In particular, the fact that learning algorithms based on simple variants of gradient methods are able to find near-optimal minima of highly nonconvex loss functions is an unexpected feature of neural networks. Moreover, such algorithms are able to fit the data even in the presence of noise, and yet they have excellent predictive capabilities. Several empirical results have shown a reproducible correlation between the so-called flatness of the minima achieved by the algorithms and the generalization performance. At the same time, statistical physics results have shown that in nonconvex networks a multitude of narrow minima may coexist with a much smaller number of wide flat minima, which generalize well. Here, we show that wide flat minima arise as complex extensive structures, from the coalescence of minima around ``high-margin'' (i.e., locally robust) configurations. Despite being exponentially rare compared to zero-margin ones, high-margin minima tend to concentrate in particular regions. These minima are in turn surrounded by other solutions of smaller and smaller margin, leading to dense regions of solutions over long distances. Our analysis also provides an alternative analytical method for estimating when flat minima appear and when algorithms begin to find solutions, as the number of model parameters varies.},
  keywords = {flat minima,flat optimizers,Loss Landscape,Weight space},
  file = {/home/luisaam/Zotero/storage/3CBXVYIV/Baldassi et al. - 2021 - Unveiling the Structure of Wide Flat Minima in Neu.pdf;/home/luisaam/Zotero/storage/NWFMQIGB/PhysRevLett.127.html}
}

@article{balduzziShatteredGradientsProblem2018,
  title = {The {{Shattered Gradients Problem}}: {{If}} Resnets Are the Answer, Then What Is the Question?},
  shorttitle = {The {{Shattered Gradients Problem}}},
  author = {Balduzzi, David and Frean, Marcus and Leary, Lennox and Lewis, J. P. and Ma, Kurt Wan-Duo and McWilliams, Brian},
  year = {2018},
  month = jun,
  journal = {arXiv:1702.08591 [cs, stat]},
  eprint = {1702.08591},
  primaryclass = {cs, stat},
  urldate = {2021-06-28},
  abstract = {A long-standing obstacle to progress in deep learning is the problem of vanishing and exploding gradients. Although, the problem has largely been overcome via carefully constructed initializations and batch normalization, architectures incorporating skip-connections such as highway and resnets perform much better than standard feedforward architectures despite well-chosen initialization and batch normalization. In this paper, we identify the shattered gradients problem. Specifically, we show that the correlation between gradients in standard feedforward networks decays exponentially with depth resulting in gradients that resemble white noise whereas, in contrast, the gradients in architectures with skip-connections are far more resistant to shattering, decaying sublinearly. Detailed empirical evidence is presented in support of the analysis, on both fully-connected networks and convnets. Finally, we present a new "looks linear" (LL) initialization that prevents shattering, with preliminary experiments showing the new initialization allows to train very deep networks without the addition of skip-connections.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning,Why no global second order},
  file = {/home/luisaam/Zotero/storage/CZ2XYW5T/Balduzzi et al. - 2018 - The Shattered Gradients Problem If resnets are th.pdf;/home/luisaam/Zotero/storage/VHVDKE7F/1702.html}
}

@article{ballardParallelVisualComputation1983,
  title = {Parallel Visual Computation},
  author = {Ballard, Dana H. and Hinton, Geoffrey E. and Sejnowski, Terrence J.},
  year = {1983},
  month = nov,
  journal = {Nature},
  volume = {306},
  number = {5938},
  pages = {21--26},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/306021a0},
  urldate = {2023-12-06},
  abstract = {The functional abilities and parallel architecture of the human visual system are a rich source of ideas about visual processing. Any visual task that we can perform quickly and effortlessly is likely to have a computational solution using a parallel algorithm. Recently, several such parallel algorithms have been found that exploit information implicit in an image to compute intrinsic properties of surfaces, such as surface orientation, reflectance and depth. These algorithms require a computational architecture that has similarities to that of visual cortex in primates.},
  copyright = {1983 Springer Nature Limited},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Receptive Field,Science},
  file = {/home/luisaam/Zotero/storage/9WJBAPEF/Ballard et al. - 1983 - Parallel visual computation.pdf}
}

@inproceedings{baninoPonderNetLearningPonder2021,
  title = {{{PonderNet}}: {{Learning}} to {{Ponder}}},
  shorttitle = {{{PonderNet}}},
  booktitle = {8th {{ICML Workshop}} on {{Automated Machine Learning}} ({{AutoML}})},
  author = {Banino, Andrea and Balaguer, Jan and Blundell, Charles},
  year = {2021},
  month = may,
  urldate = {2021-08-17},
  abstract = {PonderNet is a new algorithm that learns to adapt the amount of computation in a probabilist fashion, using end-to-end learning.},
  langid = {english},
  keywords = {small networks},
  file = {/home/luisaam/Zotero/storage/UL45PL6P/Banino et al. - 2021 - PonderNet Learning to Ponder.pdf;/home/luisaam/Zotero/storage/NQ6J4K5P/forum.html}
}

@article{bassilyExponentialConvergenceSGD2018,
  title = {On Exponential Convergence of {{SGD}} in Non-Convex over-Parametrized Learning},
  author = {Bassily, Raef and Belkin, Mikhail and Ma, Siyuan},
  year = {2018},
  month = nov,
  journal = {arXiv:1811.02564 [cs, math, stat]},
  eprint = {1811.02564},
  primaryclass = {cs, math, stat},
  urldate = {2021-05-17},
  abstract = {Large over-parametrized models learned via stochastic gradient descent (SGD) methods have become a key element in modern machine learning. Although SGD methods are very effective in practice, most theoretical analyses of SGD suggest slower convergence than what is empirically observed. In our recent work [8] we analyzed how interpolation, common in modern over-parametrized learning, results in exponential convergence of SGD with constant step size for convex loss functions. In this note, we extend those results to a much broader non-convex function class satisfying the Polyak-Lojasiewicz (PL) condition. A number of important non-convex problems in machine learning, including some classes of neural networks, have been recently shown to satisfy the PL condition. We argue that the PL condition provides a relevant and attractive setting for many machine learning problems, particularly in the over-parametrized regime.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Firs\_order\_Methods,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/VSBGIH57/Bassily et al. - 2018 - On exponential convergence of SGD in non-convex ov.pdf;/home/luisaam/Zotero/storage/Y2E92CUQ/1811.html}
}

@article{BayesianNonparametricsSparse,
  title = {Bayesian {{Nonparametrics}} for {{Sparse Dynamic Networks}}. ({{arXiv}}:1607.01624v2 [Stat.{{ML}}] {{UPDATED}})},
  journal = {arXiv Machine Learning (Statistics)},
  doi = {arXiv:1607.01624v2},
  abstract = {In this paper we propose a Bayesian nonparametric approach to modelling sparse time-varying networks. A positive parameter is associated to each node of a network, which models the sociability of that node. Sociabilities are assumed to evolve over time, and are modelled via a dynamic point process model. The model is able to capture long term evolution of the sociabilities. Moreover, it yields sparse graphs, where the number of edges grows subquadratically with the number of nodes. The evolution of the sociabilities is described by a tractable time-varying generalised gamma process. We provide some theoretical insights into the model and apply it to three datasets: a simulated network, a network of hyperlinks between communities on Reddit, and a network of co-occurences of words in Reuters news articles after the September 11th attacks.},
  keywords = {Researcher App}
}

@article{bc9b3a04b0275f2cc28a5a432771cbd93850c496,
  title = {Likelihood Landscapes: {{A}} Unifying Principle behind Many Adversarial Defenses},
  author = {Lin, Fu-Huei and Mittapalli, Rohit and Chattopadhyay, Prithvijit and Bolya, Daniel and Hoffman, Judy},
  year = {2020},
  doi = {10.1007/978-3-030-66415-2_3},
  abstract = {Convolutional Neural Networks have been shown to be vulnerable to adversarial examples, which are known to locate in subspaces close to where normal data lies but are not naturally occurring and of low probability. In this work, we investigate the potential effect defense techniques have on the geometry of the likelihood landscape - likelihood of the input images under the trained model. We first propose a way to visualize the likelihood landscape leveraging an energy-based model interpretation of discriminative classifiers. Then we introduce a measure to quantify the flatness of the likelihood landscape. We observe that a subset of adversarial defense techniques results in a similar effect of flattening the likelihood landscape. We further explore directly regularizing towards a flat landscape for adversarial robustness.},
  arxivid = {2008.11300}
}

@inproceedings{beerWhySpikingNeural2020,
  title = {Why {{Spiking Neural Networks Are~Efficient}}: {{A Theorem}}},
  shorttitle = {Why {{Spiking Neural Networks Are~Efficient}}},
  booktitle = {Information {{Processing}} and {{Management}} of {{Uncertainty}} in {{Knowledge-Based Systems}}},
  author = {Beer, Michael and Urenda, Julio and Kosheleva, Olga and Kreinovich, Vladik},
  editor = {Lesot, Marie-Jeanne and Vieira, Susana and Reformat, Marek Z. and Carvalho, Jo{\~a}o Paulo and Wilbik, Anna and {Bouchon-Meunier}, Bernadette and Yager, Ronald R.},
  year = {2020},
  series = {Communications in {{Computer}} and {{Information Science}}},
  pages = {59--69},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-50146-4_5},
  abstract = {Current artificial neural networks are very successful in many machine learning applications, but in some cases they still lag behind human abilities. To improve their performance, a natural idea is to simulate features of biological neurons which are not yet implemented in machine learning. One of such features is the fact that in biological neural networks, signals are represented by a train of spikes. Researchers have tried adding this spikiness to machine learning and indeed got very good results, especially when processing time series (and, more generally, spatio-temporal data). In this paper, we provide a possible theoretical explanation for this empirical success.},
  isbn = {978-3-030-50146-4},
  langid = {english},
  keywords = {Scale-invariance,Shift-invariance,Spiking neural networks},
  file = {/home/luisaam/Zotero/storage/J8VMHVAY/Beer et al. - 2020 - Why Spiking Neural Networks AreÂ Efficient A Theor.pdf}
}

@inproceedings{bellecDeepRewiringTraining2018,
  title = {Deep {{Rewiring}}: {{Training}} Very Sparse Deep Networks},
  booktitle = {International Conference on Learning Representations},
  author = {Bellec, Guillaume and Kappel, David and Maass, Wolfgang and Legenstein, Robert},
  year = {2018},
  keywords = {dynamical sparse training},
  file = {/home/luisaam/Zotero/storage/22TK85R8/Bellec et al. - 2018 - Deep Rewiring Training very sparse deep networks.pdf}
}

@inproceedings{bengioExpressivePowerDeep2011,
  title = {On the {{Expressive Power}} of {{Deep Architectures}}},
  booktitle = {Algorithmic {{Learning Theory}}},
  author = {Bengio, Yoshua and Delalleau, Olivier},
  editor = {Kivinen, Jyrki and Szepesv{\'a}ri, Csaba and Ukkonen, Esko and Zeugmann, Thomas},
  year = {2011},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {18--36},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-24412-4_3},
  abstract = {Deep architectures are families of functions corresponding to deep circuits. Deep Learning algorithms are based on parametrizing such circuits and tuning their parameters so as to approximately optimize some training objective. Whereas it was thought too difficult to train deep architectures, several successful algorithms have been proposed in recent years. We review some of the theoretical motivations for deep architectures, as well as some of their practical successes, and propose directions of investigations to address some of the remaining challenges.},
  isbn = {978-3-642-24412-4},
  langid = {english},
  keywords = {Deep Neural Network,Hide Unit,Neural Information Processing System,NN as universal approximators,Restrict Boltzmann Machine,Sparse Code},
  file = {/home/luisaam/Zotero/storage/EILN9F54/Bengio y Delalleau - 2011 - On the Expressive Power of Deep Architectures.pdf}
}

@inproceedings{bengioGreedyLayerWiseTraining2007,
  title = {Greedy {{Layer-Wise Training}} of {{Deep Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
  year = {2007},
  volume = {19},
  publisher = {{MIT Press}},
  urldate = {2021-08-18},
  keywords = {examples},
  file = {/home/luisaam/Zotero/storage/98KVDTD8/Bengio et al. - 2007 - Greedy Layer-Wise Training of Deep Networks.pdf}
}

@article{bengioMachineLearningCombinatorial2021,
  title = {Machine Learning for Combinatorial Optimization: {{A}} Methodological Tour d'horizon},
  shorttitle = {Machine Learning for Combinatorial Optimization},
  author = {Bengio, Yoshua and Lodi, Andrea and Prouvost, Antoine},
  year = {2021},
  month = apr,
  journal = {European Journal of Operational Research},
  volume = {290},
  number = {2},
  pages = {405--421},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2020.07.063},
  urldate = {2022-07-19},
  abstract = {This paper surveys the recent attempts, both from the machine learning and operations research communities, at leveraging machine learning to solve combinatorial optimization problems. Given the hard nature of these problems, state-of-the-art algorithms rely on handcrafted heuristics for making decisions that are otherwise too expensive to compute or mathematically not well defined. Thus, machine learning looks like a natural candidate to make such decisions in a more principled and optimized way. We advocate for pushing further the integration of machine learning and combinatorial optimization and detail a methodology to do so. A main point of the paper is seeing generic optimization problems as data points and inquiring what is the relevant distribution of problems to use for learning on a given task.},
  langid = {english},
  keywords = {Branch and bound,Combinatorial optimization,Machine learning,Mixed-integer programming solvers},
  file = {/home/luisaam/Zotero/storage/HSKX5I7Y/Bengio et al. - 2021 - Machine learning for combinatorial optimization A.pdf;/home/luisaam/Zotero/storage/YNV7HESZ/S0377221720306895.html}
}

@article{beyerEvolutionStrategiesComprehensive2002,
  title = {Evolution Strategies{\textendash}a Comprehensive Introduction},
  author = {Beyer, Hans-Georg and Schwefel, Hans-Paul},
  year = {2002},
  journal = {Natural computing},
  volume = {1},
  number = {1},
  pages = {3--52},
  publisher = {{Springer}}
}

@misc{biecekPerformanceNotEnough2023,
  title = {Performance Is Not Enough: A Story of the {{Rashomon}}'s Quartet},
  shorttitle = {Performance Is Not Enough},
  author = {Biecek, Przemyslaw and Baniecki, Hubert and Krzyznski, Mateusz},
  year = {2023},
  month = feb,
  number = {arXiv:2302.13356},
  eprint = {2302.13356},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  urldate = {2023-03-08},
  abstract = {Predictive modelling is often reduced to finding a single best model that optimises a selected model quality criterion. But what if the second best model describes the data equally well but in a completely different way? What about the third best? Following the Anscombe's quartet point, in this paper, we present a synthetic dataset for which four models from different classes have practically identical predictive performance. But, visualisation of these models reveals that they describe this dataset in very different ways.},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning,Statistics - Applications,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/EN4I6ZX2/Biecek et al. - 2023 - Performance is not enough a story of the Rashomon.pdf}
}

@article{blalockWhatStateNeural2020,
  title = {What Is the {{State}} of {{Neural Network Pruning}}?},
  author = {Blalock, Davis and Ortiz, Jose Javier Gonzalez and Frankle, Jonathan and Guttag, John},
  year = {2020},
  month = mar,
  journal = {arXiv:2003.03033 [cs, stat]},
  eprint = {2003.03033},
  primaryclass = {cs, stat},
  urldate = {2021-08-02},
  abstract = {Neural network pruning---the task of reducing the size of a network by removing parameters---has been the subject of a great deal of work in recent years. We provide a meta-analysis of the literature, including an overview of approaches to pruning and consistent findings in the literature. After aggregating results across 81 papers and pruning hundreds of models in controlled conditions, our clearest finding is that the community suffers from a lack of standardized benchmarks and metrics. This deficiency is substantial enough that it is hard to compare pruning techniques to one another or determine how much progress the field has made over the past three decades. To address this situation, we identify issues with current practices, suggest concrete remedies, and introduce ShrinkBench, an open-source framework to facilitate standardized evaluations of pruning methods. We use ShrinkBench to compare various pruning techniques and show that its comprehensive evaluation can prevent common pitfalls when comparing pruning methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Pruning,small networks,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/D5PDUGVZ/Blalock et al. - 2020 - What is the State of Neural Network Pruning.pdf;/home/luisaam/Zotero/storage/V7ZGH5UR/Blalock et al. - 2020 - What is the State of Neural Network Pruning.pdf;/home/luisaam/Zotero/storage/Y46M95YR/2003.html}
}

@inproceedings{blumTraining3nodeNeural1988,
  title = {Training a 3-Node Neural Network Is {{NP-complete}}},
  booktitle = {Proceedings of the 1st {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Blum, Avrim and Rivest, Ronald L},
  year = {1988},
  pages = {494--501},
  file = {/home/luisaam/Zotero/storage/IPGT99GG/Blum y Rivest - Training a 3-node neural network is NP-complete.pdf}
}

@incollection{bottouLargescaleMachineLearning2010,
  title = {Large-Scale Machine Learning with Stochastic Gradient Descent},
  booktitle = {Proceedings of {{COMPSTAT}}'2010},
  author = {Bottou, L{\'e}on},
  year = {2010},
  pages = {177--186},
  publisher = {{Springer}}
}

@article{bottouOptimizationMethodsLargeScale2018,
  title = {Optimization {{Methods}} for {{Large-Scale Machine Learning}}},
  author = {Bottou, L{\'e}on and Curtis, Frank E. and Nocedal, Jorge},
  year = {2018},
  month = feb,
  journal = {arXiv:1606.04838 [cs, math, stat]},
  eprint = {1606.04838},
  primaryclass = {cs, math, stat},
  urldate = {2021-06-24},
  abstract = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/IMES5ATI/Bottou et al. - 2018 - Optimization Methods for Large-Scale Machine Learn.pdf;/home/luisaam/Zotero/storage/DXGJ7A4L/1606.html}
}

@article{bouchacourtHowAgentsSee2020,
  title = {How Agents See Things: {{On}} Visual Representations in an Emergent Language Game},
  author = {Bouchacourt, D. and Baroni, M.},
  year = {2020},
  journal = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018},
  publisher = {{Association for Computational Linguistics}},
  abstract = {There is growing interest in the language developed by agents interacting in emergent-communication settings. Earlier studies have focused on the agents' symbol usage, rather than on their representation of visual input. In this paper, we consider the referential games of Lazaridou et al. (2017), and investigate the representations the agents develop during their evolving interaction. We find that the agents establish successful communication by inducing visual representations that almost perfectly align with each other, but, surprisingly, do not capture the conceptual properties of the objects depicted in the input images. We conclude that, if we are interested in developing language-like communication systems, we must pay more attention to the visual semantics agents associate to the symbols they use.}
}

@article{bousquetTradeoffsLargeScale,
  title = {The {{Tradeoffs}} of {{Large Scale Learning}}},
  author = {Bousquet, Olivier and Bottou, L{\'e}on},
  pages = {8},
  abstract = {This contribution develops a theoretical framework that takes into account the effect of approximate optimization on learning algorithms. The analysis shows distinct tradeoffs for the case of small-scale and large-scale learning problems. Small-scale learning problems are subject to the usual approximation{\textendash}estimation tradeoff. Large-scale learning problems are subject to a qualitatively different tradeoff involving the computational complexity of the underlying optimization algorithms in non-trivial ways.},
  langid = {english},
  file = {/home/luisaam/Zotero/storage/5PIPMEQ7/Bousquet y Bottou - The Tradeoffs of Large Scale Learning.pdf}
}

@article{BregmanLearningFramework,
  title = {A {{Bregman Learning Framework}} for {{Sparse Neural Networks}}},
  journal = {Journal of Machine Learning Research},
  doi = {79.12958.543d229d-efdf-490a-899e-331be0f1a165.1659604977},
  abstract = {We propose a learning framework based on stochastic Bregman iterations, also known as mirror descent, to train sparse neural networks with an inverse scale space approach. We derive a baseline algorithm called LinBreg, an accelerated version using momentum, and AdaBreg, which is a Bregmanized generalization of the Adam algorithm. In contrast to established methods for sparse training the proposed family of algorithms constitutes a regrowth strategy for neural networks that is solely optimization-based without additional heuristics.  Our Bregman learning framework starts the training with very few initial parameters, successively adding only significant ones to obtain a sparse and expressive network. The proposed approach is extremely easy and efficient, yet supported by the rich mathematical theory of inverse scale space methods. We derive a statistically profound sparse parameter initialization strategy and provide a rigorous stochastic convergence analysis of the loss decay and additional convergence proofs in the convex regime. Using only  of the parameters of ResNet-18 we achieve  test accuracy on CIFAR-10, compared to  using the dense network. Our algorithm also unveils an autoencoder architecture for a denoising task. The proposed framework also has a huge potential for integrating sparse backpropagation and resource-friendly training. Code is available at https://github.com/TimRoith/BregmanLearning.},
  keywords = {Pruning,Researcher App}
}

@article{breimanStatisticalModelingTwo2001,
  title = {Statistical {{Modeling}}: {{The Two Cultures}} (with Comments and a Rejoinder by the Author)},
  shorttitle = {Statistical {{Modeling}}},
  author = {Breiman, Leo},
  year = {2001},
  month = aug,
  journal = {Statistical Science},
  volume = {16},
  number = {3},
  pages = {199--231},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/ss/1009213726},
  urldate = {2023-03-08},
  abstract = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.},
  keywords = {Pruning},
  file = {/home/luisaam/Zotero/storage/BJ77E4SL/Breiman - 2001 - Statistical Modeling The Two Cultures (with comme.pdf}
}

@article{breuelEffectsHyperparametersSGD2015,
  title = {The {{Effects}} of {{Hyperparameters}} on {{SGD Training}} of {{Neural Networks}}},
  author = {Breuel, Thomas M.},
  year = {2015},
  month = aug,
  journal = {arXiv:1508.02788 [cs]},
  eprint = {1508.02788},
  primaryclass = {cs},
  urldate = {2021-05-17},
  abstract = {The performance of neural network classifiers is determined by a number of hyperparameters, including learning rate, batch size, and depth. A number of attempts have been made to explore these parameters in the literature, and at times, to develop methods for optimizing them. However, exploration of parameter spaces has often been limited. In this note, I report the results of large scale experiments exploring these different parameters and their interactions.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,K.3.2},
  file = {/home/luisaam/Zotero/storage/A2WPKSE8/Breuel - 2015 - The Effects of Hyperparameters on SGD Training of .pdf}
}

@article{brownHouseholdFinancesBig2014,
  title = {Household Finances and the `{{Big Five}}' Personality Traits},
  author = {Brown, Sarah and Taylor, Karl},
  year = {2014},
  month = dec,
  journal = {Journal of Economic Psychology},
  volume = {45},
  pages = {197--212},
  issn = {0167-4870},
  doi = {10.1016/j.joep.2014.10.006},
  urldate = {2023-06-23},
  abstract = {Using data drawn from the British Household Panel Survey, we analyse the relationship between personality traits and financial decision-making focusing on unsecured debt and financial assets. Personality traits are classified according to the `Big Five' taxonomy: openness to experience, conscientiousness, extraversion, agreeableness and neuroticism. We explore personality traits at the individual level and also within couples, specifically the personality traits of the head of household and personality traits averaged across the couple. We find that certain personality traits such as extraversion are generally significantly associated with household finances in terms of the levels of debt and assets held and the correlation is often relatively large. The results also suggest that the magnitude and statistical significance of the association between personality traits and household finances differs across the various types of debt and assets held in the household portfolio.},
  langid = {english},
  keywords = {Big Five personality traits,Financial assets,Financial vulnerability,Personality,Unsecured debt},
  file = {/home/luisaam/Zotero/storage/8DUX8LPL/Brown and Taylor - 2014 - Household finances and the âBig Fiveâ personality .pdf;/home/luisaam/Zotero/storage/2WEXWAQA/S016748701400083X.html}
}

@article{brownLanguageModelsAre2020,
  title = {Language {{Models}} Are {{Few-Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  journal = {arXiv:2005.14165 [cs]},
  eprint = {2005.14165},
  primaryclass = {cs},
  urldate = {2021-08-10},
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,examples},
  file = {/home/luisaam/Zotero/storage/QVKKWUVM/Brown et al. - 2020 - Language Models are Few-Shot Learners.pdf;/home/luisaam/Zotero/storage/NIEFC6YV/2005.html}
}

@article{brutzkusSGDLearnsOverparameterized2017,
  title = {{{SGD Learns Over-parameterized Networks}} That {{Provably Generalize}} on {{Linearly Separable Data}}},
  author = {Brutzkus, Alon and Globerson, Amir and Malach, Eran and {Shalev-Shwartz}, Shai},
  year = {2017},
  month = oct,
  journal = {arXiv:1710.10174 [cs]},
  eprint = {1710.10174},
  primaryclass = {cs},
  urldate = {2021-05-17},
  abstract = {Neural networks exhibit good generalization behavior in the over-parameterized regime, where the number of network parameters exceeds the number of observations. Nonetheless, current generalization bounds for neural networks fail to explain this phenomenon. In an attempt to bridge this gap, we study the problem of learning a two-layer over-parameterized neural network, when the data is generated by a linearly separable function. In the case where the network has Leaky ReLU activations, we provide both optimization and generalization guarantees for over-parameterized networks. Specifically, we prove convergence rates of SGD to a global minimum and provide generalization guarantees for this global minimum that are independent of the network size. Therefore, our result clearly shows that the use of SGD for optimization both finds a global minimum, and avoids overfitting despite the high capacity of the model. This is the first theoretical demonstration that SGD can avoid overfitting, when learning over-specified neural network classifiers.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/luisaam/Zotero/storage/IXLHMNCR/Brutzkus et al. - 2017 - SGD Learns Over-parameterized Networks that Provab.pdf;/home/luisaam/Zotero/storage/XDKXRI7S/1710.html}
}

@article{burovDistributionDirectionalChange2013,
  title = {Distribution of Directional Change as a Signature of Complex Dynamics},
  author = {Burov, Stanislav and Tabei, S. M. Ali and Huynh, Toan and Murrell, Michael P. and Philipson, Louis H. and Rice, Stuart A. and Gardel, Margaret L. and Scherer, Norbert F. and Dinner, Aaron R.},
  year = {2013},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {110},
  number = {49},
  pages = {19689--19694},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1319473110},
  urldate = {2022-03-14},
  keywords = {angle measurement,to read},
  file = {/home/luisaam/Zotero/storage/QVRMICIG/Burov et al. - 2013 - Distribution of directional change as a signature .pdf}
}

@article{c7e02b23e403e28570f6f61dbeaa6c5abb6744d5,
  title = {Optimal Market-{{Making}} Strategies under Synchronised Order Arrivals with Deep Neural Networks},
  author = {Choi, So Eun and Jang, Hyun Jin and Lee, Kyungsub and Zheng, Harry},
  year = {2021},
  journal = {Journal of Economic Dynamics and Control},
  volume = {125},
  pages = {104098},
  doi = {10.1016/J.JEDC.2021.104098},
  abstract = {Abstract This study investigates the optimal execution strategy of market-making for market and limit order arrival dynamics under a novel framework that includes a synchronised factor between buy and sell order arrivals. Using statistical tests, we empirically confirm that a synchrony propensity appears in the market, where a buy order arrival tends to follow the sell order's long-term mean level and vice versa. This is presumably closely related to the drastic increase in the influence of high-frequency trading activities in markets. To solve the high-dimensional Hamilton{\textendash}Jacobi{\textendash}Bellman equation, we propose a deep neural network approximation and theoretically verify the existence of a network structure that guarantees a sufficiently small loss function. Finally, we implement the terminal profit and loss profile of market-making using the estimated optimal strategy and compare its performance distribution with that of other feasible strategies. We find that our estimation of the optimal market-making placement allows significantly stable and steady profit accumulation over time through the implementation of strict inventory management.}
}

@article{c8ed74816c58a32dd87d809acac9388378104e32,
  title = {On the Human-Recognizability Phenomenon of Adversarially Trained Deep Image Classifiers},
  author = {Helland, Jonathan W. and VanHoudnos, Nathan M.},
  year = {2020},
  journal = {ArXiv},
  volume = {abs/2101.05219},
  abstract = {In this work, we investigate the phenomenon that robust image classifiers have human-recognizable features {\textendash} often referred to as interpretability {\textendash} as revealed through the input gradients of their score functions and their subsequent adversarial perturbations. In particular, we demonstrate that state-of-theart methods for adversarial training incorporate two terms {\textendash} one that orients the decision boundary via minimizing the expected loss, and another that induces smoothness of the classifier's decision surface by penalizing the local Lipschitz constant. Through this demonstration, we provide a unified discussion of gradient and Jacobian-based regularizers that have been used to encourage adversarial robustness in prior works. Following this discussion, we give qualitative evidence that the coupling of smoothness and orientation of the decision boundary is sufficient to induce the aforementioned human-recognizability phenomenon.},
  arxivid = {2101.05219}
}

@article{caiNSGAIILocalSearch2018,
  title = {{{NSGAII With Local Search Based Heavy Perturbation}} for {{Bi-Objective Weighted Clique Problem}}},
  author = {Cai, Dunbo and Gao, Yuhui and Yin, Minghao},
  year = {2018},
  journal = {IEEE Access},
  volume = {6},
  pages = {51253--51261},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2018.2869732},
  abstract = {This paper focuses on the clique problem of a weighted graph where weights of vertices can be either positive or negative (WG-PN). Two objectives, the size and the total weight of a clique, are considered. In particular, the larger size in terms of set inclusion and the higher total weight a clique is, the better it is. This problem is termed BiOWCP which is a multi-objective combinatorial optimization problem that contains conflict objectives. We show that BiOWCP cannot be translated into the clique problem of a weighted graph with positive vertex weights (WG-P) via a straight forward transformation, which highlights the difficulty of BiOWCP. We propose a heavy perturbation (HP)-based NSGAII (nondominated sorting genetic algorithm II) algorithm HP-NSGAII for the problem, where the perturbation is done by either improving a selected elitist with a local search procedure or swapping its left and right parts. Benchmarks for BiOWCP were adapted from the DIMACS set. Extensive experiments were carried out for different ratios of the heavy perturbation effort to the total search effort. Computational results show that HP-NSGAII is superior to NSGAII on many problems with respect to the hyper volume indicator and the set coverage indicator.},
  keywords = {Aerodynamics,Aerospace control,clique problem,Clique problem,evolutionary algorithm,graphs,Multi-objective,multi-objective combinatorial optimization,negative vertex weight,Optimization,Perturbation methods,Search problems,Sorting,Task analysis,weighted graph},
  file = {/home/luisaam/Zotero/storage/DQ3UTDTV/Cai et al. - 2018 - NSGAII With Local Search Based Heavy Perturbation .pdf;/home/luisaam/Zotero/storage/5U6C9YMI/8463462.html}
}

@article{CanPruningImprove,
  title = {Can Pruning Improve Certified Robustness of Neural Networks?. ({{arXiv}}:2206.07311v1 [Cs.{{LG}}])},
  journal = {arXiv Computer Science},
  doi = {arXiv:2206.07311v1},
  abstract = {With the rapid development of deep learning, the sizes of neural networks become larger and larger so that the training and inference often overwhelm the hardware resources. Given the fact that neural networks are often over-parameterized, one effective way to reduce such computational overhead is neural network pruning, by removing redundant parameters from trained neural networks. It has been recently observed that pruning can not only reduce computational overhead but also can improve empirical robustness of deep neural networks (NNs), potentially owing to removing spurious correlations while preserving the predictive accuracies. This paper for the first time demonstrates that pruning can generally improve certified robustness for ReLU-based NNs under the complete verification setting. Using the popular Branch-and-Bound (BaB) framework, we find that pruning can enhance the estimated bound tightness of certified robustness verification, by alleviating linear relaxation and sub-domain split problems. We empirically verify our findings with off-the-shelf pruning methods and further present a new stability-based pruning method tailored for reducing neuron instability, that outperforms existing pruning methods in enhancing certified robustness. Our experiments show that by appropriately pruning an NN, its certified accuracy can be boosted up to 8.2\% under standard training, and up to 24.5\% under adversarial training on the CIFAR10 dataset. We additionally observe the existence of certified lottery tickets that can match both standard and certified robust accuracies of the original dense models across different datasets. Our findings offer a new angle to study the intriguing interaction between sparsity and robustness, i.e. interpreting the interaction of sparsity and certified robustness via neuron stability. Codes are available at: https://github.com/VITA-Group/CertifiedPruning.},
  keywords = {Researcher App}
}

@article{carlWhatVecProbabilistically,
  title = {What the {{Vec}}? {{Towards Probabilistically Grounded Embeddings}}. ({{arXiv}}:1805.12164v2 [Cs.{{CL}}] {{UPDATED}})},
  author = {Carl, Allen and Ivana, Bala{\v z}evi{\'c} and Timothy, Hospedales},
  journal = {arXiv Computer Science},
  doi = {arXiv:1805.12164v2},
  abstract = {Word2Vec (W2V) and Glove are popular word embedding algorithms that perform well on a variety of natural language processing tasks. The algorithms are fast, efficient and their embeddings widely used. Moreover, the W2V algorithm has recently been adopted in the field of graph embedding, where it underpins several leading algorithms. However, despite their ubiquity and the relative simplicity of their common architecture, what the embedding parameters of W2V and Glove learn and why that it useful in downstream tasks largely remains a mystery. We show that different interactions of PMI vectors encode semantic properties that can be captured in low dimensional word embeddings by suitable projection, theoretically explaining why the embeddings of W2V and Glove work, and, in turn, revealing an interesting mathematical interconnection between the semantic relationships of relatedness, similarity, paraphrase and analogy.},
  keywords = {Researcher App}
}

@inproceedings{carreira-perpinanLearningCompressionAlgorithmsNeural2018,
  title = {"{{Learning-Compression}}" {{Algorithms}} for {{Neural Net Pruning}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {{Carreira-Perpinan}, Miguel A. and Idelbayev, Yerlan},
  year = {2018},
  month = jun,
  pages = {8532--8541},
  issn = {2575-7075},
  doi = {10.1109/CVPR.2018.00890},
  abstract = {Pruning a neural net consists of removing weights without degrading its performance. This is an old problem of renewed interest because of the need to compress ever larger nets so they can run in mobile devices. Pruning has been traditionally done by ranking or penalizing weights according to some criterion (such as magnitude), removing low-ranked weights and retraining the remaining ones. We formulate pruning as an optimization problem of finding the weights that minimize the loss while satisfying a pruning cost condition. We give a generic algorithm to solve this which alternates "learning" steps that optimize a regularized, data-dependent loss and "compression" steps that mark weights for pruning in a data-independent way. Magnitude thresholding arises naturally in the compression step, but unlike existing magnitude pruning approaches, our algorithm explores subsets of weights rather than committing irrevocably to a specific subset from the beginning. It is also able to learn automatically the best number of weights to prune in each layer of the net without incurring an exponentially costly model selection. Using a single pruning-level user parameter, we achieve state-of-the-art pruning in LeNet and ResNets of various sizes.},
  keywords = {Mobile handsets,Neural networks,Neurons,Optimization,Performance evaluation,Pruning,Quantization (signal),Training},
  file = {/home/luisaam/Zotero/storage/UQ3KEMPF/Carreira-Perpinan y Idelbayev - 2018 - Learning-Compression Algorithms for Neural Net P.pdf;/home/luisaam/Zotero/storage/328PLEK9/8578988.html}
}

@article{cartisAdaptiveCubicRegularisation2011,
  title = {Adaptive Cubic Regularisation Methods for Unconstrained Optimization. {{Part I}}: Motivation, Convergence and Numerical Results},
  shorttitle = {Adaptive Cubic Regularisation Methods for Unconstrained Optimization. {{Part I}}},
  author = {Cartis, Coralia and Gould, Nicholas I. M. and Toint, Philippe L.},
  year = {2011},
  month = apr,
  journal = {Mathematical Programming},
  volume = {127},
  number = {2},
  pages = {245--295},
  issn = {1436-4646},
  doi = {10.1007/s10107-009-0286-5},
  urldate = {2021-08-21},
  abstract = {An Adaptive Regularisation algorithm using Cubics (ARC) is proposed for unconstrained optimization, generalizing at the same time an unpublished method due to Griewank (Technical Report NA/12, 1981, DAMTP, University of Cambridge), an algorithm by Nesterov and Polyak (Math Program 108(1):177{\textendash}205, 2006) and a proposal by Weiser et~al. (Optim Methods Softw 22(3):413{\textendash}431, 2007). At each iteration of our approach, an approximate global minimizer of a local cubic regularisation of the objective function is determined, and this ensures a significant improvement in the objective so long as the Hessian of the objective is locally Lipschitz continuous. The new method uses an adaptive estimation of the local Lipschitz constant and approximations to the global model-minimizer which remain computationally-viable even for large-scale problems. We show that the excellent global and local convergence properties obtained by Nesterov and Polyak are retained, and sometimes extended to a wider class of problems, by our ARC approach. Preliminary numerical experiments with small-scale test problems from the CUTEr set show encouraging performance of the ARC algorithm when compared to a basic trust-region implementation.},
  langid = {english},
  keywords = {examples},
  file = {/home/luisaam/Zotero/storage/P9WTHUWS/Cartis et al. - 2011 - Adaptive cubic regularisation methods for unconstr.pdf}
}

@article{casteraSecondorderStepsizeTuning2021,
  title = {Second-Order Step-Size Tuning of {{SGD}} for Non-Convex Optimization},
  author = {Castera, Camille and Bolte, J{\'e}r{\^o}me and F{\'e}votte, C{\'e}dric and Pauwels, Edouard},
  year = {2021},
  month = nov,
  journal = {arXiv:2103.03570 [cs, math]},
  eprint = {2103.03570},
  primaryclass = {cs, math},
  urldate = {2021-12-30},
  abstract = {In view of a direct and simple improvement of vanilla SGD, this paper presents a fine-tuning of its step-sizes in the mini-batch case. For doing so, one estimates curvature, based on a local quadratic model and using only noisy gradient approximations. One obtains a new stochastic first-order method (Step-Tuned SGD), enhanced by second-order information, which can be seen as a stochastic version of the classical Barzilai-Borwein method. Our theoretical results ensure almost sure convergence to the critical set and we provide convergence rates. Experiments on deep residual network training illustrate the favorable properties of our approach. For such networks we observe, during training, both a sudden drop of the loss and an improvement of test accuracy at medium stages, yielding better results than SGD, RMSprop, or ADAM.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,to read},
  file = {/home/luisaam/Zotero/storage/IC882HNV/Castera et al. - 2021 - Second-order step-size tuning of SGD for non-conve.pdf;/home/luisaam/Zotero/storage/LCPZDXX6/2103.html}
}

@article{casteraSecondorderStepsizeTuning2021a,
  title = {Second-Order Step-Size Tuning of {{SGD}} for Non-Convex Optimization},
  author = {Castera, Camille and Bolte, J{\'e}r{\^o}me and F{\'e}votte, C{\'e}dric and Pauwels, Edouard},
  year = {2021},
  month = nov,
  journal = {arXiv:2103.03570 [cs, math]},
  eprint = {2103.03570},
  primaryclass = {cs, math},
  urldate = {2021-12-30},
  abstract = {In view of a direct and simple improvement of vanilla SGD, this paper presents a fine-tuning of its step-sizes in the mini-batch case. For doing so, one estimates curvature, based on a local quadratic model and using only noisy gradient approximations. One obtains a new stochastic first-order method (Step-Tuned SGD), enhanced by second-order information, which can be seen as a stochastic version of the classical Barzilai-Borwein method. Our theoretical results ensure almost sure convergence to the critical set and we provide convergence rates. Experiments on deep residual network training illustrate the favorable properties of our approach. For such networks we observe, during training, both a sudden drop of the loss and an improvement of test accuracy at medium stages, yielding better results than SGD, RMSprop, or ADAM.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {/home/luisaam/Zotero/storage/CKYIKSVS/Castera et al. - 2021 - Second-order step-size tuning of SGD for non-conve.pdf;/home/luisaam/Zotero/storage/SH9EN7TP/2103.html}
}

@article{chanMeasuringReliabilityReinforcement2020,
  title = {Measuring the {{Reliability}} of {{Reinforcement Learning Algorithms}}},
  author = {Chan, Stephanie C. Y. and Fishman, Samuel and Canny, John and Korattikara, Anoop and Guadarrama, Sergio},
  year = {2020},
  month = feb,
  journal = {arXiv:1912.05663 [cs, stat]},
  eprint = {1912.05663},
  primaryclass = {cs, stat},
  urldate = {2021-07-14},
  abstract = {Lack of reliability is a well-known issue for reinforcement learning (RL) algorithms. This problem has gained increasing attention in recent years, and efforts to improve it have grown substantially. To aid RL researchers and production users with the evaluation and improvement of reliability, we propose a set of metrics that quantitatively measure different aspects of reliability. In this work, we focus on variability and risk, both during training and after learning (on a fixed policy). We designed these metrics to be general-purpose, and we also designed complementary statistical tests to enable rigorous comparisons on these metrics. In this paper, we first describe the desired properties of the metrics and their design, the aspects of reliability that they measure, and their applicability to different scenarios. We then describe the statistical tests and make additional practical recommendations for reporting results. The metrics and accompanying statistical tools have been made available as an open-source library at https://github.com/google-research/rl-reliability-metrics. We apply our metrics to a set of common RL algorithms and environments, compare them, and analyze the results.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Reliability of Reinforcement Learning,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/5PC35SQH/Chan et al. - 2020 - Measuring the Reliability of Reinforcement Learnin.pdf;/home/luisaam/Zotero/storage/ZVWP4AKL/1912.html}
}

@inproceedings{chaoDirectionalPruningDeep2020,
  title = {Directional {{Pruning}} of {{Deep Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Chao, Shih-Kang and Wang, Zhanyu and Xing, Yue and Cheng, Guang},
  year = {2020},
  volume = {33},
  pages = {13986--13998},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-11-11},
  keywords = {Pruning},
  file = {/home/luisaam/Zotero/storage/9DMQ7JE5/Chao et al. - 2020 - Directional Pruning of Deep Neural Networks.pdf}
}

@inproceedings{chaudhariEntropysgdBiasingGradient2017,
  title = {Entropy-Sgd: {{Biasing}} Gradient Descent into Wide Valleys},
  booktitle = {5th International Conference on Learning Representations, {{ICLR}} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  author = {Chaudhari, Pratik and Choromanska, Anna and Soatto, Stefano and LeCun, Yann and Baldassi, Carlo and Borgs, Christian and Chayes, Jennifer T. and Sagun, Levent and Zecchina, Riccardo},
  year = {2017},
  publisher = {{OpenReview.net}},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/ChaudhariCSLBBC17.bib},
  keywords = {flat optimizers},
  timestamp = {Thu, 25 Jul 2019 14:26:02 +0200},
  file = {/home/luisaam/Zotero/storage/G3HFV56M/Chaudhari et al. - 2017 - Entropy-sgd Biasing gradient descent into wide va.pdf}
}

@inproceedings{chauvinBackPropagationAlgorithmOptimal1988,
  title = {A {{Back-Propagation Algorithm}} with {{Optimal Use}} of {{Hidden Units}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Chauvin, Yves},
  year = {1988},
  volume = {1},
  publisher = {{Morgan-Kaufmann}},
  urldate = {2022-12-18},
  abstract = {This  paper  presents  a  variation  of  the  back-propagation  algo(cid:173) rithm  that makes  optimal  use  of  a  network  hidden units  by  de(cid:173) cr{\textasciitilde}asing an  "energy"  term written  as  a  function  of  the  squared  activations  of  these  hidden units.  The  algorithm  can automati(cid:173) cally  find  optimal  or  nearly  optimal  architectures  necessary  to  solve  known  Boolean  functions,  facilitate  the  interpretation  of  the  activation  of  the  remaining  hidden  units  and  automatically  estimate the complexity of architectures appropriate for phonetic  labeling  problems.  The  general  principle  of the  algorithm  can  also be adapted to different tasks:  for  example,  it can be used to  eliminate the  [0,  0]  local minimum  of the  [-1.  +1]  logistic  acti(cid:173) vation  function  while  preserving  a  much  faster  convergence  and  forcing  binary  activations  over the  set of hidden  units.},
  keywords = {Pruning,Regularisation},
  file = {/home/luisaam/Zotero/storage/5UKRMM5S/Chauvin - 1988 - A Back-Propagation Algorithm with Optimal Use of H.pdf}
}

@misc{chengAdaptiveFactorizationNetwork2020,
  title = {Adaptive {{Factorization Network}}: {{Learning Adaptive-Order Feature Interactions}}},
  shorttitle = {Adaptive {{Factorization Network}}},
  author = {Cheng, Weiyu and Shen, Yanyan and Huang, Linpeng},
  year = {2020},
  month = jun,
  number = {arXiv:1909.03276},
  eprint = {1909.03276},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1909.03276},
  urldate = {2022-06-17},
  abstract = {Various factorization-based methods have been proposed to leverage second-order, or higher-order cross features for boosting the performance of predictive models. They generally enumerate all the cross features under a predefined maximum order, and then identify useful feature interactions through model training, which suffer from two drawbacks. First, they have to make a trade-off between the expressiveness of higher-order cross features and the computational cost, resulting in suboptimal predictions. Second, enumerating all the cross features, including irrelevant ones, may introduce noisy feature combinations that degrade model performance. In this work, we propose the Adaptive Factorization Network (AFN), a new model that learns arbitrary-order cross features adaptively from data. The core of AFN is a logarithmic transformation layer to convert the power of each feature in a feature combination into the coefficient to be learned. The experimental results on four real datasets demonstrate the superior predictive performance of AFN against the start-of-the-arts.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Retrieval,Computer Science - Machine Learning,feature interaction,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/CCJL94BR/Cheng et al. - 2020 - Adaptive Factorization Network Learning Adaptive-.pdf;/home/luisaam/Zotero/storage/H284SRWE/1909.html}
}

@article{chengSolvingIncrementalOptimization2019,
  title = {Solving {{Incremental Optimization Problems}} via {{Cooperative Coevolution}}},
  author = {Cheng, Ran and Omidvar, Mohammad Nabi and Gandomi, Amir H. and Sendhoff, Bernhard and Menzel, Stefan and Yao, Xin},
  year = {2019},
  month = oct,
  journal = {IEEE Transactions on Evolutionary Computation},
  volume = {23},
  number = {5},
  pages = {762--775},
  issn = {1089-778X, 1089-778X, 1941-0026},
  doi = {10.1109/TEVC.2018.2883599},
  urldate = {2021-05-18},
  abstract = {Engineering designs can involve multiple stages, where at each stage, the design models are incrementally modified and optimized. In contrast to traditional dynamic optimization problems where the changes are caused by some objective factors, the changes in such incremental optimization problems are usually caused by the modifications made by the decision makers during the design process. While existing work in the literature is mainly focused on traditional dynamic optimization, little research has been dedicated to solving such incremental optimization problems. In this work, we study how to adopt cooperative coevolution to efficiently solve a specific type of incremental optimization problems, namely, those with increasing decision variables. First, we present a benchmark function generator on the basis of some basic formulations of incremental optimization problems with increasing decision variables and exploitable modular structure. Then, we propose a contribution based cooperative coevolutionary framework coupled with an incremental grouping method for dealing with them. On one hand, the benchmark function generator is capable of generating various benchmark functions with various characteristics. On the other hand, the proposed framework is promising in solving such problems in terms of both optimization accuracy and computational efficiency. In addition, the proposed method is further assessed using a real-world application, i.e., the design optimization of a stepped cantilever beam.},
  langid = {english},
  keywords = {Incremental},
  file = {/home/luisaam/Zotero/storage/8W52QXIP/Cheng et al. - 2019 - Solving Incremental Optimization Problems via Coop.pdf}
}

@misc{chengWideDeepLearning2016,
  title = {Wide \& {{Deep Learning}} for {{Recommender Systems}}},
  author = {Cheng, Heng-Tze and Koc, Levent and Harmsen, Jeremiah and Shaked, Tal and Chandra, Tushar and Aradhye, Hrishi and Anderson, Glen and Corrado, Greg and Chai, Wei and Ispir, Mustafa and Anil, Rohan and Haque, Zakaria and Hong, Lichan and Jain, Vihan and Liu, Xiaobing and Shah, Hemal},
  year = {2016},
  month = jun,
  number = {arXiv:1606.07792},
  eprint = {1606.07792},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1606.07792},
  urldate = {2022-06-13},
  abstract = {Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide \& Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide \& Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow.},
  archiveprefix = {arxiv},
  keywords = {click through prediction,Computer Science - Information Retrieval,Computer Science - Machine Learning,feature interaction,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/M9XG6PJU/Cheng et al. - 2016 - Wide & Deep Learning for Recommender Systems.pdf;/home/luisaam/Zotero/storage/EUK4EFUA/1606.html}
}

@article{chenInformationtheoreticConsiderationsBatch2019,
  title = {Information-Theoretic Considerations in Batch Reinforcement Learning},
  author = {Chen, J. and Jiang, N.},
  year = {2019},
  journal = {36th International Conference on Machine Learning, ICML 2019},
  volume = {2019-},
  publisher = {{International Machine Learning Society (IMLS) rasmussen@ptd.net}},
  abstract = {Value-function approximation methods that operate in batch mode have foundational importance to reinforcement learning (RL). Finite sample guarantees for these methods often crucially rely on two types of assumptions: (1) mild distribution shift, and (2) representation conditions that arc stronger than realizability. However, the necessity ("why do we need them?") and the naturalness ("when do they hold?") of such assumptions have largely eluded the literature. In this paper, we revisit these assumptions and provide theoretical results towards answering the above questions, and make steps towards a deeper understanding of value-function approximation.}
}

@article{chenNovelMethodFinancial2022,
  title = {A Novel Method for Financial Distress Prediction Based on Sparse Neural Networks with \$\${{L}}\_\{1/2\}\$\$regularization},
  author = {Chen, Ying and Guo, Jifeng and Huang, Junqin and Lin, Bin},
  year = {2022},
  month = jul,
  journal = {International Journal of Machine Learning and Cybernetics},
  volume = {13},
  number = {7},
  pages = {2089--2103},
  issn = {1868-808X},
  doi = {10.1007/s13042-022-01566-y},
  urldate = {2022-05-25},
  abstract = {Corporate financial distress is related to the interests of the enterprise and stakeholders. Therefore, its accurate prediction is of great significance to avoid huge losses from them. Despite significant effort and progress in this field, the existing prediction methods are either limited by the number of input variables or restricted to those financial predictors. To alleviate those issues, both financial variables and non-financial variables are screened out from the existing accounting and finance theory to use as financial distress predictors. In addition, a novel method for financial distress prediction (FDP) based on sparse neural networks is proposed, namely FDP-SNN, in which the weight of the hidden layer is constrained with \$\$L\_\{1/2\}\$\$regularization to achieve the sparsity, so as to select relevant and important predictors, improving the predicted accuracy. It also provides support for the interpretability of the model. The results show that non-financial variables, such as investor protection and governance structure, play a key role in financial distress prediction than those financial ones, especially when the forecast period grows longer. By comparing those classic models proposed by predominant researchers in accounting and finance, the proposed model outperforms in terms of accuracy, precision, and AUC performance.},
  langid = {english},
  keywords = {Application,Features selection,finance,Financial distress prediction,L1-L2 regularization,sparse neural networks},
  file = {/home/luisaam/Zotero/storage/QPH3LV5N/Chen et al. - 2022 - A novel method for financial distress prediction b.pdf}
}

@inproceedings{chenOnlyTrainOnce2021,
  title = {Only {{Train Once}}: {{A One-Shot Neural Network Training And Pruning Framework}}},
  shorttitle = {Only {{Train Once}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Chen, Tianyi and Ji, Bo and Ding, Tianyu and Fang, Biyi and Wang, Guanyi and Zhu, Zhihui and Liang, Luming and Shi, Yixin and Yi, Sheng and Tu, Xiao},
  year = {2021},
  volume = {34},
  pages = {19637--19651},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-11-08},
  abstract = {Structured pruning is a commonly used technique in deploying deep neural networks (DNNs) onto resource-constrained devices. However, the existing pruning methods are usually heuristic, task-specified, and require an extra fine-tuning procedure. To overcome these limitations, we propose a framework that compresses DNNs into slimmer architectures with competitive performances and significant FLOPs reductions by Only-Train-Once (OTO). OTO contains two key steps: (i) we partition the parameters of DNNs into zero-invariant groups, enabling us to prune zero groups without affecting the output; and (ii) to promote zero groups, we then formulate a structured-sparsity optimization problem, and propose a novel optimization algorithm, Half-Space Stochastic Projected Gradient (HSPG), to solve it, which outperforms the standard proximal methods on group sparsity exploration, and maintains comparable convergence. To demonstrate the effectiveness of OTO, we train and compress full models simultaneously from scratch without fine-tuning for inference speedup and parameter reduction, and achieve state-of-the-art results on VGG16 for CIFAR10, ResNet50 for CIFAR10 and Bert for SQuAD and competitive result on ResNet50 for ImageNet. The source code is available at https://github.com/tianyic/onlytrainonce.},
  keywords = {grouping,Pruning},
  file = {/home/luisaam/Zotero/storage/E5DADY25/Chen et al. - 2021 - Only Train Once A One-Shot Neural Network Trainin.pdf}
}

@inproceedings{chenOptimizedNeuralNetwork2019,
  title = {Optimized {{Neural Network}} by {{Genetic Algorithm}} and {{Its Application}} in {{Fault Diagnosis}} of {{Three-level Inverter}}},
  booktitle = {2019 {{CAA Symposium}} on {{Fault Detection}}, {{Supervision}} and {{Safety}} for {{Technical Processes}} ({{SAFEPROCESS}})},
  author = {Chen, Danjiang and Liu, Yutian and Zhou, Junwei},
  year = {2019},
  month = jul,
  pages = {116--120},
  doi = {10.1109/SAFEPROCESS45799.2019.9213395},
  abstract = {Multilevel inverters have been widely applied in high-voltage and high-power applications. Therefore, fault diagnosis of such circuits is becoming more and more important. Fault diagnosis for single device open-circuit fault of three-level inverter based on BP (back propagation) neural network is studied in this paper. One of the weak-points of BP algorithm which is commonly used is that the optimal procedure is easily stacked into the local minimal value and cause strict demands of initial value. So a fault diagnosis method based on BP neural network and genetic algorithm (GA) is proposed in this paper. Firstly, bridge voltage of three-level inverter is collected as fault signal and feature is extracted to determine the structure of the BP neural network. After this, GA is applied to optimize the initial weights and thresholds of BP neural network, and then the network is trained to diagnose faults of three-level inverter to determine the specific failure device. The simulation result shows that the method can isolate fault modes proposed exactly, and the weak-point of network can effectively avoid, improve the diagnostic accuracy.},
  keywords = {Biological neural networks,Bridge circuits,Circuit faults,Fault diagnosis,genetic algorithm,Genetic algorithms,inverter,Inverters,neural network,three-level},
  file = {/home/luisaam/Zotero/storage/DRV6Y8W8/Chen et al. - 2019 - Optimized Neural Network by Genetic Algorithm and .pdf;/home/luisaam/Zotero/storage/4XDY64JX/9213395.html}
}

@article{chenWhenVisionTransformers2021,
  title = {When {{Vision Transformers Outperform ResNets}} without {{Pretraining}} or {{Strong Data Augmentations}}},
  author = {Chen, Xiangning and Hsieh, Cho-Jui and Gong, Boqing},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.01548 [cs]},
  eprint = {2106.01548},
  primaryclass = {cs},
  urldate = {2021-07-25},
  abstract = {Vision Transformers (ViTs) and MLPs signal further efforts on replacing hand-wired features or inductive biases with general-purpose neural architectures. Existing works empower the models by massive data, such as large-scale pretraining and/or repeated strong data augmentations, and still report optimization-related problems (e.g., sensitivity to initialization and learning rate). Hence, this paper investigates ViTs and MLP-Mixers from the lens of loss geometry, intending to improve the models' data efficiency at training and generalization at inference. Visualization and Hessian reveal extremely sharp local minima of converged models. By promoting smoothness with a recently proposed sharpness-aware optimizer, we substantially improve the accuracy and robustness of ViTs and MLP-Mixers on various tasks spanning supervised, adversarial, contrastive, and transfer learning (e.g., +5.3{\textbackslash}\% and +11.0{\textbackslash}\% top-1 accuracy on ImageNet for ViT-B/16 and Mixer-B/16, respectively, with the simple Inception-style preprocessing). We show that the improved smoothness attributes to sparser active neurons in the first few layers. The resultant ViTs outperform ResNets of similar size and throughput when trained from scratch on ImageNet without large-scale pretraining or strong data augmentations. They also possess more perceptive attention maps.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,SAM,small networks},
  file = {/home/luisaam/Zotero/storage/DMMQXDI7/Chen et al. - 2021 - When Vision Transformers Outperform ResNets withou.pdf;/home/luisaam/Zotero/storage/QH32QTMV/2106.html}
}

@inproceedings{choromanskaLossSurfacesMultilayer2015,
  title = {{The loss surfaces of multilayer networks}},
  booktitle = {{Journal of Machine Learning Research}},
  author = {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'e}rard Ben and LeCun, Yann},
  year = {2015},
  volume = {38},
  pages = {192--204},
  publisher = {{Microtome Publishing}},
  issn = {1532-4435},
  urldate = {2021-07-04},
  langid = {English (US)},
  keywords = {Loss Landscape},
  file = {/home/luisaam/Zotero/storage/YG9ELBC3/the-loss-surfaces-of-multilayer-networks-2.html}
}

@article{choromanskaLossSurfacesMultilayer2015a,
  title = {The {{Loss Surfaces}} of {{Multilayer Networks}}},
  author = {Choromanska, Anna and Henaff, Mikael and Mathieu, Michael and Arous, G{\'e}rard Ben and LeCun, Yann},
  year = {2015},
  month = jan,
  journal = {arXiv:1412.0233 [cs]},
  eprint = {1412.0233},
  primaryclass = {cs},
  urldate = {2021-06-22},
  abstract = {We study the connection between the highly non-convex loss function of a simple model of the fully-connected feed-forward neural network and the Hamiltonian of the spherical spin-glass model under the assumptions of: i) variable independence, ii) redundancy in network parametrization, and iii) uniformity. These assumptions enable us to explain the complexity of the fully decoupled neural network through the prism of the results from random matrix theory. We show that for large-size decoupled networks the lowest critical values of the random loss function form a layered structure and they are located in a well-defined band lower-bounded by the global minimum. The number of local minima outside that band diminishes exponentially with the size of the network. We empirically verify that the mathematical model exhibits similar behavior as the computer simulations, despite the presence of high dependencies in real networks. We conjecture that both simulated annealing and SGD converge to the band of low critical points, and that all critical points found there are local minima of high quality measured by the test error. This emphasizes a major difference between large- and small-size networks where for the latter poor quality local minima have non-zero probability of being recovered. Finally, we prove that recovering the global minimum becomes harder as the network size increases and that it is in practice irrelevant as global minimum often leads to overfitting.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Loss Landscape},
  file = {/home/luisaam/Zotero/storage/CWAXAMK7/Choromanska et al. - 2015 - The Loss Surfaces of Multilayer Networks.pdf;/home/luisaam/Zotero/storage/KC37TICK/1412.html}
}

@misc{chouMoreLessInducing2022,
  title = {More Is {{Less}}: {{Inducing Sparsity}} via {{Overparameterization}}},
  shorttitle = {More Is {{Less}}},
  author = {Chou, Hung-Hsu and Maly, Johannes and Rauhut, Holger},
  year = {2022},
  month = nov,
  number = {arXiv:2112.11027},
  eprint = {2112.11027},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2112.11027},
  urldate = {2022-12-21},
  abstract = {In deep learning it is common to overparameterize neural networks, that is, to use more parameters than training samples. Quite surprisingly training the neural network via (stochastic) gradient descent leads to models that generalize very well, while classical statistics would suggest overfitting. In order to gain understanding of this implicit bias phenomenon we study the special case of sparse recovery (compressed sensing) which is of interest on its own. More precisely, in order to reconstruct a vector from underdetermined linear measurements, we introduce a corresponding overparameterized square loss functional, where the vector to be reconstructed is deeply factorized into several vectors. We show that, if there exists an exact solution, vanilla gradient flow for the overparameterized loss functional converges to a good approximation of the solution of minimal \${\textbackslash}ell\_1\$-norm. The latter is well-known to promote sparse solutions. As a by-product, our results significantly improve the sample complexity for compressed sensing via gradient flow/descent on overparameterized models derived in previous works. The theory accurately predicts the recovery rate in numerical experiments. Our proof relies on analyzing a certain Bregman divergence of the flow. This bypasses the obstacles caused by non-convexity and should be of independent interest.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Information Theory,Computer Science - Machine Learning,Mathematics - Optimization and Control},
  file = {/home/luisaam/Zotero/storage/5NFEZE6N/Chou et al. - 2022 - More is Less Inducing Sparsity via Overparameteri.pdf;/home/luisaam/Zotero/storage/VZ8M62RM/2112.html}
}

@inproceedings{citeNeedCitation200007,
  title = {The Need of Citation},
  booktitle = {Adv. {{Conc}}. for {{Intell}}. {{Vis}}. {{Sys}}.: 9th {{Intl}}. {{Conf}}.},
  author = {Cite, Need},
  year = {200007},
  pages = {122--10000000}
}

@inproceedings{coatesSelectingReceptiveFields2011,
  title = {Selecting {{Receptive Fields}} in {{Deep Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Coates, Adam and Ng, Andrew},
  year = {2011},
  volume = {24},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-11-20},
  abstract = {Recent deep learning and unsupervised feature learning systems that learn from unlabeled data have achieved high performance in benchmarks by using extremely large architectures with many features (hidden units) at each layer.  Unfortunately, for such large architectures the number of parameters usually grows quadratically in the width of the network, thus necessitating hand-coded "local receptive fields" that limit the number of connections from lower level features to higher ones (e.g., based on spatial locality).  In this paper we propose a fast method to choose these connections that may be incorporated into a wide variety of unsupervised training methods.  Specifically, we choose local receptive fields that group together those low-level features that are most similar to each other according to a pairwise similarity metric.  This approach  allows us to harness the advantages of local receptive fields (such  as improved scalability, and reduced data requirements) when we do  not know how to specify such receptive fields by hand or where our  unsupervised training algorithm has no obvious generalization to a  topographic setting.  We produce results showing how this method allows us to use even simple unsupervised training algorithms to train successful multi-layered etworks that achieve state-of-the-art results on CIFAR and STL datasets: 82.0\% and 60.1\% accuracy, respectively.},
  keywords = {Receptive Field},
  file = {/home/luisaam/Zotero/storage/HPCMDI83/Coates and Ng - 2011 - Selecting Receptive Fields in Deep Networks.pdf}
}

@article{cobbeQuantifyingGeneralizationReinforcement2019,
  title = {Quantifying Generalization in Reinforcement Learning},
  author = {Cobbe, K. and Klimov, O. and Hesse, C. and Kim, T. and Schulman, J.},
  year = {2019},
  journal = {36th International Conference on Machine Learning, ICML 2019},
  volume = {2019-},
  publisher = {{International Machine Learning Society (IMLS) rasmussen@ptd.net}},
  abstract = {In this paper, we investigate the problem of over-fitting in deep reinforcement learning. Among the most common benchmarks in RL, it is customary to use the same environments for both training and testing. This practice offers relatively little insight into an agent's ability to generalize. We address this issue by using procedurally generated environments to construct distinct training and test sets. Most notably, we introduce a new environment called CoinRun, designed as a benchmark for generalization in RL. Using CoinRun, we find that agents overfit to surprisingly large training sets. We then show that deeper convolutional architectures improve generalization, as do methods traditionally found in supervised learning, including L2 regularization, dropout, data augmentation and batch normalization.},
  file = {/home/luisaam/Zotero/storage/M35X3Q2M/Cobbe et al. - 2019 - Quantifying generalization in reinforcement learning.pdf}
}

@article{CodedResNeXtNetwork,
  title = {Coded {{ResNeXt}}: A Network for Designing Disentangled Information Paths. ({{arXiv}}:2202.05343v1 [Cs.{{CV}}])},
  journal = {arXiv Computer Science},
  doi = {arXiv:2202.05343v1},
  abstract = {To avoid treating neural networks as highly complex black boxes, the deep learning research community has tried to build interpretable models allowing humans to understand the decisions taken by the model. Unfortunately, the focus is mostly on manipulating only the very high-level features associated with the last layers. In this work, we look at neural network architectures for classification in a more general way and introduce an algorithm which defines before the training the paths of the network through which the per-class information flows. We show that using our algorithm we can extract a lighter single-purpose binary classifier for a particular class by removing the parameters that do not participate in the predefined information path of that class, which is approximately 60\% of the total parameters. Notably, leveraging coding theory to design the information paths enables us to use intermediate network layers for making early predictions without having to evaluate the full network. We demonstrate that a slightly modified ResNeXt model, trained with our algorithm, can achieve higher classification accuracy on CIFAR-10/100 and ImageNet than the original ResNeXt, while having all the aforementioned properties.},
  keywords = {Researcher App}
}

@article{colasCURIOUSIntrinsicallyMotivated2019,
  title = {{{CURIOUS}}: {{Intrinsically}} Motivated Modular Multi-Goal Reinforcement Learning},
  author = {Colas, C. and Oudeyer, P. and Founder, P. and Sigaud, O. and Chetouani, M.},
  year = {2019},
  journal = {36th International Conference on Machine Learning, ICML 2019},
  volume = {2019-},
  publisher = {{International Machine Learning Society (IMLS) rasmussen@ptd.net}},
  abstract = {In open-ended environments, autonomous learning agents must set their own goals and build their own curriculum through an intrinsically motivated exploration. They may consider a large diversity of goals, aiming to discover what is controllable in their environments, and what is not. Because some goals might prove easy and some impossible, agents must actively select which goal to practice at any moment, to maximize their overall mastery on the set of learnable goals. This paper proposes CURIOUS, an algorithm that leverages 1) a modular Universal Value Function Approximator with hindsight learning to achieve a diversity of goals of different kinds within a unique policy and 2) an automated curriculum learning mechanism that biases the attention of the agent towards goals maximizing the absolute learning progress. Agents focus sequentially on goals of increasing complexity, and focus back on goals that are being forgotten. Experiments conducted in a new modular-goal robotic environment show the resulting developmental self-organization of a learning curriculum, and demonstrate properties of robustness to distracting goals, forgetting and changes in body properties.}
}

@article{ComparativeStudyCNN,
  title = {A {{Comparative Study}} of {{CNN-}} and {{Transformer-Based Visual Style Transfer}}},
  journal = {Journal of Computer Science and Technology},
  doi = {10.1007/s11390-022-2140-7},
  abstract = {Vision Transformer has shown impressive performance on the image classification tasks. Observing that most existing visual style transfer (VST) algorithms are based on the texture-biased convolution neural network (CNN), here raises the question of whether the shape-biased Vision Transformer can perform style transfer as CNN. In this work, we focus on comparing and analyzing the shape bias between CNN- and transformer-based models from the view of VST tasks. For comprehensive comparisons, we propose three kinds of transformer-based visual style transfer (Tr-VST) methods (Tr-NST for optimization-based VST, Tr-WCT for reconstruction-based VST and Tr-AdaIN for perceptual-based VST). By engaging three mainstream VST methods in the transformer pipeline, we show that transformer-based models pre-trained on ImageNet are not proper for style transfer methods. Due to the strong shape bias of the transformer-based models, these Tr-VST methods cannot render style patterns. We further analyze the shape bias by considering the inuence of the learned parameters and the structure design. Results prove that with proper style supervision, the transformer can learn similar texture-biased features as CNN does. With the reduced shape bias in the transformer encoder, Tr-VST methods can generate higher-quality results compared with state-of-the-art VST methods.},
  keywords = {Researcher App}
}

@article{ConstructingNeuralNetwork,
  title = {Constructing Neural Network Models from Brain Data Reveals Representational Transformations Linked to Adaptive Behavior},
  journal = {Nature Communications},
  doi = {10.1038/s41467-022-28323-7},
  abstract = {Nature Communications, Published online: 03 February 2022; doi:10.1038/s41467-022-28323-7  The brain dynamically transforms cognitive information. Here the authors build task-performing, functioning neural network models of sensorimotor transformations constrained by human brain data without the use of typical deep learning techniques.},
  keywords = {Researcher App}
}

@inproceedings{contiImprovingExplorationEvolution2018,
  title = {Improving {{Exploration}} in {{Evolution Strategies}} for {{Deep Reinforcement Learning}} via a {{Population}} of {{Novelty-Seeking Agents}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Conti, Edoardo and Madhavan, Vashisht and Petroski Such, Felipe and Lehman, Joel and Stanley, Kenneth and Clune, Jeff},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-03-18},
  keywords = {novelty seeking},
  file = {/home/luisaam/Zotero/storage/P6GZPNJV/Conti et al. - 2018 - Improving Exploration in Evolution Strategies for .pdf}
}

@article{ContinualLearningDynamic,
  title = {Continual {{Learning}} with {{Dynamic Sparse Training}}: {{Exploring Algorithms}} for {{Effective Model Updates}}. ({{arXiv}}:2308.14831v1 [Cs.{{LG}}])},
  journal = {arXiv Computer Science},
  doi = {arXiv:2308.14831v1},
  abstract = {Continual learning (CL) refers to the ability of an intelligent system to sequentially acquire and retain knowledge from a stream of data with as little computational overhead as possible. To this end; regularization, replay, architecture, and parameter isolation approaches were introduced to the literature. Parameter isolation using a sparse network which enables to allocate distinct parts of the neural network to different tasks and also allows to share of parameters between tasks if they are similar. Dynamic Sparse Training (DST) is a prominent way to find these sparse networks and isolate them for each task. This paper is the first empirical study investigating the effect of different DST components under the CL paradigm to fill a critical research gap and shed light on the optimal configuration of DST for CL if it exists. Therefore, we perform a comprehensive study in which we investigate various DST components to find the best topology per task on well-known CIFAR100 and miniImageNet benchmarks in a task-incremental CL setup since our primary focus is to evaluate the performance of various DST criteria, rather than the process of mask selection. We found that, at a low sparsity level, Erdos-Renyi Kernel (ERK) initialization utilizes the backbone more efficiently and allows to effectively learn increments of tasks. At a high sparsity level, however, uniform initialization demonstrates more reliable and robust performance. In terms of growth strategy; performance is dependent on the defined initialization strategy, and the extent of sparsity. Finally, adaptivity within DST components is a promising way for better continual learners.},
  keywords = {Researcher App}
}

@article{ControlAdaptiveQlearning,
  title = {Control with Adaptive {{Q-learning}}: {{A}} Comparison for Two Classical Control Problems},
  journal = {Engineering Applications of Artificial Intelligence},
  doi = {10.1016/j.engappai.2022.104797},
  abstract = {This paper evaluates   adaptive Q-learning   (AQL) and   single-partition adaptive Q-learning   (SPAQL), two algorithms for efficient model-free episodic reinforcement learning (RL), in two classical control problems (  Pendulum  ~and   CartPole  ). AQL adaptively partitions the state{\textendash}action space of a   Markov decision process   (MDP), while learning the control policy,   i.e.  , the mapping from states to actions. The main difference between AQL and SPAQL is that the latter learns time-invariant policies, where the mapping from states to actions does not depend explicitly on the time step. This paper also proposes the SPAQL with terminal state (SPAQL-TS), an improved version of SPAQL tailored for the design of regulators for control problems. The time-invariant policies are shown to result in a better performance than the time-variant ones in both problems studied. These algorithms are particularly fitted to RL problems where the action space is finite, as is the case with the   CartPole  ~problem. SPAQL-TS solves the   OpenAI Gym  ~  CartPole  ~problem, while also displaying a higher sample efficiency than   trust region policy optimization   (TRPO), a standard RL algorithm for solving control tasks. Moreover, the policies learned by SPAQL are interpretable, while TRPO policies are typically encoded as neural networks, and therefore hard to interpret. Yielding interpretable policies while being sample-efficient are the major advantages of SPAQL.  The code for the experiments is available at https://github.com/jaraujo98/SinglePartitionAdaptiveQLearning.},
  keywords = {Researcher App}
}

@misc{curciTrulySparseNeural2022,
  title = {Truly {{Sparse Neural Networks}} at {{Scale}}},
  author = {Curci, Selima and Mocanu, Decebal Constantin and Pechenizkiyi, Mykola},
  year = {2022},
  month = jul,
  number = {arXiv:2102.01732},
  eprint = {2102.01732},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2102.01732},
  urldate = {2022-07-15},
  abstract = {Recently, sparse training methods have started to be established as a de facto approach for training and inference efficiency in artificial neural networks. Yet, this efficiency is just in theory. In practice, everyone uses a binary mask to simulate sparsity since the typical deep learning software and hardware are optimized for dense matrix operations. In this paper, we take an orthogonal approach, and we show that we can train truly sparse neural networks to harvest their full potential. To achieve this goal, we introduce three novel contributions, specially designed for sparse neural networks: (1) a parallel training algorithm and its corresponding sparse implementation from scratch, (2) an activation function with non-trainable parameters to favour the gradient flow, and (3) a hidden neurons importance metric to eliminate redundancies. All in one, we are able to break the record and to train the largest neural network ever trained in terms of representational power -- reaching the bat brain size. The results show that our approach has state-of-the-art performance while opening the path for an environmentally friendly artificial intelligence era.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Pruning},
  file = {/home/luisaam/Zotero/storage/QI6TSL62/Curci et al. - 2022 - Truly Sparse Neural Networks at Scale.pdf;/home/luisaam/Zotero/storage/GTQXHHMJ/2102.html}
}

@article{cuturiDifferentiableRanksSorting2019,
  title = {Differentiable {{Ranks}} and {{Sorting}} Using {{Optimal Transport}}},
  author = {Cuturi, Marco and Teboul, Olivier and Vert, Jean-Philippe},
  year = {2019},
  month = may,
  abstract = {Sorting an array is a fundamental routine in machine learning, one that is used to compute rank-based statistics, cumulative distribution functions (CDFs), quantiles, or to select closest neighbors and labels. The sorting function is however piece-wise constant (the sorting permutation of a vector does not change if the entries of that vector are infinitesimally perturbed) and therefore has no gradient information to back-propagate. We propose a framework to sort elements that is algorithmically differentiable. We leverage the fact that sorting can be seen as a particular instance of the optimal transport (OT) problem on \${\textbackslash}backslashmathbb\{R\}\$, from input values to a predefined array of sorted values (e.g. \$1,2,{\textbackslash}backslashdots,n\$ if the input array has \$n\$ elements). Building upon this link , we propose generalized CDFs and quantile operators by varying the size and weights of the target presorted array. Because this amounts to using the so-called Kantorovich formulation of OT, we call these quantities K-sorts, K-CDFs and K-quantiles. We recover differentiable algorithms by adding to the OT problem an entropic regularization, and approximate it using a few Sinkhorn iterations. We call these operators S-sorts, S-CDFs and S-quantiles, and use them in various learning settings: we benchmark them against the recently proposed neuralsort [Grover et al. 2019], propose applications to quantile regression and introduce differentiable formulations of the top-k accuracy that deliver state-of-the art performance.},
  file = {/home/luisaam/Zotero/storage/KLEBVII4/Cuturi, Teboul, Vert - 2019 - Differentiable Ranks and Sorting using Optimal Transport.pdf}
}

@article{cybenkoApproximationSuperpositionsSigmoidal1989,
  title = {Approximation by Superpositions of a Sigmoidal Function},
  author = {Cybenko, G.},
  year = {1989},
  month = dec,
  journal = {Mathematics of Control, Signals and Systems},
  volume = {2},
  number = {4},
  pages = {303--314},
  issn = {1435-568X},
  doi = {10.1007/BF02551274},
  urldate = {2021-07-02},
  abstract = {In this paper we demonstrate that finite linear combinations of compositions of a fixed, univariate function and a set of affine functionals can uniformly approximate any continuous function ofn real variables with support in the unit hypercube; only mild conditions are imposed on the univariate function. Our results settle an open question about representability in the class of single hidden layer neural networks. In particular, we show that arbitrary decision regions can be arbitrarily well approximated by continuous feedforward neural networks with only a single internal, hidden layer and any continuous sigmoidal nonlinearity. The paper discusses approximation properties of other possible types of nonlinearities that might be implemented by artificial neural networks.},
  langid = {english},
  keywords = {NN as universal approximators},
  file = {/home/luisaam/Zotero/storage/H5ANSXLS/Cybenko - 1989 - Approximation by superpositions of a sigmoidal fun.pdf}
}

@article{czarneckiSobolevTrainingNeural2017,
  title = {Sobolev Training for Neural Networks},
  author = {Czarnecki, Wojciech M and Osindero, Simon and Jaderberg, Max and Swirszcz, Grzegorz and Pascanu, Razvan},
  year = {2017},
  journal = {Advances in Neural Information Processing Systems},
  volume = {30},
  keywords = {Jacobian regularisation,Sobolev training}
}

@techreport{czarneckiUnderstandingSyntheticGradients,
  title = {Understanding {{Synthetic Gradients}} and {{Decoupled Neural Interfaces}}},
  author = {Czarnecki, Wojciech Marian and Grzegorz{\textasciiacute} Grzegorz{\textasciiacute}swirszcz, Grzegorz{\textasciiacute}swirszcz and Jaderberg, Max and Osindero, Simon and Vinyals, Oriol and Kavukcuoglu, Koray},
  abstract = {When training neural networks, the use of Synthetic Gradients (SG) allows layers or modules to be trained without update locking-without waiting for a true error gradient to be backprop-agated-resulting in Decoupled Neural Interfaces (DNIs). This unlocked ability of being able to update parts of a neural network asyn-chronously and with only local information was demonstrated to work empirically in Jaderberg et al. (2016). However, there has been very little demonstration of what changes DNIs and SGs impose from a functional, representational, and learning dynamics point of view. In this paper, we study DNIs through the use of synthetic gradients on feed-forward networks to better understand their behaviour and elucidate their effect on optimisation. We show that the incorporation of SGs does not affect the representational strength of the learning system for a neural network , and prove the convergence of the learning system for linear and deep linear models. On practical problems we investigate the mechanism by which synthetic gradient estimators approximate the true loss, and, surprisingly, how that leads to drastically different layer-wise representations. Finally, we also expose the relationship of using synthetic gradients to other error approximation techniques and find a unifying language for discussion and comparison.},
  file = {/home/luisaam/Zotero/storage/7UEB465Y/Czarnecki et al. - Unknown - Understanding Synthetic Gradients and Decoupled Neural Interfaces.pdf}
}

@inproceedings{daavaraniaslNewApproachMultiagent2017,
  title = {A New Approach on Multi-Agent {{Multi-Objective Reinforcement Learning}} Based on Agents' Preferences},
  booktitle = {19th {{CSI International Symposium}} on {{Artificial Intelligence}} and {{Signal Processing}}, {{AISP}} 2017},
  author = {Daavarani Asl, Zeinab and Derhami, Vali and {Yazdian-Dehkordi}, Mehdi},
  year = {2017},
  pages = {75--79},
  doi = {10.1109/AISP.2017.8324111},
  abstract = {Reinforcement Learning (RL) is a powerful machine learning paradigm for solving Markov Decision Process (MDP). Traditional RL algorithms aim to solve one-objective problems, but many real-world problems have more than one objective which conflict each other. In recent years, Multi-Objective Reinforcement Learning (MORL) algorithms, which employ a reward vector instead of a scalar reward signal, have been proposed to solve multi-objective problems. In MORL, because of conflicting objectives, there is no one optimal solution and a set of solutions named Pareto Front will be learned. In this paper, we proposed a new multi-agent method, which uses a shared Q-table for all agents to solve bi-objective problems. However, each agent selects actions based on its preference. These preferences are different with each other and the agents reach to Pareto Front solutions based on this preferences. The proposed method is simple in understanding and its computational cost is very low. Moreover, after finding the Pareto Front set, we can easily track the policy. Simulation results show that our proposed method outperforms the available methods in the term of learning speed.},
  isbn = {978-1-5386-2585-9},
  keywords = {multi-agent systems,multi-objective,Pareto Front,Reinforcement Learning},
  file = {/home/luisaam/Zotero/storage/7ICJ76LP/Daavarani Asl, Derhami, Yazdian-Dehkordi - 2017 - A new approach on multi-agent Multi-Objective Reinforcement Learning based on agents'.pdf}
}

@article{daiCoAtNetMarryingConvolution2021,
  title = {{{CoAtNet}}: {{Marrying}} Convolution and Attention for All Data Sizes},
  author = {Dai, Zihang and Liu, Hanxiao and Le, Quoc V and Tan, Mingxing},
  year = {2021},
  journal = {arXiv preprint arXiv:2106.04803},
  eprint = {2106.04803},
  archiveprefix = {arxiv}
}

@misc{daiDiscriminativeEmbeddingsLatent2020,
  title = {Discriminative {{Embeddings}} of {{Latent Variable Models}} for {{Structured Data}}},
  author = {Dai, Hanjun and Dai, Bo and Song, Le},
  year = {2020},
  month = jan,
  number = {arXiv:1603.05629},
  eprint = {1603.05629},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1603.05629},
  urldate = {2022-08-11},
  abstract = {Kernel classifiers and regressors designed for structured data, such as sequences, trees and graphs, have significantly advanced a number of interdisciplinary areas such as computational biology and drug design. Typically, kernels are designed beforehand for a data type which either exploit statistics of the structures or make use of probabilistic generative models, and then a discriminative classifier is learned based on the kernels via convex optimization. However, such an elegant two-stage approach also limited kernel methods from scaling up to millions of data points, and exploiting discriminative information to learn feature representations. We propose, structure2vec, an effective and scalable approach for structured data representation based on the idea of embedding latent variable models into feature spaces, and learning such feature spaces using discriminative information. Interestingly, structure2vec extracts features by performing a sequence of function mappings in a way similar to graphical model inference procedures, such as mean field and belief propagation. In applications involving millions of data points, we showed that structure2vec runs 2 times faster, produces models which are \$10,000\$ times smaller, while at the same time achieving the state-of-the-art predictive performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,struc2vec},
  file = {/home/luisaam/Zotero/storage/543VJBSP/Dai et al. - 2020 - Discriminative Embeddings of Latent Variable Model.pdf;/home/luisaam/Zotero/storage/N7WHF49Z/1603.html}
}

@inproceedings{dasLearningCooperativeVisual2017,
  title = {Learning {{Cooperative Visual Dialog Agents}} with {{Deep Reinforcement Learning}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Das, Abhishek and Kottur, Satwik and Moura, Jose M.F. and Lee, Stefan and Batra, Dhruv},
  year = {2017},
  month = dec,
  volume = {2017-October},
  pages = {2970--2979},
  publisher = {{Institute of Electrical and Electronics Engineers Inc.}},
  issn = {15505499},
  doi = {10.1109/ICCV.2017.321},
  abstract = {We introduce the first goal-driven training for visual question answering and dialog agents. Specifically, we pose a cooperative 'image guessing' game between two agents {\textendash} Qbot and Abot {\textendash} who communicate in natural language dialog so that Qbot can select an unseen image from a lineup of images. We use deep reinforcement learning (RL) to learn the policies of these agents end-to-end {\textendash} from pixels to multi-agent multi-round dialog to game reward. We demonstrate two experimental results. First, as a 'sanity check' demonstration of pure RL (from scratch), we show results on a synthetic world, where the agents communicate in ungrounded vocabulary, i.e., symbols with no pre-specified meanings (X, Y, Z). We find that two bots invent their own communication protocol and start using certain symbols to ask/answer about certain visual attributes (shape/color/style). Thus, we demonstrate the emergence of grounded language and communication among 'visual' dialog agents with no human supervision. Second, we conduct large-scale real-image experiments on the VisDial dataset, where we pretrain with supervised dialog data and show that the RL 'fine-tuned' agents significantly outperform SL agents. Interestingly, the RL Qbot learns to ask questions that Abot is good at, ultimately resulting in more informative dialog and a better team.},
  isbn = {978-1-5386-1032-9},
  file = {/home/luisaam/Zotero/storage/4PRN6IFM/das2017.pdf}
}

@article{DataPresentationHow,
  title = {Data {{Presentation}}: {{How}} to {{Organize Your Tables}} and {{Figures}}},
  journal = {Edanz - Innovative Scientific Solutions},
  doi = {isBfV0oQLPpoSvgnTl3o},
  abstract = {MAR 14    Data Presentation: How to Organize Your Tables and Figures    10:00 am - 11:00 am GMT+0      FREE          ~        J  oin us on 14th March at 10am GMT for the next episode in   Edanz  ' series focused on early career researchers!    ~    So far in this series, we have selected a journal and checked its requirements (  Episode 1  ), started our first draft (  Episode 2  ), and learned how to avoid many common manuscript mistakes (  Episode 3  ).    ~    Welcome to Episode 4 -- now it's time to present our~  DATA  !~    ~    But which types of figures should we use, and where and how? What are some DOs and DON'Ts for visualizing data? ~    ~    Research publishing expert~  Dr Dean Meyer  ~will share her practical, time-saving tips for drafting tables, graphs, charts, legends, and more!    ~    Want your data to be clearly understood by readers? Want to be accepted by your target journal? You DON'T want to miss this event.  ~    ~    Join us! As always, we invite YOUR questions and comments. See you there!    ~    The slides for this event can be found   here  .~    ~    All Researcher users get a   free one-year membership   to My Edanz Premium (usually \$120) - sign up   here  .    ~    To receive updates for Edanz' events - follow Edanz by clicking   here   and selecting 'follow'.    ~    An events schedule can be found   here  .    ~    If you'd like to host your own Researcher Live event, please email   kristine.lennie@researcher-app.com  ~    ~    ~          Date and Time    Monday, March 14, 2022 10:00 AM 10:00 am - 11:00 am GMT+0          Speakers        Dr Dean Meyer    Dr Dean has a background in environmental science with a specialist interest in toxicology and public health. Her doctoral research work focused on molecular mechanisms of metal detoxification in an invertebrate model. Her other research interests include the mechanisms of toxicity and disease causation, and the occupational sources of xenobiotics and their physiological effects. Dr Meyer spent eight years working at the Centers for Disease Control and Prevention in Atlanta, and has an extensive background in the areas of laboratory safety and environmental health. Dr Meyer is a certified Editor in the Life Sciences (ELS) and joined Edanz as an editor in 2015.  ~    ,     Scott McCleary    Scott specializes in online instructional design for lifelong learning. He has developed and delivered training programs for more than 40 organizations worldwide in fields such as medicine \& pharma (Merck, Pfizer, ~Banyu, NCGM hospital), business (Deutsche Bank, Skandia, LEC Tokyo), technology (Sony, Toshiba, Hitachi, Mitsubishi), government (The Japan Ministry of Foreign Affairs), K-12 (Benesse, Ochanomizu Seminar) and higher education (Komazawa University, Gakushuin University, Asia University, Kaetsu University, and the Berlin School of Economics and Law).},
  keywords = {Researcher App}
}

@article{dattnerHowCurateYour2020,
  title = {How to {{Curate Your Digital Persona}}},
  author = {Dattner, Ben and {Chamorro-Premuzic}, Tomas},
  year = {2020},
  month = jul,
  journal = {Harvard Business Review},
  issn = {0017-8012},
  urldate = {2023-07-07},
  abstract = {If you are looking for a new job, interested in boosting your personal brand, enhancing your career, or simply trying to improve your understanding of how other people see you, it is important to become aware of the story your public data tells {\textemdash} and understand how to change it. A common myth is that algorithms are impossible to fool or trick. But if you understand the formula they are using to analyze you, you can modify your behaviors to curate the image you want others to see, including: your photographs, videos, tone of voice, the words you use to communicate, and social media posts, shares, and likes.},
  chapter = {Hiring and recruitment},
  keywords = {Cybersecurity and digital privacy,Hiring and recruitment,Web-based technologies},
  file = {/home/luisaam/Zotero/storage/DI777DW7/how-to-curate-your-digital-persona.html}
}

@article{dauphinIdentifyingAttackingSaddle2014,
  title = {Identifying and Attacking the Saddle Point Problem in High-Dimensional Non-Convex Optimization},
  author = {Dauphin, Yann N. and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  year = {2014},
  journal = {Advances in Neural Information Processing Systems},
  volume = {27},
  urldate = {2021-06-22},
  langid = {english},
  file = {/home/luisaam/Zotero/storage/ZLEZ962C/Dauphin et al. - 2014 - Identifying and attacking the saddle point problem.pdf;/home/luisaam/Zotero/storage/8N8KRPPS/17e23e50bedc63b4095e3d8204ce063b-Abstract.html}
}

@misc{DeepLearningDiminishing2021,
  title = {Deep {{Learning}}'s {{Diminishing Returns}}},
  year = {2021},
  month = sep,
  journal = {IEEE Spectrum},
  urldate = {2022-01-11},
  abstract = {The cost of improvement is becoming unsustainable},
  chapter = {Artificial Intelligence},
  howpublished = {https://spectrum.ieee.org/deep-learning-computational-cost},
  langid = {english},
  file = {/home/luisaam/Zotero/storage/NHAV9L4F/deep-learning-computational-cost.html}
}

@article{DeepLearningModel,
  title = {Deep {{Learning Model}} with {{GA}} Based {{Feature Selection}} and {{Context Integration}}. ({{arXiv}}:2204.06189v1 [Cs.{{CV}}])},
  journal = {arXiv Computer Vision and Pattern Recognition},
  doi = {arXiv:2204.06189v1},
  abstract = {Deep learning models have been very successful in computer vision and image processing applications. Since its inception, Many top-performing methods for image segmentation are based on deep CNN models. However, deep CNN models fail to integrate global and local context alongside visual features despite having complex multi-layer architectures. We propose a novel three-layered deep learning model that assiminlate or learns independently global and local contextual information alongside visual features. The novelty of the proposed model is that One-vs-All binary class-based learners are introduced to learn Genetic Algorithm (GA) optimized features in the visual layer, followed by the contextual layer that learns global and local contexts of an image, and finally the third layer integrates all the information optimally to obtain the final class label. Stanford Background and CamVid benchmark image parsing datasets were used for our model evaluation, and our model shows promising results. The empirical analysis reveals that optimized visual features with global and local contextual information play a significant role to improve accuracy and produce stable predictions comparable to state-of-the-art deep CNN models.},
  keywords = {Researcher App}
}

@article{DeepReinforcementLearning,
  title = {Deep {{Reinforcement Learning}}: {{A Brief Survey}}},
  journal = {IEEE Signal Processing Magazine},
  doi = {10.1109/MSP.2017.2743240},
  abstract = {Deep reinforcement learning (DRL) is poised to revolutionize the field of artificial intelligence (AI) and represents a step toward building autonomous systems with a higher-level understanding of the visual world. Currently, deep learning is enabling reinforcement learning (RL) to scale to problems that were previously intractable, such as learning to play video games directly from pixels. DRL algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of RL, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep RL, including the deep Q-network (DQN), trust region policy optimization (TRPO), and asynchronous advantage actor critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via RL. To conclude, we describe several current areas of research within the field.},
  keywords = {Researcher App}
}

@inproceedings{dengImageNetLargescaleHierarchical2009,
  title = {{{ImageNet}}: {{A}} Large-Scale Hierarchical Image Database},
  shorttitle = {{{ImageNet}}},
  booktitle = {2009 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and {Fei-Fei}, Li},
  year = {2009},
  month = jun,
  pages = {248--255},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2009.5206848},
  abstract = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called ``ImageNet'', a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500{\textendash}1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  keywords = {Datasets,Explosions,Image databases,Image Recognition,Image retrieval,Information retrieval,Internet,Large-scale systems,Multimedia databases,Ontologies,Robustness,Spine},
  file = {/home/luisaam/Zotero/storage/8BK2ZJPY/Deng et al. - 2009 - ImageNet A large-scale hierarchical image databas.pdf;/home/luisaam/Zotero/storage/IXU7M23F/5206848.html}
}

@inproceedings{dentonExploitingLinearStructure2014,
  title = {Exploiting {{Linear Structure Within Convolutional Networks}} for {{Efficient Evaluation}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Denton, Emily L and Zaremba, Wojciech and Bruna, Joan and LeCun, Yann and Fergus, Rob},
  year = {2014},
  volume = {27},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-03-24},
  abstract = {We present techniques for speeding up the test-time evaluation of large convolutional networks, designed for object recognition tasks. These models deliver impressive accuracy, but each image evaluation requires millions of floating point operations, making their deployment on smartphones and Internet-scale clusters problematic. The computation is dominated by the convolution operations in the lower layers of the model. We exploit the redundancy present within the convolutional filters to derive approximations that significantly reduce the required computation. Using large state-of-the-art models, we demonstrate speedups of convolutional layers on both CPU and GPU by a factor of 2{\texttimes}, while keeping the accuracy within 1\% of the original model.},
  keywords = {Pruning,redundancy in NN},
  file = {/home/luisaam/Zotero/storage/DFBS8827/Denton et al. - 2014 - Exploiting Linear Structure Within Convolutional N.pdf}
}

@article{dettmersSparseNetworksScratch2019,
  title = {Sparse {{Networks}} from {{Scratch}}: {{Faster Training}} without {{Losing Performance}}},
  shorttitle = {Sparse {{Networks}} from {{Scratch}}},
  author = {Dettmers, Tim and Zettlemoyer, Luke},
  year = {2019},
  month = aug,
  journal = {arXiv:1907.04840 [cs, stat]},
  eprint = {1907.04840},
  primaryclass = {cs, stat},
  urldate = {2022-03-09},
  abstract = {We demonstrate the possibility of what we call sparse learning: accelerated training of deep neural networks that maintain sparse weights throughout training while achieving dense performance levels. We accomplish this by developing sparse momentum, an algorithm which uses exponentially smoothed gradients (momentum) to identify layers and weights which reduce the error efficiently. Sparse momentum redistributes pruned weights across layers according to the mean momentum magnitude of each layer. Within a layer, sparse momentum grows weights according to the momentum magnitude of zero-valued weights. We demonstrate state-of-the-art sparse performance on MNIST, CIFAR-10, and ImageNet, decreasing the mean error by a relative 8\%, 15\%, and 6\% compared to other sparse algorithms. Furthermore, we show that sparse momentum reliably reproduces dense performance levels while providing up to 5.61x faster training. In our analysis, ablations show that the benefits of momentum redistribution and growth increase with the depth and size of the network. Additionally, we find that sparse momentum is insensitive to the choice of its hyperparameters suggesting that sparse momentum is robust and easy to use.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,dynamical sparse training,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/HAPSFWL7/Dettmers y Zettlemoyer - 2019 - Sparse Networks from Scratch Faster Training with.pdf;/home/luisaam/Zotero/storage/3SERVD26/1907.html}
}

@inproceedings{devlinBERTPretrainingDeep2019,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  month = jun,
  pages = {4171--4186},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-1423},
  urldate = {2021-07-02},
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  keywords = {examples},
  file = {/home/luisaam/Zotero/storage/ZK2D4WV7/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf}
}

@article{DICER1HotX2010,
  title = {{{DICER1}} Hot\&\#x2010;Spot Mutations in Ovarian Gynandroblastoma},
  journal = {HISTOPATHOLOGY},
  doi = {10.1111/his.13630},
  abstract = {{$<$}div class="abstract-group"{$>$}    {$<$}section class="article-section article-section\_\_abstract" lang="en" data-lang="en" id="section-1-en"{$>$}       {$<$}h3 class="article-section\_\_header main abstractlang\_en main"{$>$}Abstract{$<$}/h3{$>$}       {$<$}div class="article-section\_\_content en main"{$>$}                    {$<$}div class="article-section\_\_content" id="his13630-sec-0001"{$>$}                          {$<$}h3 class="article-section\_\_sub-title section1"{$>$} Aims{$<$}/h3{$>$}                          {$<$}p{$>$}Gynandroblastoma is a rare ovarian sex cord-stromal tumour characterised by the presence                of both male (Sertoli and/or Leydig cells) and female (granulosa cells) components.                We investigated the mutational status of \emph{DICER1},\emph{ FOXL2} and \emph{AKT1} genes at hot-spot regions that are known to be the key driving events in the development                of Sertoli{\textendash}Leydig cell tumour (SLCT), adult granulosa cell tumour (aGCT) and juvenile                granulosa cell tumour (jGCT), respectively, to gain insights into the molecular pathogenesis                of gynandroblastoma.             {$<$}/p{$>$}},
  keywords = {Researcher App}
}

@article{DigitalTwinVirtualization,
  title = {Digital {{Twin Virtualization}} with {{Machine Learning}} for {{IoT}} and {{Beyond 5G Networks}}: {{Research Directions}} for {{Security}} and {{Optimal Control}}. ({{arXiv}}:2204.01950v1 [Cs.{{NI}}])},
  journal = {arXiv Machine Learning (Computer Science)},
  doi = {arXiv:2204.01950v1},
  abstract = {Digital twin (DT) technologies have emerged as a solution for real-time data-driven modeling of cyber physical systems (CPS) using the vast amount of data available by Internet of Things (IoT) networks. In this position paper, we elucidate unique characteristics and capabilities of a DT framework that enables realization of such promises as online learning of a physical environment, real-time monitoring of assets, Monte Carlo heuristic search for predictive prevention, on-policy, and off-policy reinforcement learning in real-time. We establish a conceptual layered architecture for a DT framework with decentralized implementation on cloud computing and enabled by artificial intelligence (AI) services for modeling, event detection, and decision-making processes. The DT framework separates the control functions, deployed as a system of logically centralized process, from the physical devices under control, much like software-defined networking (SDN) in fifth generation (5G) wireless networks. We discuss the moment of the DT framework in facilitating implementation of network-based control processes and its implications for critical infrastructure. To clarify the significance of DT in lowering the risk of development and deployment of innovative technologies on existing system, we discuss the application of implementing zero trust architecture (ZTA) as a necessary security framework in future data-driven communication networks.},
  keywords = {Researcher App}
}

@article{dingCentripetalSGDPruning2019,
  title = {Centripetal {{SGD}} for {{Pruning Very Deep Convolutional Networks With Complicated Structure}}},
  author = {Ding, Xiaohan and Ding, Guiguang and Guo, Yuchen and Han, Jungong},
  year = {2019},
  month = jun,
  journal = {2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages = {4938--4948},
  publisher = {{IEEE}},
  address = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00508},
  urldate = {2023-03-23},
  abstract = {The redundancy is widely recognized in Convolutional Neural Networks (CNNs), which enables to remove some unimportant filters from convolutional layers so as to slim the network with acceptable performance drop. Inspired by the linearity of convolution, we seek to make some filters increasingly close and eventually identical for network slimming. To this end, we propose Centripetal SGD (C-SGD), a novel optimization method, which can train several filters to collapse into a single point in the parameter hyperspace. When the training is completed, the removal of the identical filters can trim the network with NO performance loss, thus no finetuning is needed. By doing so, we have partly solved an open problem of constrained filter pruning on CNNs with complicated structure, where some layers must be pruned following the others. Our experimental results on CIFAR-10 and ImageNet have justified the effectiveness of C-SGD-based filter pruning. Moreover, we have provided empirical evidences for the assumption that the redundancy in deep neural networks helps the convergence of training by showing that a redundant CNN trained using C-SGD outperforms a normally trained counterpart with the equivalent width.},
  isbn = {9781728132938},
  keywords = {Channel pruning,Pruning},
  file = {/home/luisaam/Zotero/storage/3FCLVPHP/Ding et al. - 2019 - Centripetal SGD for Pruning Very Deep Convolutiona.pdf}
}

@article{dingEvolutionaryArtificialNeural2013,
  title = {Evolutionary Artificial Neural Networks: A Review},
  shorttitle = {Evolutionary Artificial Neural Networks},
  author = {Ding, Shifei and Li, Hui and Su, Chunyang and Yu, Junzhao and Jin, Fengxiang},
  year = {2013},
  month = mar,
  journal = {Artificial Intelligence Review},
  volume = {39},
  number = {3},
  pages = {251--260},
  issn = {1573-7462},
  doi = {10.1007/s10462-011-9270-6},
  urldate = {2021-04-23},
  abstract = {This paper reviews the use of evolutionary algorithms (EAs) to optimize artificial neural networks (ANNs). First, we briefly introduce the basic principles of artificial neural networks and evolutionary algorithms and, by analyzing the advantages and disadvantages of EAs and ANNs, explain the advantages of using EAs to optimize ANNs. We then provide a brief survey on the basic theories and algorithms for optimizing the weights, optimizing the network architecture and optimizing the learning rules, and discuss recent research from these three aspects. Finally, we speculate on new trends in the development of this area.},
  langid = {english},
  keywords = {EvolutionaryMethods,NNoptimization},
  file = {/home/luisaam/Zotero/storage/GI4T2284/Ding et al. - 2013 - Evolutionary artificial neural networks a review.pdf}
}

@inproceedings{dingModelChannelPruning2021,
  title = {Model {{Channel Pruning Method Based}} on {{Squeeze-and-Excitation Mechanism}} and {{Upper Quartile Truncation}}},
  booktitle = {2021 {{International Symposium}} on {{Computer Science}} and {{Intelligent Controls}} ({{ISCSIC}})},
  author = {Ding, Zening and Zhao, Jianyu and Sun, Jiaqi},
  year = {2021},
  month = nov,
  pages = {114--118},
  doi = {10.1109/ISCSIC54682.2021.00031},
  abstract = {As the application of neural networks becomes more profound and more widespread, it is common to deepen the network or increase the number of parameters to improve its performance. Nevertheless, the accompanying increase in computational complexity makes the model difficult to implement on resource-limited mobile devices. Pruning connections are one of the main methods used for deep network compression. The existing pruning techniques suffer from disadvantages such as generating additional energy consumption, short practical estimation of channel importance, and leading the network to fall into overfitting. In this paper, we propose a channel pruning method based on the SE mechanism and upper quartile truncation, which uses the SE mechanism to extract the channel feature importance and upper quartile points as truncation factors to accelerate the model and reduce its computational effort, and the effectiveness of the method was demonstrated by experiments.},
  keywords = {Channel pruning,Computational modeling,Computer science,convolutional neural network,Energy consumption,Estimation,Feature extraction,feature interaction,Neural networks,Performance evaluation,Pruning,squeeze-and-excitation mechanism,upper quartile},
  file = {/home/luisaam/Zotero/storage/AVMNBBIW/9644357.html}
}

@misc{dingSpuriousLocalMinima,
  title = {Spurious {{Local Minima Exist}} for {{Almost All Over-parameterized Neural Networks}} {\textendash} {{Optimization Online}}},
  author = {Ding, Tian and Sun, Ruoyu and Li, Dawei},
  urldate = {2022-11-08},
  langid = {american},
  file = {/home/luisaam/Zotero/storage/VQGUE2FC/7409.html}
}

@misc{dingSubOptimalLocalMinima2020,
  title = {Sub-{{Optimal Local Minima Exist}} for {{Neural Networks}} with {{Almost All Non-Linear Activations}}},
  author = {Ding, Tian and Li, Dawei and Sun, Ruoyu},
  year = {2020},
  month = nov,
  number = {arXiv:1911.01413},
  eprint = {1911.01413},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1911.01413},
  urldate = {2022-11-08},
  abstract = {Does over-parameterization eliminate sub-optimal local minima for neural networks? An affirmative answer was given by a classical result in [59] for 1-hidden-layer wide neural networks. A few recent works have extended the setting to multi-layer neural networks, but none of them has proved every local minimum is global. Why is this result never extended to deep networks? In this paper, we show that the task is impossible because the original result for 1-hidden-layer network in [59] can not hold. More specifically, we prove that for any multi-layer network with generic input data and non-linear activation functions, sub-optimal local minima can exist, no matter how wide the network is (as long as the last hidden layer has at least two neurons). While the result of [59] assumes sigmoid activation, our counter-example covers a large set of activation functions (dense in the set of continuous functions), indicating that the limitation is not due to the specific activation. Our result indicates that "no bad local-min" may be unable to explain the benefit of over-parameterization for training neural nets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning,Why no global second order},
  file = {/home/luisaam/Zotero/storage/S45KIZNG/Ding et al. - 2020 - Sub-Optimal Local Minima Exist for Neural Networks.pdf;/home/luisaam/Zotero/storage/5CQ7A8U3/1911.html}
}

@article{DivideConquerNeural,
  title = {Divide and {{Conquer Neural Networks}}},
  doi = {10.1016/S0893-6080(09)80022-1}
}

@article{dodgeShowYourWork2019,
  title = {Show {{Your Work}}: {{Improved Reporting}} of {{Experimental Results}}},
  shorttitle = {Show {{Your Work}}},
  author = {Dodge, Jesse and Gururangan, Suchin and Card, Dallas and Schwartz, Roy and Smith, Noah A.},
  year = {2019},
  month = sep,
  journal = {arXiv:1909.03004 [cs, stat]},
  eprint = {1909.03004},
  primaryclass = {cs, stat},
  urldate = {2022-01-30},
  abstract = {Research in natural language processing proceeds, in part, by demonstrating that new models achieve superior performance (e.g., accuracy) on held-out test data, compared to previous results. In this paper, we demonstrate that test-set performance scores alone are insufficient for drawing accurate conclusions about which model performs best. We argue for reporting additional details, especially performance on validation data obtained during model development. We present a novel technique for doing so: expected validation performance of the best-found model as a function of computation budget (i.e., the number of hyperparameter search trials or the overall training time). Using our approach, we find multiple recent model comparisons where authors would have reached a different conclusion if they had used more (or less) computation. Our approach also allows us to estimate the amount of computation required to obtain a given accuracy; applying it to several recently published results yields massive variation across papers, from hours to weeks. We conclude with a set of best practices for reporting experimental results which allow for robust future comparisons, and provide code to allow researchers to use our technique.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,reporting results,Statistics - Machine Learning,Statistics - Methodology,to read},
  file = {/home/luisaam/Zotero/storage/LHVFYWGD/Dodge et al. - 2019 - Show Your Work Improved Reporting of Experimental.pdf;/home/luisaam/Zotero/storage/X4T287CC/1909.html}
}

@inproceedings{dosovitskiyImageWorth16x162022,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  year = {2022},
  month = mar,
  urldate = {2022-12-15},
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied in conjunction with convolutional networks, or used to replace certain components of convolutional networks while keeping their overall structure in place. We show that this reliance on CNNs is not necessary and a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks. When pre-trained on large amounts of data and transferred to multiple mid-sized or small image recognition benchmarks (ImageNet, CIFAR-100, VTAB, etc.), Vision Transformer (ViT) attains excellent results compared to state-of-the-art convolutional networks while requiring substantially fewer computational resources to train.},
  langid = {english},
  keywords = {image recognition,Transformers,Visual Transformers},
  file = {/home/luisaam/Zotero/storage/SDTPBWPX/Dosovitskiy et al. - 2022 - An Image is Worth 16x16 Words Transformers for Im.pdf}
}

@article{drozdzalImportanceSkipConnections2016,
  title = {The {{Importance}} of {{Skip Connections}} in {{Biomedical Image Segmentation}}},
  author = {Drozdzal, Michal and Vorontsov, Eugene and Chartrand, Gabriel and Kadoury, Samuel and Pal, Chris},
  year = {2016},
  month = sep,
  journal = {arXiv:1608.04117 [cs]},
  eprint = {1608.04117},
  primaryclass = {cs},
  urldate = {2021-07-12},
  abstract = {In this paper, we study the influence of both long and short skip connections on Fully Convolutional Networks (FCN) for biomedical image segmentation. In standard FCNs, only long skip connections are used to skip features from the contracting path to the expanding path in order to recover spatial information lost during downsampling. We extend FCNs by adding short skip connections, that are similar to the ones introduced in residual networks, in order to build very deep FCNs (of hundreds of layers). A review of the gradient flow confirms that for a very deep FCN it is beneficial to have both long and short skip connections. Finally, we show that a very deep FCN can achieve near-to-state-of-the-art results on the EM dataset without any further post-processing.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,skip connections},
  file = {/home/luisaam/Zotero/storage/WD7IGAUD/Drozdzal et al. - 2016 - The Importance of Skip Connections in Biomedical I.pdf;/home/luisaam/Zotero/storage/9E7NAFPD/1608.html}
}

@article{druckerDoubleBackpropagationIncreasing1992,
  title = {Double Backpropagation Increasing Generalization Performance: {{International Joint Conference}} on {{Neural Networks}} - {{IJCNN-91-Seattle}}},
  shorttitle = {Double Backpropagation Increasing Generalization Performance},
  author = {Drucker, Harris and Le Cun, Yann},
  editor = {Anon, null},
  year = {1992},
  journal = {Proceedings. IJCNN - International Joint Conference on Neural Networks},
  series = {Proceedings. {{IJCNN}} - {{International Joint Conference}} on {{Neural Networks}}},
  pages = {145--150},
  publisher = {{Publ by IEEE}},
  issn = {0780301641},
  urldate = {2022-05-15},
  abstract = {One test of a new training algorithm is how well the algorithm generalizes from the training data to the test data. It is shown that a new training algorithm termed double backpropagation improves generalization by simultaneously minimizing the normal energy term found in backpropagation and an additional energy term that is related to the sum of the squares of the input derivatives (gradients). In normal backpropagation training, minimizing the energy function tends to push the input gradient to zero. However, this is not always possible. Double backpropagation explicitly pushes the input gradients to zero, making the minimum broader, and increases the generalization on the test data. The authors show the improvement over normal backpropagation on four candidate architectures and a training set of 320 handwritten numbers and a test set of size 180.},
  keywords = {gradients training,Sobolev training}
}

@inproceedings{duanBenchmarkingDeepReinforcement2016,
  title = {Benchmarking Deep Reinforcement Learning for Continuous Control},
  booktitle = {International Conference on Machine Learning},
  author = {Duan, Yan and Chen, Xi and Houthooft, Rein and Schulman, John and Abbeel, Pieter},
  year = {2016},
  pages = {1329--1338},
  organization = {{PMLR}}
}

@article{duchiAdaptiveSubgradientMethods2011,
  title = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
  author = {Duchi, John and Hazan, Elad and Singer, Yoram},
  year = {2011},
  journal = {Journal of Machine Learning Research},
  volume = {12},
  number = {61},
  pages = {2121--2159},
  file = {/home/luisaam/Zotero/storage/IRRFFUH7/Duchi et al. - 2011 - Adaptive subgradient methods for online learning a.pdf}
}

@inproceedings{duGradientDescentFinds2019,
  title = {Gradient {{Descent Finds Global Minima}} of {{Deep Neural Networks}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Du, Simon and Lee, Jason and Li, Haochuan and Wang, Liwei and Zhai, Xiyu},
  year = {2019},
  month = may,
  pages = {1675--1685},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2021-06-28},
  abstract = {Gradient descent finds a global minimum in training deep neural networks despite the objective function being non-convex. The current paper proves gradient descent achieves zero training loss in po...},
  langid = {english},
  keywords = {Why no global second order},
  file = {/home/luisaam/Zotero/storage/GPCS5SST/Du et al. - 2019 - Gradient Descent Finds Global Minima of Deep Neura.pdf;/home/luisaam/Zotero/storage/KEV355I7/du19c.html}
}

@article{DynamicSparseTraining,
  title = {Dynamic {{Sparse Training}} for {{Deep Reinforcement Learning}}. ({{arXiv}}:2106.04217v3 [Cs.{{LG}}] {{UPDATED}})},
  journal = {arXiv Computer Science},
  doi = {arXiv:2106.04217v3},
  abstract = {Deep reinforcement learning (DRL) agents are trained through trial-and-error interactions with the environment. This leads to a long training time for dense neural networks to achieve good performance. Hence, prohibitive computation and memory resources are consumed. Recently, learning efficient DRL agents has received increasing attention. Yet, current methods focus on accelerating inference time. In this paper, we introduce for the first time a dynamic sparse training approach for deep reinforcement learning to accelerate the training process. The proposed approach trains a sparse neural network from scratch and dynamically adapts its topology to the changing data distribution during training. Experiments on continuous control tasks show that our dynamic sparse agents achieve higher performance than the equivalent dense methods, reduce the parameter count and floating-point operations (FLOPs) by 50\%, and have a faster learning speed that enables reaching the performance of dense agents with 40-50\% reduction in the training steps.},
  keywords = {dynamical sparse training,Researcher App}
}

@article{e088f262c96a957b0760b753d1f4d34d3b9923e1,
  title = {Does Interpretability of Neural Networks Imply Adversarial Robustness?},
  author = {Noack, Adam and Ahern, Isaac and Dou, D. and Li, Boyang},
  year = {2019},
  journal = {ArXiv},
  volume = {abs/1912.03430},
  abstract = {The success of deep neural networks is clouded by two issues: (1) a vulnerability to adversarial examples and (2) a tendency to be uninterpretable. Interestingly, recent empirical evidence in the literature as well as theoretical analysis on simple models suggest these two seemingly disparate issues are actually connected. In particular, robust models tend to be more interpretable than non-robust models. In this paper, we provide evidence for the claim that this relationship is bidirectional. Viz., models that are optimized to have interpretable gradients are more robust to adversarial examples than models trained in a standard manner. With further analysis and experiments on standard image classification datasets, we identify two factors behind this phenomenon{\textemdash}namely the suppression of the gradient's magnitude and the selective use of features guided by high-quality interpretations{\textemdash}which explain model behaviors under various regularization and target interpretation settings.},
  arxivid = {1912.03430}
}

@article{elsayedLargeMarginDeep2018,
  title = {Large Margin Deep Networks for Classification},
  author = {Elsayed, Gamaleldin and Krishnan, Dilip and Mobahi, Hossein and Regan, Kevin and Bengio, Samy},
  year = {2018},
  journal = {Advances in neural information processing systems},
  volume = {31},
  keywords = {large margin neural networks,Sobolev training},
  file = {/home/luisaam/Zotero/storage/XIDK5A3Z/Elsayed et al. - 2018 - Large margin deep networks for classification.pdf}
}

@article{EmpiricalInvestigationModeltoModel,
  title = {An {{Empirical Investigation}} of {{Model-to-Model Distribution Shifts}} in {{Trained Convolutional Filters}}. ({{arXiv}}:2201.08465v1 [Cs.{{CV}}])},
  journal = {arXiv Computer Science},
  doi = {arXiv:2201.08465v1},
  abstract = {We present first empirical results from our ongoing investigation of distribution shifts in image data used for various computer vision tasks. Instead of analyzing the original training and test data, we propose to study shifts in the learned weights of trained models. In this work, we focus on the properties of the distributions of dominantly used 3x3 convolution filter kernels. We collected and publicly provide a data set with over half a billion filters from hundreds of trained CNNs, using a wide range of data sets, architectures, and vision tasks. Our analysis shows interesting distribution shifts (or the lack thereof) between trained filters along different axes of meta-parameters, like data type, task, architecture, or layer depth. We argue, that the observed properties are a valuable source for further investigation into a better understanding of the impact of shifts in the input data to the generalization abilities of CNN models and novel methods for more robust transfer-learning in this domain. Data available at: https://github.com/paulgavrikov/CNN-Filter-DB/.},
  keywords = {Researcher App}
}

@inproceedings{entezariRolePermutationInvariance2022,
  title = {The Role of Permutation Invariance in Linear Mode Connectivity of Neural Networks},
  booktitle = {International Conference on Learning Representations},
  author = {Entezari, Rahim and Sedghi, Hanie and Saukh, Olga and Neyshabur, Behnam},
  year = {2022},
  keywords = {linear interpolation,to read}
}

@article{EquivariantNeuralNetworks,
  title = {Equivariant {{Neural Networks}} for {{Indirect Measurements}}. ({{arXiv}}:2306.16506v1 [Math.{{NA}}])},
  journal = {arXiv Computer Science},
  doi = {arXiv:2306.16506v1},
  abstract = {In the recent years, deep learning techniques have shown great success in various tasks related to inverse problems, where a target quantity of interest can only be observed through indirect measurements by a forward operator. Common approaches apply deep neural networks in a post-processing step to the reconstructions obtained by classical reconstruction methods. However, the latter methods can be computationally expensive and introduce artifacts that are not present in the measured data and, in turn, can deteriorate the performance on the given task. To overcome these limitations, we propose a class of equivariant neural networks that can be directly applied to the measurements to solve the desired task. To this end, we build appropriate network structures by developing layers that are equivariant with respect to data transformations induced by well-known symmetries in the domain of the forward operator. We rigorously analyze the relation between the measurement operator and the resulting group representations and prove a representer theorem that characterizes the class of linear operators that translate between a given pair of group actions. Based on this theory, we extend the existing concepts of Lie group equivariant deep learning to inverse problems and introduce new representations that result from the involved measurement operations. This allows us to efficiently solve classification, regression or even reconstruction tasks based on indirect measurements also for very sparse data problems, where a classical reconstruction-based approach may be hard or even impossible. We illustrate the effectiveness of our approach in numerical experiments and compare with existing methods.},
  keywords = {Researcher App}
}

@article{erhanWhyDoesUnsupervised2010,
  title = {Why {{Does Unsupervised Pre-training Help Deep Learning}}?},
  author = {Erhan, Dumitru and Bengio, Yoshua and Courville, Aaron and Manzagol, Pierre-Antoine and Vincent, Pascal and Bengio, Samy},
  year = {2010},
  journal = {Journal of Machine Learning Research},
  volume = {11},
  number = {19},
  pages = {625--660},
  issn = {1533-7928},
  urldate = {2021-08-07},
  abstract = {Much recent research has been devoted to learning algorithms for deep architectures such as Deep Belief Networks and stacks of auto-encoder variants, with impressive results obtained in several areas, mostly on vision and language data sets. The best results obtained on supervised learning tasks involve an unsupervised learning component, usually in an unsupervised pre-training phase. Even though these new algorithms have enabled training deep models, many questions remain as to the nature of this difficult learning problem. The main question investigated here is the following: how does unsupervised pre-training work? Answering this questions is important if learning in deep architectures is to be further improved. We propose several explanatory hypotheses and test them through extensive simulations. We empirically show the influence of pre-training with respect to architecture depth, model capacity, and number of training examples. The experiments confirm and clarify the advantage of unsupervised pre-training. The results suggest that unsupervised pre-training guides the learning towards basins of attraction of minima that support better generalization from the training data set; the evidence from these results supports a regularization explanation for the effect of pre-training.},
  file = {/home/luisaam/Zotero/storage/DJCPNV5K/Erhan et al. - 2010 - Why Does Unsupervised Pre-training Help Deep Learn.pdf}
}

@article{evciDifficultyTrainingSparse2020,
  title = {The {{Difficulty}} of {{Training Sparse Neural Networks}}},
  author = {Evci, Utku and Pedregosa, Fabian and Gomez, Aidan and Elsen, Erich},
  year = {2020},
  month = oct,
  journal = {arXiv:1906.10732 [cs, stat]},
  eprint = {1906.10732},
  primaryclass = {cs, stat},
  urldate = {2022-02-07},
  abstract = {We investigate the difficulties of training sparse neural networks and make new observations about optimization dynamics and the energy landscape within the sparse regime. Recent work of {\textbackslash}citep\{Gale2019, Liu2018\} has shown that sparse ResNet-50 architectures trained on ImageNet-2012 dataset converge to solutions that are significantly worse than those found by pruning. We show that, despite the failure of optimizers, there is a linear path with a monotonically decreasing objective from the initialization to the "good" solution. Additionally, our attempts to find a decreasing objective path from "bad" solutions to the "good" ones in the sparse subspace fail. However, if we allow the path to traverse the dense subspace, then we consistently find a path between two solutions. These findings suggest traversing extra dimensions may be needed to escape stationary points found in the sparse subspace.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Pruning,sparse NN,sparse training,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/XXRAWEBT/Evci et al. - 2020 - The Difficulty of Training Sparse Neural Networks.pdf;/home/luisaam/Zotero/storage/B6XPVDM9/1906.html}
}

@article{evciGradientFlowSparse2020,
  title = {Gradient {{Flow}} in {{Sparse Neural Networks}} and {{How Lottery Tickets Win}}},
  author = {Evci, Utku and Ioannou, Yani A. and Keskin, Cem and Dauphin, Yann},
  year = {2020},
  month = oct,
  journal = {arXiv:2010.03533 [cs]},
  eprint = {2010.03533},
  primaryclass = {cs},
  urldate = {2021-11-15},
  abstract = {Sparse Neural Networks (NNs) can match the generalization of dense NNs using a fraction of the compute/storage for inference, and also have the potential to enable efficient training. However, naively training unstructured sparse NNs from random initialization results in significantly worse generalization, with the notable exception of Lottery Tickets (LTs) and Dynamic Sparse Training (DST). In this work, we attempt to answer: (1) why training unstructured sparse networks from random initialization performs poorly and; (2) what makes LTs and DST the exceptions? We show that sparse NNs have poor gradient flow at initialization and propose a modified initialization for unstructured connectivity. Furthermore, we find that DST methods significantly improve gradient flow during training over traditional sparse training methods. Finally, we show that LTs do not improve gradient flow, rather their success lies in re-learning the pruning solution they are derived from - however, this comes at the cost of learning novel solutions.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Optimization,Pruning},
  file = {/home/luisaam/Zotero/storage/UYPUZVY8/2010.03533v2.pdf;/home/luisaam/Zotero/storage/7YEHMMVD/2010.html}
}

@inproceedings{evciRiggingLotteryMaking2020,
  title = {Rigging the Lottery: {{Making}} All Tickets Winners},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  author = {Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich},
  editor = {III, Hal Daum{\'e} and Singh, Aarti},
  year = {2020},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {119},
  pages = {2943--2952},
  publisher = {{PMLR}},
  abstract = {Many applications require sparse neural networks due to space or inference time restrictions. There is a large body of work on training dense networks to yield sparse networks for inference, but this limits the size of the largest trainable sparse model to that of the largest trainable dense model. In this paper we introduce a method to train sparse neural networks with a fixed parameter count and a fixed computational cost throughout training, without sacrificing accuracy relative to existing dense-to-sparse training methods. Our method updates the topology of the sparse network during training by using parameter magnitudes and infrequent gradient calculations. We show that this approach requires fewer floating-point operations (FLOPs) to achieve a given level of accuracy compared to prior techniques. We demonstrate state-of-the-art sparse training results on a variety of networks and datasets, including ResNet-50, MobileNets on Imagenet-2012, and RNNs on WikiText-103. Finally, we provide some insights into why allowing the topology to change during the optimization can overcome local minima encountered when the topology remains static.},
  pdf = {http://proceedings.mlr.press/v119/evci20a/evci20a.pdf},
  keywords = {dynamical sparse training},
  file = {/home/luisaam/Zotero/storage/3KJ22CFL/Evci et al. - 2020 - Rigging the lottery Making all tickets winners.pdf}
}

@article{ExplainableAnalysisDeep,
  title = {Explainable {{Analysis}} of {{Deep Learning Methods}} for {{SAR Image Classification}}. ({{arXiv}}:2204.06783v1 [Cs.{{CV}}])},
  journal = {arXiv Computer Vision and Pattern Recognition},
  doi = {arXiv:2204.06783v1},
  abstract = {Deep learning methods exhibit outstanding performance in synthetic aperture radar (SAR) image interpretation tasks. However, these are black box models that limit the comprehension of their predictions. Therefore, to meet this challenge, we have utilized explainable artificial intelligence (XAI) methods for the SAR image classification task. Specifically, we trained state-of-the-art convolutional neural networks for each polarization format on OpenSARUrban dataset and then investigate eight explanation methods to analyze the predictions of the CNN classifiers of SAR images. These XAI methods are also evaluated qualitatively and quantitatively which shows that Occlusion achieves the most reliable interpretation performance in terms of Max-Sensitivity but with a low-resolution explanation heatmap. The explanation results provide some insights into the internal mechanism of black-box decisions for SAR image classification.},
  keywords = {Researcher App}
}

@article{ExtendedCriticalRegimes,
  title = {Extended Critical Regimes of Deep Neural Networks. ({{arXiv}}:2203.12967v1 [Cs.{{LG}}])},
  journal = {arXiv Statistical Mechanics},
  doi = {arXiv:2203.12967v1},
  abstract = {Deep neural networks (DNNs) have been successfully applied to many real-world problems, but a complete understanding of their dynamical and computational principles is still lacking. Conventional theoretical frameworks for analysing DNNs often assume random networks with coupling weights obeying Gaussian statistics. However, non-Gaussian, heavy-tailed coupling is a ubiquitous phenomenon in DNNs. Here, by weaving together theories of heavy-tailed random matrices and non-equilibrium statistical physics, we develop a new type of mean field theory for DNNs which predicts that heavy-tailed weights enable the emergence of an extended critical regime without fine-tuning parameters. In this extended critical regime, DNNs exhibit rich and complex propagation dynamics across layers. We further elucidate that the extended criticality endows DNNs with profound computational advantages: balancing the contraction as well as expansion of internal neural representations and speeding up training processes, hence providing a theoretical guide for the design of efficient neural architectures.},
  keywords = {Researcher App}
}

@inproceedings{fanFastSecondorderStochastic2015,
  title = {Fast Second-Order Stochastic Backpropagation for Variational Inference},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Fan, Kai and Wang, Ziteng and Beck, Jeffrey and Kwok, James T. and Heller, Katherine},
  year = {2015},
  issn = {10495258},
  abstract = {We propose a second-order (Hessian or Hessian-free) based optimization method for variational inference inspired by Gaussian backpropagation, and argue that quasi-Newton optimization can be developed as well. This is accomplished by generalizing the gradient computation in stochastic backpropagation via a reparametrization trick with lower complexity. As an illustrative example, we apply this approach to the problems of Bayesian logistic regression and variational auto-encoder (VAE). Additionally, we compute bounds on the estimator variance of intractable expectations for the family of Lipschitz continuous function. Our method is practical, scalable and model free. We demonstrate our method on several real-world datasets and provide comparisons with other stochastic gradient methods to show substantial enhancement in convergence rates.}
}

@article{FeatureFlowRegularization,
  title = {Feature {{Flow Regularization}}: {{Improving Structured Sparsity}} in {{Deep Neural Networks}}. ({{arXiv}}:2106.02914v1 [Cs.{{CV}}])},
  journal = {arXiv Computer Science},
  doi = {arXiv:2106.02914v1},
  abstract = {Pruning is a model compression method that removes redundant parameters in deep neural networks (DNNs) while maintaining accuracy. Most available filter pruning methods require complex treatments such as iterative pruning, features statistics/ranking, or additional optimization designs in the training process. In this paper, we propose a simple and effective regularization strategy from a new perspective of evolution of features, which we call feature flow regularization (FFR), for improving structured sparsity and filter pruning in DNNs. Specifically, FFR imposes controls on the gradient and curvature of feature flow along the neural network, which implicitly increases the sparsity of the parameters. The principle behind FFR is that coherent and smooth evolution of features will lead to an efficient network that avoids redundant parameters. The high structured sparsity obtained from FFR enables us to prune filters effectively. Experiments with VGGNets, ResNets on CIFAR-10/100, and Tiny ImageNet datasets demonstrate that FFR can significantly improve both unstructured and structured sparsity. Our pruning results in terms of reduction of parameters and FLOPs are comparable to or even better than those of state-of-the-art pruning methods.},
  keywords = {Researcher App}
}

@article{FG21GuidanceFirms,
  title = {{{FG21}}/1: {{Guidance}} for Firms on the Fair Treatment of Vulnerable Customers},
  langid = {english},
  keywords = {Finalised Guidance},
  file = {/home/luisaam/Zotero/storage/IKCR9VP5/FG211 Guidance for firms on the fair treatment o.pdf}
}

@article{FieldAwareAttentiveNeural,
  title = {Field-{{Aware Attentive Neural Factorization}} with {{Fuzzy Mutual Information}} for {{Company Investment Valuation}}},
  journal = {Information Sciences},
  doi = {10.1016/j.ins.2022.03.073},
  abstract = {The proliferation of a digital transformation area is inspiring researchers and practitioners in finance to embrace emerging innovative fintech development (i.e., finance + technology). In this study, we propose a field-aware attentive neural factorization machine (FAFM) model for large-scale data-driven company investment valuation. The proposed FAFM model utilizes the advantage of factorization machine (FM) to efficiently capture nonlinear feature interactions in a sparse dataset. We additionally consider field heterogeneity among features with fuzzy mutual information and develop an attention neural network to learn predictive strengths of pair-wise feature interactions. FAFM contributes to the literature by overcoming the limitation of FM that ignores field heterogeneity by factorizing pair-wise feature interactions with same weight. Further more, FAFM learns the prediction strengths in a stratified manner by using the attention deep learning mechanism, which demonstrates more structured control ability and allows for more leverage in tweaking the interactions in the feature-wise level. Experiments are conducted on a unique real dataset set consisting of 3,500 listed companies in the Chinese market with features from eight fields: demographics, annual reports, stock financial disclosure, land use, intellectual property, tax, bond financing, and certification. Results showed the superiority of FAFM on prediction accuracy and model interpretability over existing baselines. Our study provides a useful tool for company investment valuation that can not only generate accurate investment valuations but also provide interpretations of both individual features and their pair-wise interactions effects, thereby allowing investors better investment decisions.},
  keywords = {Researcher App}
}

@article{FilterPruningFeature,
  title = {Filter Pruning via Feature Map Clustering},
  journal = {Intelligent Data Analysis},
  doi = {10.3233/ida-226810},
  abstract = {With the help of network compression algorithms, deep neural networks can be applied on low-power embedded systems and mobile devices such as drones, satellites, and smartphones. Filter pruning is a sub-direction of network compression research, which reduces memory and computational consumption by reducing the number of parameters of model filters. Previous works utilized the "more-simple-less-important" criterion for pruning filters. That is, filters with the smaller norm or more sparse weights in the network are preferentially pruned. In this paper, we found that feature maps are not fully positively correlated with the sparsity of filter weights by observing the visualization of feature maps and the corresponding filters. Hence, we came up with the idea that the priority of filter pruning should be determined by redundancy rather than sparsity. The redundancy of a filter is the measure of whether the output of the filter is repeated with other filters. Based on this, we defined a criterion called redundancy index to rank the filters and introduced it into our filter pruning strategy. Extensive experiments demonstrate the effectiveness of our approach on different model architectures, including VGGNet, GoogleNet, DenseNet, and ResNet. The models compressed with our strategy surpass the state-of-the-art in terms of Floating Point Operations Per Second (FLOPs), parameters reduction, and classification accuracy.},
  keywords = {Researcher App}
}

@article{foersterBayesianActionDecoder2019,
  title = {Bayesian Action Decoder for Deep Multi-Agent Reinforcement Learning},
  author = {Foerster, J.N. and Song, H.F. and Hughes, E. and Burch, N. and Dunning, I. and Whiteson, S. and Botvinick, M.M. and Bowling, M.},
  year = {2019},
  journal = {36th International Conference on Machine Learning, ICML 2019},
  volume = {2019-June},
  publisher = {{International Machine Learning Society (IMLS) rasmussen@ptd.net}},
  abstract = {Copyright 2019 by the author(s). When observing the actions of others, humans make inferences about why they acted as they did, and what this implies about the world; humans also use the fact that their actions will be interpreted in this manner, allowing them to act informatively and thereby communicate efficiently with others. Although learning algorithms have recently achieved superhuman performance in a number of two-player, zero-sum games, scalable multi-agent reinforcement learning algorithms that can discover effective strategies and conventions in complex, partially observable settings have proven elusive. We present the Bayesian action decoder (BAD), a new multi-agent learning method that uses an approximate Bayesian update to obtain a public belief that conditions on the actions taken by all agents in the environment. BAD introduces a new Markov decision process, the public beliefMDP, in which the action space consists of all deterministic partial policies, and exploits the fact that an agent acting only on this public belief state can still learn to use its private information if the action space is augmented to be over all partial policies mapping private information into environment actions. The Bayesian update is closely related to the theory of mind reasoning that humans carry out when observing others' actions. Wc first validate BAD on a proof-of-principle two-step matrix game, where it outperforms policy gradient methods; we then evaluate BAD on the challenging, cooperative partial-information card game Hanabi, where, in the two-player setting, it surpasses all previously published learning and hand-coded approaches, establishing a new state of the art.}
}

@inproceedings{fogelEvolutionaryMethodsTraining1991,
  title = {Evolutionary Methods for Training Neural Networks},
  booktitle = {[1991 {{Proceedings}}] {{IEEE Conference}} on {{Neural Networks}} for {{Ocean Engineering}}},
  author = {Fogel, D.B. and Fogel, L.J. and Porto, V.W.},
  year = {1991},
  month = aug,
  pages = {317--327},
  doi = {10.1109/ICNN.1991.163368},
  abstract = {Training neural networks by the implementation of a gradient-based optimization algorithm (e.g., back-propagation) often leads to locally optimal solutions which may be far removed from the global optimum. Evolutionary optimization methods offer a procedure to stochastically search for suitable weights and bias terms given a specific network topology. The topics discussed are evolutionary programming; genetic algorithms; evolutionary function optimization experiments; background to classification problems and experimental results with evolutionary training.{$<>$}},
  keywords = {Classification algorithms,Fault tolerance,Logistics,Network topology,Neural networks,Pattern recognition,Response surface methodology,Statistics,Supervised learning,Unsupervised learning},
  file = {/home/luisaam/Zotero/storage/Z4BJPJD6/Fogel et al. - 1991 - Evolutionary methods for training neural networks.pdf;/home/luisaam/Zotero/storage/D4W4NRSA/163368.html}
}

@misc{FOMOTinyMLNeural,
  title = {{{FOMO}} Is a {{TinyML}} Neural Network for Real-Time Object Detection {\textendash} {{TechTalks}}},
  urldate = {2022-04-23},
  howpublished = {https://bdtechtalks.com/2022/04/18/fomo-tinyml-object-detection/},
  keywords = {TinyML},
  file = {/home/luisaam/Zotero/storage/ZEI5UUJN/fomo-tinyml-object-detection.html}
}

@inproceedings{foretSharpnessawareMinimizationEfficiently2021,
  title = {Sharpness-Aware Minimization for Efficiently Improving Generalization},
  booktitle = {International Conference on Learning Representations},
  author = {Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  year = {2021}
}

@misc{fortDeepEnsemblesLoss2020a,
  title = {Deep {{Ensembles}}: {{A Loss Landscape Perspective}}},
  shorttitle = {Deep {{Ensembles}}},
  author = {Fort, Stanislav and Hu, Huiyi and Lakshminarayanan, Balaji},
  year = {2020},
  month = jun,
  number = {arXiv:1912.02757},
  eprint = {1912.02757},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1912.02757},
  urldate = {2023-07-31},
  abstract = {Deep ensembles have been empirically shown to be a promising approach for improving accuracy, uncertainty and out-of-distribution robustness of deep learning models. While deep ensembles were theoretically motivated by the bootstrap, non-bootstrap ensembles trained with just random initialization also perform well in practice, which suggests that there could be other explanations for why deep ensembles work well. Bayesian neural networks, which learn distributions over the parameters of the network, are theoretically well-motivated by Bayesian principles, but do not perform as well as deep ensembles in practice, particularly under dataset shift. One possible explanation for this gap between theory and practice is that popular scalable variational Bayesian methods tend to focus on a single mode, whereas deep ensembles tend to explore diverse modes in function space. We investigate this hypothesis by building on recent work on understanding the loss landscape of neural networks and adding our own exploration to measure the similarity of functions in the space of predictions. Our results show that random initializations explore entirely different modes, while functions along an optimization trajectory or sampled from the subspace thereof cluster within a single mode predictions-wise, while often deviating significantly in the weight space. Developing the concept of the diversity--accuracy plane, we show that the decorrelation power of random initializations is unmatched by popular subspace sampling methods. Finally, we evaluate the relative effects of ensembling, subspace based methods and ensembles of subspace based methods, and the experimental results validate our hypothesis.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,loss landscape,Pruning,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/JKWQGBAR/Fort et al. - 2020 - Deep Ensembles A Loss Landscape Perspective.pdf;/home/luisaam/Zotero/storage/N7VJKCKV/1912.html}
}

@misc{fortEmergentPropertiesLocal2019,
  title = {Emergent Properties of the Local Geometry of Neural Loss Landscapes},
  author = {Fort, Stanislav and Ganguli, Surya},
  year = {2019},
  month = oct,
  number = {arXiv:1910.05929},
  eprint = {1910.05929},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1910.05929},
  urldate = {2023-12-03},
  abstract = {The local geometry of high dimensional neural network loss landscapes can both challenge our cherished theoretical intuitions as well as dramatically impact the practical success of neural network training. Indeed recent works have observed 4 striking local properties of neural loss landscapes on classification tasks: (1) the landscape exhibits exactly \$C\$ directions of high positive curvature, where \$C\$ is the number of classes; (2) gradient directions are largely confined to this extremely low dimensional subspace of positive Hessian curvature, leaving the vast majority of directions in weight space unexplored; (3) gradient descent transiently explores intermediate regions of higher positive curvature before eventually finding flatter minima; (4) training can be successful even when confined to low dimensional \{{\textbackslash}it random\} affine hyperplanes, as long as these hyperplanes intersect a Goldilocks zone of higher than average curvature. We develop a simple theoretical model of gradients and Hessians, justified by numerical experiments on architectures and datasets used in practice, that \{{\textbackslash}it simultaneously\} accounts for all \$4\$ of these surprising and seemingly unrelated properties. Our unified model provides conceptual insights into the emergence of these properties and makes connections with diverse topics in neural networks, random matrix theory, and spin glasses, including the neural tangent kernel, BBP phase transitions, and Derrida's random energy model.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Second Order,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/Z3VXATKV/Fort and Ganguli - 2019 - Emergent properties of the local geometry of neura.pdf;/home/luisaam/Zotero/storage/GBJSJ93D/1910.html}
}

@article{fountoulakisPerformanceFirstSecondorder2016,
  title = {Performance of First- and Second-Order Methods for \$\${\textbackslash}ell \_1\$\$-Regularized Least Squares Problems},
  author = {Fountoulakis, Kimon and Gondzio, Jacek},
  year = {2016},
  month = dec,
  journal = {Computational Optimization and Applications},
  volume = {65},
  number = {3},
  pages = {605--635},
  issn = {1573-2894},
  doi = {10.1007/s10589-016-9853-x},
  urldate = {2021-07-11},
  abstract = {We study the performance of first- and second-order optimization methods for \$\${\textbackslash}ell \_1\$\$-regularized sparse least-squares problems as the conditioning of the problem changes and the dimensions of the problem increase up to one trillion. A rigorously defined generator is presented which allows control of the dimensions, the conditioning and the sparsity of the problem. The generator has very low memory requirements and scales well with the dimensions of the problem.},
  langid = {english},
  file = {/home/luisaam/Zotero/storage/PJGWR3WK/Fountoulakis y Gondzio - 2016 - Performance of first- and second-order methods for.pdf}
}

@inproceedings{frankleLinearModeConnectivity2020,
  title = {Linear {{Mode Connectivity}} and the {{Lottery Ticket Hypothesis}}},
  booktitle = {Proceedings of the 37th {{International Conference}} on {{Machine Learning}}},
  author = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel and Carbin, Michael},
  year = {2020},
  month = nov,
  pages = {3259--3269},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-12-05},
  abstract = {We study whether a neural network optimizes to the same, linearly connected minimum under different samples of SGD noise (e.g., random data order and augmentation). We find that standard vision models become stable to SGD noise in this way early in training. From then on, the outcome of optimization is determined to a linearly connected region. We use this technique to study iterative magnitude pruning (IMP), the procedure used by work on the lottery ticket hypothesis to identify subnetworks that could have trained in isolation to full accuracy. We find that these subnetworks only reach full accuracy when they are stable to SGD noise, which either occurs at initialization for small-scale settings (MNIST) or early in training for large-scale settings (ResNet-50 and Inception-v3 on ImageNet).},
  langid = {english},
  keywords = {Pruning},
  file = {/home/luisaam/Zotero/storage/BPYF89M4/Frankle et al. - 2020 - Linear Mode Connectivity and the Lottery Ticket Hy.pdf;/home/luisaam/Zotero/storage/HZCEJWL5/Frankle et al. - 2020 - Linear Mode Connectivity and the Lottery Ticket Hy.pdf}
}

@article{frankleLotteryTicketHypothesis2019,
  title = {The {{Lottery Ticket Hypothesis}}: {{Finding Sparse}}, {{Trainable Neural Networks}}},
  shorttitle = {The {{Lottery Ticket Hypothesis}}},
  author = {Frankle, Jonathan and Carbin, Michael},
  year = {2019},
  month = mar,
  journal = {arXiv:1803.03635 [cs]},
  eprint = {1803.03635},
  primaryclass = {cs},
  urldate = {2021-07-22},
  abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the "lottery ticket hypothesis:" dense, randomly-initialized, feed-forward networks contain subnetworks ("winning tickets") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20\% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Pruning,small networks},
  file = {/home/luisaam/Zotero/storage/ZBLEE5XL/Frankle y Carbin - 2019 - The Lottery Ticket Hypothesis Finding Sparse, Tra.pdf;/home/luisaam/Zotero/storage/V4GRM48M/1803.html}
}

@article{franklePruningNeuralNetworks2021,
  title = {Pruning {{Neural Networks}} at {{Initialization}}: {{Why}} Are {{We Missing}} the {{Mark}}?},
  shorttitle = {Pruning {{Neural Networks}} at {{Initialization}}},
  author = {Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel M. and Carbin, Michael},
  year = {2021},
  month = mar,
  journal = {arXiv:2009.08576 [cs, stat]},
  eprint = {2009.08576},
  primaryclass = {cs, stat},
  urldate = {2021-08-05},
  abstract = {Recent work has explored the possibility of pruning neural networks at initialization. We assess proposals for doing so: SNIP (Lee et al., 2019), GraSP (Wang et al., 2020), SynFlow (Tanaka et al., 2020), and magnitude pruning. Although these methods surpass the trivial baseline of random pruning, they remain below the accuracy of magnitude pruning after training, and we endeavor to understand why. We show that, unlike pruning after training, randomly shuffling the weights these methods prune within each layer or sampling new initial values preserves or improves accuracy. As such, the per-weight pruning decisions made by these methods can be replaced by a per-layer choice of the fraction of weights to prune. This property suggests broader challenges with the underlying pruning heuristics, the desire to prune at initialization, or both.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Pruning,small networks,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/X3CTDWRK/Frankle et al. - 2021 - Pruning Neural Networks at Initialization Why are.pdf;/home/luisaam/Zotero/storage/GS47XWM6/2009.html}
}

@inproceedings{frantarMFACEfficientMatrixFree2021,
  title = {M-{{FAC}}: {{Efficient Matrix-Free Approximations}} of {{Second-Order Information}}},
  shorttitle = {M-{{FAC}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Frantar, Elias and Kurtic, Eldar and Alistarh, Dan},
  year = {2021},
  volume = {34},
  pages = {14873--14886},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-04-12},
  keywords = {Hessian},
  file = {/home/luisaam/Zotero/storage/5P8ZEB9C/Frantar et al. - 2021 - M-FAC Efficient Matrix-Free Approximations of Sec.pdf}
}

@techreport{frazierTutorialBayesianOptimization2018,
  title = {A {{Tutorial}} on {{Bayesian Optimization}}},
  author = {Frazier, Peter I},
  year = {2018},
  abstract = {Bayesian optimization is an approach to optimizing objective functions that take a long time (min-utes or hours) to evaluate. It is best-suited for optimization over continuous domains of less than 20 dimensions, and tolerates stochastic noise in function evaluations. It builds a surrogate for the objective and quantifies the uncertainty in that surrogate using a Bayesian machine learning technique, Gaussian process regression, and then uses an acquisition function defined from this surrogate to decide where to sample. In this tutorial, we describe how Bayesian optimization works, including Gaussian process regression and three common acquisition functions: expected improvement, entropy search, and knowledge gradient. We then discuss more advanced techniques, including running multiple function evaluations in parallel, multi-fidelity and multi-information source optimization, expensive-to-evaluate constraints, random environmental conditions, multi-task Bayesian optimization, and the inclusion of derivative information. We conclude with a discussion of Bayesian optimization software and future research directions in the field. Within our tutorial material we provide a generalization of expected improvement to noisy evaluations, beyond the noise-free setting where it is more commonly applied. This generalization is justified by a formal decision-theoretic argument, standing in contrast to previous ad hoc modifications.},
  file = {/home/luisaam/Zotero/storage/EBSSYUCB/Frazier - 2018 - A Tutorial on Bayesian Optimization.pdf}
}

@inproceedings{gabbasovInfluenceReceptiveField2020a,
  title = {Influence of the Receptive Field Size on Accuracy and Performance of a Convolutional Neural Network},
  booktitle = {2020 {{International Conference}} on {{Information Technology}} and {{Nanotechnology}} ({{ITNT}})},
  author = {Gabbasov, Rail and Paringer, Rustam},
  year = {2020},
  month = may,
  pages = {1--4},
  doi = {10.1109/ITNT49337.2020.9253219},
  urldate = {2023-11-20},
  abstract = {Convolutional neural networks (CNNs) have been successfully applied to many tasks such as digit and object recognition. In this paper we study the size of the receptive field of deep convolutional neural networks, in particular, we check the idea of a "redundant" receptive field. We run a set of experiments on two common CNN models - VGG16 and ResNet18 - in order to explore the influence of receptive field size on CNN's training time, accuracy, and performance. We run experiments using the MakiFlow framework on the CALTECH256 dataset. The experiments' results show that the optimization of neural networks (NNs) by reducing the size of the receptive field allows to reduce the NN's training time by 5-7\% while maintaining the accuracy of the network.},
  keywords = {Receptive Field},
  file = {/home/luisaam/Zotero/storage/2WJB7DQX/Gabbasov and Paringer - 2020 - Influence of the receptive field size on accuracy .pdf;/home/luisaam/Zotero/storage/ZRC72TR7/9253219.html}
}

@article{gaierWeightAgnosticNeural2019,
  title = {Weight {{Agnostic Neural Networks}}},
  author = {Gaier, Adam and Ha, David},
  year = {2019},
  month = jun,
  abstract = {Not all neural network architectures are created equal, some perform much better than others for certain tasks. But how important are the weight parameters of a neural network compared to its architecture? In this work, we question to what extent neural network architectures alone, without learning any weight parameters, can encode solutions for a given task. We propose a search method for neural network architectures that can already perform a task without any explicit weight training. To evaluate these networks, we populate the connections with a single shared weight parameter sampled from a uniform random distribution, and measure the expected performance. We demonstrate that our method can find minimal neural network architectures that can perform several reinforcement learning tasks without weight training. On a supervised learning domain, we find network architectures that achieve much higher than chance accuracy on MNIST using random weights. Interactive version of this paper at https://weightagnostic.github.io/},
  file = {/home/luisaam/Zotero/storage/VAT536CN/Gaier, Ha - 2019 - Weight Agnostic Neural Networks.pdf}
}

@article{galvanNeuroevolutionDeepLearning2021,
  title = {Neuroevolution in {{Deep Learning}}: {{The Role}} of {{Neutrality}}},
  shorttitle = {Neuroevolution in {{Deep Learning}}},
  author = {Galv{\'a}n, Edgar},
  year = {2021},
  month = feb,
  journal = {arXiv:2102.08475 [cs]},
  eprint = {2102.08475},
  primaryclass = {cs},
  urldate = {2021-07-04},
  abstract = {A variety of methods have been applied to the architectural configuration and learning or training of artificial deep neural networks (DNN). These methods play a crucial role in the success or failure of the DNN for most problems and applications. Evolutionary Algorithms (EAs) are gaining momentum as a computationally feasible method for the automated optimisation of DNNs. Neuroevolution is a term which describes these processes of automated configuration and training of DNNs using EAs. However, the automatic design and/or training of these modern neural networks through evolutionary algorithms is computanalli expensive. Kimura's neutral theory of molecular evolution states that the majority of evolutionary changes at molecular level are the result of random fixation of selectively neutral mutations. A mutation from one gene to another is neutral if it does not affect the phenotype. This work discusses how neutrality, given certain conditions, can help to speed up the training/design of deep neural networks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Neural and Evolutionary Computing,Why ES in neural networks},
  file = {/home/luisaam/Zotero/storage/KQY32875/GalvÃ¡n - 2021 - Neuroevolution in Deep Learning The Role of Neutr.pdf;/home/luisaam/Zotero/storage/DPHC7L96/2102.html}
}

@article{gaoChannelInteractionNetworks,
  title = {Channel {{Interaction Networks}} for {{Fine-Grained Image Categorization}}. ({{arXiv}}:2003.05235v1 [Cs.{{CV}}])},
  author = {Gao, Yu and Han, Xintong and Wang, Xun and Huang, Weilin and Scott, Matthew R.},
  journal = {arXiv Computer Science},
  doi = {arXiv:2003.05235v1},
  abstract = {Fine-grained image categorization is challenging due to the subtle inter-class differences.We posit that exploiting the rich relationships between channels can help capture such differences since different channels correspond to different semantics. In this paper, we propose a channel interaction network (CIN), which models the channel-wise interplay both within an image and across images. For a single image, a self-channel interaction (SCI) module is proposed to explore channel-wise correlation within the image. This allows the model to learn the complementary features from the correlated channels, yielding stronger fine-grained features. Furthermore, given an image pair, we introduce a contrastive channel interaction (CCI) module to model the cross-sample channel interaction with a metric learning framework, allowing the CIN to distinguish the subtle visual differences between images. Our model can be trained efficiently in an end-to-end fashion without the need of multi-stage training and testing. Finally, comprehensive experiments are conducted on three publicly available benchmarks, where the proposed method consistently outperforms the state-of-theart approaches, such as DFL-CNN (Wang, Morariu, and Davis 2018) and NTS (Yang et al. 2018).},
  keywords = {Researcher App}
}

@inproceedings{gaoProgressiveFeatureInteraction2021,
  title = {Progressive {{Feature Interaction Search}} for {{Deep Sparse Network}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Gao, Chen and Li, Yinfeng and Yao, Quanming and Jin, Depeng and Li, Yong},
  year = {2021},
  volume = {34},
  pages = {392--403},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-06-13},
  abstract = {Deep sparse networks (DSNs), of which the crux is exploring the high-order feature interactions, have become the state-of-the-art on the prediction task with high-sparsity features. However, these models suffer from low computation efficiency, including large model size and slow model inference, which largely limits these models' application value. In this work, we approach this problem with neural architecture search by automatically searching the critical component in DSNs, the feature-interaction layer. We propose a distilled search space to cover the desired architectures with fewer parameters. We then develop a progressive search algorithm for efficient search on the space and well capture the order-priority property in sparse prediction tasks. Experiments on three real-world benchmark datasets show promising results of PROFIT in both accuracy and efficiency. Further studies validate the feasibility of our designed search space and search algorithm.},
  keywords = {feature interaction,Pruning,sparse neural networks},
  file = {/home/luisaam/Zotero/storage/LPQ2WSKW/Gao et al. - 2021 - Progressive Feature Interaction Search for Deep Sp.pdf}
}

@article{garbinDropoutVsBatch2020,
  title = {Dropout vs. Batch Normalization: An Empirical Study of Their Impact to Deep Learning},
  shorttitle = {Dropout vs. Batch Normalization},
  author = {Garbin, Christian and Zhu, Xingquan and Marques, Oge},
  year = {2020},
  month = may,
  journal = {Multimedia Tools and Applications},
  volume = {79},
  number = {19},
  pages = {12777--12815},
  issn = {1573-7721},
  doi = {10.1007/s11042-019-08453-9},
  urldate = {2021-07-07},
  abstract = {Overfitting and long training time are two fundamental challenges in multilayered neural network learning and deep learning in particular. Dropout and batch normalization are two well-recognized approaches to tackle these challenges. While both approaches share overlapping design principles, numerous research results have shown that they have unique strengths to improve deep learning. Many tools simplify these two approaches as a simple function call, allowing flexible stacking to form deep learning architectures. Although their usage guidelines are available, unfortunately no well-defined set of rules or comprehensive studies to investigate them concerning data input, network configurations, learning efficiency, and accuracy. It is not clear when users should consider using dropout and/or batch normalization, and how they should be combined (or used alternatively) to achieve optimized deep learning outcomes. In this paper we conduct an empirical study to investigate the effect of dropout and batch normalization on training deep learning models. We use multilayered dense neural networks and convolutional neural networks (CNN) as the deep learning models, and mix dropout and batch normalization to design different architectures and subsequently observe their performance in terms of training and test CPU time, number of parameters in the model (as a proxy for model size), and classification accuracy. The interplay between network structures, dropout, and batch normalization, allow us to conclude when and how dropout and batch normalization should be considered in deep learning. The empirical study quantified the increase in training time when dropout and batch normalization are used, as well as the increase in prediction time (important for constrained environments, such as smartphones and low-powered IoT devices). It showed that a non-adaptive optimizer (e.g. SGD) can outperform adaptive optimizers, but only at the cost of a significant amount of training times to perform hyperparameter tuning, while an adaptive optimizer (e.g. RMSProp) performs well without much tuning. Finally, it showed that dropout and batch normalization should be used in CNNs only with caution and experimentation (when in doubt and short on time to experiment, use only batch normalization).},
  langid = {english},
  keywords = {examples},
  file = {/home/luisaam/Zotero/storage/ZV7K9W32/Garbin et al. - 2020 - Dropout vs. batch normalization an empirical stud.pdf}
}

@article{garipovLossSurfacesMode2018,
  title = {Loss {{Surfaces}}, {{Mode Connectivity}}, and {{Fast Ensembling}} of {{DNNs}}},
  author = {Garipov, Timur and Izmailov, Pavel and Podoprikhin, Dmitrii and Vetrov, Dmitry and Wilson, Andrew Gordon},
  year = {2018},
  month = oct,
  journal = {arXiv:1802.10026 [cs, stat]},
  eprint = {1802.10026},
  primaryclass = {cs, stat},
  urldate = {2021-06-28},
  abstract = {The loss functions of deep neural networks are complex and their geometric properties are not well understood. We show that the optima of these complex loss functions are in fact connected by simple curves over which training and test accuracy are nearly constant. We introduce a training procedure to discover these high-accuracy pathways between modes. Inspired by this new geometric insight, we also propose a new ensembling method entitled Fast Geometric Ensembling (FGE). Using FGE we can train high-performing ensembles in the time required to train a single model. We achieve improved performance compared to the recent state-of-the-art Snapshot Ensembles, on CIFAR-10, CIFAR-100, and ImageNet.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/AAZQKCMF/Garipov et al. - 2018 - Loss Surfaces, Mode Connectivity, and Fast Ensembl.pdf;/home/luisaam/Zotero/storage/RDDCSYUX/1802.html}
}

@article{geEscapingSaddlePoints2015,
  title = {Escaping {{From Saddle Points}} --- {{Online Stochastic Gradient}} for {{Tensor Decomposition}}},
  author = {Ge, Rong and Huang, Furong and Jin, Chi and Yuan, Yang},
  year = {2015},
  month = mar,
  journal = {arXiv:1503.02101 [cs, math, stat]},
  eprint = {1503.02101},
  primaryclass = {cs, math, stat},
  urldate = {2021-08-06},
  abstract = {We analyze stochastic gradient descent for optimizing non-convex functions. In many cases for non-convex functions the goal is to find a reasonable local minimum, and the main concern is that gradient updates are trapped in saddle points. In this paper we identify strict saddle property for non-convex problem that allows for efficient optimization. Using this property we show that stochastic gradient descent converges to a local minimum in a polynomial number of iterations. To the best of our knowledge this is the first work that gives global convergence guarantees for stochastic gradient descent on non-convex functions with exponentially many local minima and saddle points. Our analysis can be applied to orthogonal tensor decomposition, which is widely used in learning a rich class of latent variable models. We propose a new optimization formulation for the tensor decomposition problem that has strict saddle property. As a result we get the first online algorithm for orthogonal tensor decomposition with global convergence guarantee.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Optimization,Saddle Points,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/84ZMMV8Y/Ge et al. - 2015 - Escaping From Saddle Points --- Online Stochastic .pdf;/home/luisaam/Zotero/storage/PUJR98XP/1503.html}
}

@article{GeometricDeepLearning,
  title = {Geometric {{Deep Learning}}: {{Going}} beyond {{Euclidean}} Data},
  journal = {IEEE Signal Processing Magazine},
  doi = {10.1109/MSP.2017.2693418},
  abstract = {Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains, such as graphs and manifolds. The purpose of this article is to overview different examples of geometric deep-learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.},
  keywords = {Researcher App}
}

@inproceedings{ghorbaniInvestigationNeuralNet2019,
  title = {An {{Investigation}} into {{Neural Net Optimization}} via {{Hessian Eigenvalue Density}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Ghorbani, Behrooz and Krishnan, Shankar and Xiao, Ying},
  year = {2019},
  month = may,
  pages = {2232--2241},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2021-07-23},
  langid = {english},
  keywords = {metrics for suitability for optimization,to read},
  file = {/home/luisaam/Zotero/storage/FK4WXWE7/Ghorbani et al. - 2019 - An Investigation into Neural Net Optimization via .pdf;/home/luisaam/Zotero/storage/JECG6S3A/Ghorbani et al. - 2019 - An Investigation into Neural Net Optimization via .pdf}
}

@inproceedings{ghorbaniInvestigationNeuralNet2019a,
  title = {An {{Investigation}} into {{Neural Net Optimization}} via {{Hessian Eigenvalue Density}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Ghorbani, Behrooz and Krishnan, Shankar and Xiao, Ying},
  year = {2019},
  month = may,
  pages = {2232--2241},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-02-07},
  abstract = {To understand the dynamics of training in deep neural networks, we study the evolution of the Hessian eigenvalue density throughout the optimization process. In non-batch normalized networks, we observe the rapid appearance of large isolated eigenvalues in the spectrum, along with a surprising concentration of the gradient in the corresponding eigenspaces. In a batch normalized network, these two effects are almost absent. We give a theoretical rationale to partially explain these phenomena. As part of this work, we adapt advanced tools from numerical linear algebra that allow scalable and accurate estimation of the entire Hessian spectrum of ImageNet-scale neural networks; this technique may be of independent interest in other applications.},
  langid = {english},
  keywords = {to read},
  file = {/home/luisaam/Zotero/storage/7IBN5VLJ/Ghorbani et al. - 2019 - An Investigation into Neural Net Optimization via .pdf;/home/luisaam/Zotero/storage/F7TQWF7Y/Ghorbani et al. - 2019 - An Investigation into Neural Net Optimization via .pdf}
}

@inproceedings{gilmerLossCurvaturePerspective2021,
  title = {A {{Loss Curvature Perspective}} on {{Training Instabilities}} of {{Deep Learning Models}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Gilmer, Justin and Ghorbani, Behrooz and Garg, Ankush and Kudugunta, Sneha and Neyshabur, Behnam and Cardoze, David and Dahl, George Edward and Nado, Zachary and Firat, Orhan},
  year = {2021},
  month = oct,
  urldate = {2023-11-29},
  abstract = {In this work, we study the evolution of the loss Hessian across many classification tasks in order to understand the effect the curvature of the loss has on the training dynamics. Whereas prior work has focused on how different learning rates affect the loss Hessian observed during training, we also analyze the effects of model initialization, architectural choices, and common training heuristics such as gradient clipping and learning rate warmup. Our results demonstrate that successful model and hyperparameter choices allow the early optimization trajectory to either avoid---or navigate out of---regions of high curvature and into flatter regions that tolerate a higher learning rate. Our results suggest a unifying perspective on how disparate mitigation strategies for training instability ultimately address the same underlying failure mode of neural network optimization, namely poor conditioning. Inspired by the conditioning perspective, we show that learning rate warmup can improve training stability just as much as batch normalization, layer normalization, MetaInit, GradInit, and Fixup initialization.},
  langid = {english},
  keywords = {Loss landscape,Second Order Information},
  file = {/home/luisaam/Zotero/storage/CJU6RNTG/Gilmer et al. - 2021 - A Loss Curvature Perspective on Training Instabili.pdf}
}

@inproceedings{glorotUnderstandingDifficultyTraining2010,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  booktitle = {Proceedings of the {{Thirteenth International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Glorot, Xavier and Bengio, Yoshua},
  year = {2010},
  month = mar,
  pages = {249--256},
  publisher = {{JMLR Workshop and Conference Proceedings}},
  issn = {1938-7228},
  urldate = {2021-07-09},
  abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental resul...},
  langid = {english},
  keywords = {initialization},
  file = {/home/luisaam/Zotero/storage/5LCMAZJM/Glorot y Bengio - 2010 - Understanding the difficulty of training deep feed.pdf;/home/luisaam/Zotero/storage/4PUXSCPU/glorot10a.html}
}

@book{goodfellowDeepLearning2016,
  title = {Deep Learning},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  publisher = {{MIT Press}}
}

@article{goodfellowGenerativeAdversarialNetworks2014,
  title = {Generative {{Adversarial Networks}}},
  author = {Goodfellow, Ian J. and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  year = {2014},
  month = jun,
  journal = {arXiv:1406.2661 [cs, stat]},
  eprint = {1406.2661},
  primaryclass = {cs, stat},
  urldate = {2021-07-14},
  abstract = {We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,examples,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/UKPY7XCS/Goodfellow et al. - 2014 - Generative Adversarial Networks.pdf;/home/luisaam/Zotero/storage/P83TGPHQ/1406.html}
}

@article{GradientFlowSparse,
  title = {Gradient {{Flow}} in {{Sparse Neural Networks}} and {{How Lottery Tickets Win}}. ({{arXiv}}:2010.03533v2 [Cs.{{LG}}] {{UPDATED}})},
  journal = {arXiv Computer Vision and Pattern Recognition},
  doi = {arXiv:2010.03533v2},
  abstract = {Sparse Neural Networks (NNs) can match the generalization of dense NNs using a fraction of the compute/storage for inference, and also have the potential to enable efficient training. However, naively training unstructured sparse NNs from random initialization results in significantly worse generalization, with the notable exceptions of Lottery Tickets (LTs) and Dynamic Sparse Training (DST). Through our analysis of gradient flow during training we attempt to answer: (1) why training unstructured sparse networks from random initialization performs poorly and; (2) what makes LTs and DST the exceptions? We show that sparse NNs have poor gradient flow at initialization and demonstrate the importance of using sparsity-aware initialization. Furthermore, we find that DST methods significantly improve gradient flow during training over traditional sparse training methods. Finally, we show that LTs do not improve gradient flow, rather their success lies in re-learning the pruning solution they are derived from - however, this comes at the cost of learning novel solutions.},
  keywords = {Researcher App}
}

@misc{GreedyLayerwiseTraining,
  title = {Greedy Layer-Wise Training of Deep Networks | {{Proceedings}} of the 19th {{International Conference}} on {{Neural Information Processing Systems}}},
  urldate = {2021-08-18},
  howpublished = {https://dl.acm.org/doi/10.5555/2976456.2976476},
  file = {/home/luisaam/Zotero/storage/T4QHZXWV/2976456.html}
}

@article{GroupChannelPruning,
  title = {Group Channel Pruning and Spatial Attention Distilling for Object Detection},
  journal = {Applied Intelligence},
  doi = {10.1007/s10489-022-03293-x},
  abstract = {Due to the over-parameterization of neural networks, many model compression methods based on pruning and quantization have emerged. They are remarkable in reducing the size, parameter number, and computational complexity of the model. However, most of the models compressed by such methods need the support of special hardware and software, which increases the deployment cost. Moreover, these methods are mainly used in classification tasks, and rarely directly used in detection tasks. To address these issues, for the object detection network we introduce a three-stage model compression method: dynamic sparse training, group channel pruning, and spatial attention distilling. Firstly, to select out the unimportant channels in the network and maintain a good balance between sparsity and accuracy, we put forward a dynamic sparse training method, which introduces a variable sparse rate, and the sparse rate will change with the training process of the network. Secondly, to reduce the effect of pruning on network accuracy, we propose a novel pruning method called group channel pruning. In particular, we divide the network into multiple groups according to the scales of the feature layer and the similarity of module structure in the network, and then we use different pruning thresholds to prune the channels in each group. Finally, to recover the accuracy of the pruned network, we use an improved knowledge distillation method for the pruned network. Especially, we extract spatial attention information from the feature maps of specific scales in each group as knowledge for distillation. In the experiments, we use YOLOv4 as the object detection network and PASCAL VOC as the training dataset. Our method reduces the parameters of the model by 64.7  \%   and the calculation by 34.9  \%  . When the input image size is 416{\texttimes}416, compared with the original network model with 256MB size and 87.1 accuracies, our compressed model achieves 86.6 accuracies with 90MB size. To demonstrate the generality of our method, we replace the backbone to Darknet53 and Mobilenet and also achieve satisfactory compression results.},
  keywords = {Pruning,Researcher App}
}

@misc{guoDeepFMFactorizationMachineBased2017,
  title = {{{DeepFM}}: {{A Factorization-Machine}} Based {{Neural Network}} for {{CTR Prediction}}},
  shorttitle = {{{DeepFM}}},
  author = {Guo, Huifeng and Tang, Ruiming and Ye, Yunming and Li, Zhenguo and He, Xiuqiang},
  year = {2017},
  month = mar,
  number = {arXiv:1703.04247},
  eprint = {1703.04247},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1703.04247},
  urldate = {2022-06-13},
  abstract = {Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide {\textbackslash}\& Deep model from Google, DeepFM has a shared input to its "wide" and "deep" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.},
  archiveprefix = {arxiv},
  keywords = {click through prediction,Computer Science - Computation and Language,Computer Science - Information Retrieval,feature interaction},
  file = {/home/luisaam/Zotero/storage/8HTPJIDG/Guo et al. - 2017 - DeepFM A Factorization-Machine based Neural Netwo.pdf;/home/luisaam/Zotero/storage/LBFLMTET/1703.html}
}

@article{guptaComplexityRequiredNeural2022,
  title = {Is {{Complexity Required}} for {{Neural Network Pruning}}? {{A Case Study}} on {{Global Magnitude Pruning}}},
  author = {Gupta, Manas and Camci, Efe and Keneta, Vishandi Rudy and Vaidyanathan, Abhishek and Kanodia, Ritwik and Foo, Chuan-Sheng and Min, Wu and Jie, Lin},
  year = {2022},
  journal = {arXiv preprint arXiv:2209.14624},
  eprint = {2209.14624},
  archiveprefix = {arxiv},
  keywords = {Pruning}
}

@misc{guptaComplexityRequiredNeural2022a,
  title = {Is {{Complexity Required}} for {{Neural Network Pruning}}? {{A Case Study}} on {{Global Magnitude Pruning}}},
  shorttitle = {Is {{Complexity Required}} for {{Neural Network Pruning}}?},
  author = {Gupta, Manas and Camci, Efe and Keneta, Vishandi Rudy and Vaidyanathan, Abhishek and Kanodia, Ritwik and Foo, Chuan-Sheng and Min, Wu and Jie, Lin},
  year = {2022},
  month = sep,
  number = {arXiv:2209.14624},
  eprint = {2209.14624},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.14624},
  urldate = {2022-11-08},
  abstract = {Pruning neural networks has become popular in the last decade when it was shown that a large number of weights can be safely removed from modern neural networks without compromising accuracy. Numerous pruning methods have been proposed since then, each claiming to be better than the previous. Many state-of-the-art (SOTA) techniques today rely on complex pruning methodologies utilizing importance scores, getting feedback through back-propagation or having heuristics-based pruning rules amongst others. We question this pattern of introducing complexity in order to achieve better pruning results. We benchmark these SOTA techniques against Global Magnitude Pruning (Global MP), a naive pruning baseline, to evaluate whether complexity is really needed to achieve higher performance. Global MP ranks weights in order of their magnitudes and prunes the smallest ones. Hence, in its vanilla form, it is one of the simplest pruning techniques. Surprisingly, we find that vanilla Global MP outperforms all the other SOTA techniques and achieves a new SOTA result. It also achieves good performance on FLOPs sparsification, which we find is enhanced, when pruning is conducted in a gradual fashion. We also find that Global MP is generalizable across tasks, datasets and models with superior performance. Moreover, a common issue that many pruning algorithms run into at high sparsity rates, namely, layer-collapse, can be easily fixed in Global MP by setting a minimum threshold of weights to be retained in each layer. Lastly, unlike many other SOTA techniques, Global MP does not require any additional algorithm specific hyper-parameters and is very straightforward to tune and implement. We showcase our findings on various models (WRN-28-8, ResNet-32, ResNet-50, MobileNet-V1 and FastGRNN) and multiple datasets (CIFAR-10, ImageNet and HAR-2). Code is available at https://github.com/manasgupta-1/GlobalMP.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Pruning},
  file = {/home/luisaam/Zotero/storage/J94F8WX2/Gupta et al. - 2022 - Is Complexity Required for Neural Network Pruning.pdf;/home/luisaam/Zotero/storage/L8MSQCPC/2209.html}
}

@article{guptaLearningPruneDeep,
  title = {Learning to {{Prune Deep Neural Networks}} via {{Reinforcement Learning}}. ({{arXiv}}:2007.04756v1 [Cs.{{AI}}])},
  author = {Gupta, Manas and Aravindan, Siddharth and Kalisz, Aleksandra and Chandrasekhar, Vijay and Jie, Lin},
  journal = {arXiv Computer Science},
  doi = {arXiv:2007.04756v1},
  abstract = {This paper proposes PuRL - a deep reinforcement learning (RL) based algorithm for pruning neural networks. Unlike current RL based model compression approaches where feedback is given only at the end of each episode to the agent, PuRL provides rewards at every pruning step. This enables PuRL to achieve sparsity and accuracy comparable to current state-of-the-art methods, while having a much shorter training cycle. PuRL achieves more than 80\% sparsity on the ResNet-50 model while retaining a Top-1 accuracy of 75.37\% on the ImageNet dataset. Through our experiments we show that PuRL is also able to sparsify already efficient architectures like MobileNet-V2. In addition to performance characterisation experiments, we also provide a discussion and analysis of the various RL design choices that went into the tuning of the Markov Decision Process underlying PuRL. Lastly, we point out that PuRL is simple to use and can be easily adapted for various architectures.},
  keywords = {Pruning,Reinforcement Learning,Researcher App}
}

@article{gur-ariGradientDescentHappens2018,
  title = {Gradient {{Descent Happens}} in a {{Tiny Subspace}}},
  author = {{Gur-Ari}, Guy and Roberts, Daniel A. and Dyer, Ethan},
  year = {2018},
  month = dec,
  journal = {arXiv:1812.04754 [cs, stat]},
  eprint = {1812.04754},
  primaryclass = {cs, stat},
  urldate = {2021-07-24},
  abstract = {We show that in a variety of large-scale deep learning scenarios the gradient dynamically converges to a very small subspace after a short period of training. The subspace is spanned by a few top eigenvectors of the Hessian (equal to the number of classes in the dataset), and is mostly preserved over long periods of training. A simple argument then suggests that gradient descent may happen mostly in this subspace. We give an example of this effect in a solvable model of classification, and we comment on possible implications for optimization and learning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,small networks,Statistics - Machine Learning,to read},
  file = {/home/luisaam/Zotero/storage/9EP4CI2A/Gur-Ari et al. - 2018 - Gradient Descent Happens in a Tiny Subspace.pdf;/home/luisaam/Zotero/storage/SW6RPRXP/1812.html}
}

@inproceedings{habibilashkariDIDarknetContemporaryApproach2020,
  title = {{{DIDarknet}}: {{A Contemporary Approach}} to {{Detect}} and {{Characterize}} the {{Darknet Traffic}} Using {{Deep Image Learning}}},
  shorttitle = {{{DIDarknet}}},
  booktitle = {2020 the 10th {{International Conference}} on {{Communication}} and {{Network Security}}},
  author = {Habibi Lashkari, Arash and Kaur, Gurdip and Rahali, Abir},
  year = {2020},
  month = nov,
  series = {{{ICCNS}} 2020},
  pages = {1--13},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3442520.3442521},
  urldate = {2022-03-17},
  abstract = {Darknet traffic classification is significantly important to categorize real-time applications. Although there are notable efforts to classify darknet traffic which rely heavily on existing datasets and machine learning classifiers, there are extremely few efforts to detect and characterize darknet traffic using deep learning. This work proposes a novel approach, named DeepImage, which uses feature selection to pick the most important features to create a gray image and feed it to a two-dimensional convolutional neural network to detect and characterize darknet traffic. Two encrypted traffic datasets are merged to create a darknet dataset to evaluate the proposed approach which successfully characterizes darknet traffic with 86\% accuracy.},
  isbn = {978-1-4503-8903-7},
  keywords = {characterization,cybersecurity,darknet,darknet traffic,deep learning,detection,encrypted traffic,tor,VPN}
}

@article{haiderComprehensiveOnlineNetwork,
  title = {Comprehensive {{Online Network Pruning}} via {{Learnable Scaling Factors}}. ({{arXiv}}:2010.02623v1 [Cs.{{CV}}])},
  author = {Haider, Muhammad Umair and Taj, Murtaza},
  journal = {arXiv Computer Science},
  doi = {arXiv:2010.02623v1},
  abstract = {One of the major challenges in deploying deep neural network architectures is their size which has an adverse effect on their inference time and memory requirements. Deep CNNs can either be pruned width-wise by removing filters based on their importance or depth-wise by removing layers and blocks. Width wise pruning (filter pruning) is commonly performed via learnable gates or switches and sparsity regularizers whereas pruning of layers has so far been performed arbitrarily by manually designing a smaller network usually referred to as a student network. We propose a comprehensive pruning strategy that can perform both width-wise as well as depth-wise pruning. This is achieved by introducing gates at different granularities (neuron, filter, layer, block) which are then controlled via an objective function that simultaneously performs pruning at different granularity during each forward pass. Our approach is applicable to wide-variety of architectures without any constraints on spatial dimensions or connection type (sequential, residual, parallel or inception). Our method has resulted in a compression ratio of 70\% to 90\% without noticeable loss in accuracy when evaluated on benchmark datasets.},
  keywords = {Pruning,Researcher App,to read}
}

@inproceedings{hanDeepCompressionCompressing2016,
  title = {Deep Compression: {{Compressing}} Deep Neural Network with Pruning, Trained Quantization and Huffman Coding},
  booktitle = {4th International Conference on Learning Representations, {{ICLR}} 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  author = {Han, Song and Mao, Huizi and Dally, William J.},
  editor = {Bengio, Yoshua and LeCun, Yann},
  year = {2016},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/journals/corr/HanMD15.bib},
  timestamp = {Fri, 20 Nov 2020 16:16:06 +0100},
  file = {/home/luisaam/Zotero/storage/TPSNARAJ/Han et al. - 2016 - Deep compression Compressing deep neural network .pdf}
}

@misc{hanDeepCompressionCompressing2016a,
  title = {Deep {{Compression}}: {{Compressing Deep Neural Networks}} with {{Pruning}}, {{Trained Quantization}} and {{Huffman Coding}}},
  shorttitle = {Deep {{Compression}}},
  author = {Han, Song and Mao, Huizi and Dally, William J.},
  year = {2016},
  month = feb,
  number = {arXiv:1510.00149},
  eprint = {1510.00149},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1510.00149},
  urldate = {2022-12-17},
  abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing,Pruning},
  file = {/home/luisaam/Zotero/storage/UGFSJAPR/Han et al. - 2016 - Deep Compression Compressing Deep Neural Networks.pdf;/home/luisaam/Zotero/storage/7YBZXY6S/1510.html}
}

@article{hanGridwiseControlMultiagent2019,
  title = {Grid-Wise Control for Multi-Agent Reinforcement Learning in Video Game {{AI}}},
  author = {Han, L. and Sun, P. and Xiong, J. and Wang, Q. and Sun, X. and Du, Y. and Liu, H. and Zhang, T.},
  year = {2019},
  journal = {36th International Conference on Machine Learning, ICML 2019},
  volume = {2019-},
  publisher = {{International Machine Learning Society (IMLS) rasmussen@ptd.net}},
  abstract = {We consider the problem of multi-agent reinforcement learning (MARL) in video game AI, where the agents are located in a spatial grid-world environment and the number of agents varies both within and across episodes. The challenge is to flexibly control an arbitrary number of agents while achieving effective collaboration. Existing MARL methods usually suffer from the trade-off between these two considerations. To address the issue, we propose a novel architecture that learns a spatial joint representation of all the agents and outputs grid-wise actions. Each agent will be controlled independently by taking the action from the grid it occupies. By viewing the state information as a grid feature map, we employ a convolutional encoder-decoder as the policy network. This architecture naturally promotes agent communication because of the large receptive field provided by the stacked convolutional layers. Moreover, the spatially shared convolutional parameters enable fast parallel exploration that the experiences discovered by one agent can be immediately transferred to others. The proposed method can be conveniently integrated with general reinforcement learning algorithms, e.g., PPO and Q-leaming. We demonstrate the effectiveness of the proposed method in extensive challenging multi-agent tasks in StarCraft II.}
}

@inproceedings{hanLearningBothWeights2015,
  title = {Learning Both {{Weights}} and {{Connections}} for {{Efficient Neural Network}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
  year = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-12-16},
  abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9{\texttimes}, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the total number of parameters can be reduced by 13{\texttimes}, from 138 million to 10.3 million, again with no loss of accuracy.},
  keywords = {Pruning},
  file = {/home/luisaam/Zotero/storage/J8R8VQWX/Han et al. - 2015 - Learning both Weights and Connections for Efficien.pdf}
}

@techreport{hansenCMAEvolutionStrategy2016,
  title = {The {{CMA Evolution Strategy}}: {{A Tutorial}}},
  author = {Hansen, Nikolaus},
  year = {2016},
  file = {/home/luisaam/Zotero/storage/F9M7RF9Q/Hansen - 2016 - The CMA Evolution Strategy A Tutorial.pdf}
}

@article{hanSurveyVisualTransformer2021,
  title = {A {{Survey}} on {{Visual Transformer}}},
  author = {Han, Kai and Wang, Yunhe and Chen, Hanting and Chen, Xinghao and Guo, Jianyuan and Liu, Zhenhua and Tang, Yehui and Xiao, An and Xu, Chunjing and Xu, Yixing and Yang, Zhaohui and Zhang, Yiman and Tao, Dacheng},
  year = {2021},
  month = jan,
  journal = {arXiv:2012.12556 [cs]},
  eprint = {2012.12556},
  primaryclass = {cs},
  urldate = {2021-08-04},
  abstract = {Transformer, first applied to the field of natural language processing, is a type of deep neural network mainly based on the self-attention mechanism. Thanks to its strong representation capabilities, researchers are looking at ways to apply transformer to computer vision tasks. In a variety of visual benchmarks, transformer-based models perform similar to or better than other types of networks such as convolutional and recurrent networks. Given its high performance and no need for human-defined inductive bias, transformer is receiving more and more attention from the computer vision community. In this paper, we review these visual transformer models by categorizing them in different tasks and analyzing their advantages and disadvantages. The main categories we explore include the backbone network, high/mid-level vision, low-level vision, and video processing. We also take a brief look at the self-attention mechanism in computer vision, as it is the base component in transformer. Furthermore, we include efficient transformer methods for pushing transformer into real device-based applications. Toward the end of this paper, we discuss the challenges and provide several further research directions for visual transformers.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Interactions,small networks},
  file = {/home/luisaam/Zotero/storage/BQ3A3H3F/Han et al. - 2021 - A Survey on Visual Transformer.pdf;/home/luisaam/Zotero/storage/B4GVQCNJ/2012.html}
}

@inproceedings{hassibiOptimalBrainSurgeon1993,
  title = {Optimal {{Brain Surgeon}}: {{Extensions}} and Performance Comparisons},
  shorttitle = {Optimal {{Brain Surgeon}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hassibi, Babak and Stork, David and Wolff, Gregory},
  year = {1993},
  volume = {6},
  publisher = {{Morgan-Kaufmann}},
  urldate = {2022-03-24},
  keywords = {Hessian,Pruning},
  file = {/home/luisaam/Zotero/storage/T28GKRXX/Hassibi et al. - 1993 - Optimal Brain Surgeon Extensions and performance .pdf}
}

@inproceedings{hassibiSecondOrderDerivatives1992,
  title = {Second Order Derivatives for Network Pruning: {{Optimal Brain Surgeon}}},
  shorttitle = {Second Order Derivatives for Network Pruning},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hassibi, Babak and Stork, David},
  year = {1992},
  volume = {5},
  publisher = {{Morgan-Kaufmann}},
  urldate = {2022-03-24},
  keywords = {Hessian,Pruning},
  file = {/home/luisaam/Zotero/storage/SCLNMUQA/Hassibi y Stork - 1992 - Second order derivatives for network pruning Opti.pdf}
}

@article{hassonDirectFitNature2020,
  title = {Direct {{Fit}} to {{Nature}}: {{An Evolutionary Perspective}} on {{Biological}} and {{Artificial Neural Networks}}},
  shorttitle = {Direct {{Fit}} to {{Nature}}},
  author = {Hasson, Uri and Nastase, Samuel A. and Goldstein, Ariel},
  year = {2020},
  month = feb,
  journal = {Neuron},
  volume = {105},
  number = {3},
  pages = {416--434},
  issn = {0896-6273},
  doi = {10.1016/j.neuron.2019.12.002},
  urldate = {2021-04-23},
  abstract = {Evolution is a blind fitting process by which organisms become adapted to their environment. Does the brain use similar brute-force fitting processes to learn how to perceive and act upon the world? Recent advances in artificial neural networks have exposed the power of optimizing millions of synaptic weights over millions of observations to operate robustly in real-world contexts. These models do not learn simple, human-interpretable rules or representations of the world; rather, they use local computations to interpolate over task-relevant manifolds in a high-dimensional parameter space. Counterintuitively, similar to evolutionary processes, over-parameterized models can be simple and parsimonious, as they provide a versatile, robust solution for learning a diverse set of functions. This new family of direct-fit models present a radical challenge to many of the theoretical assumptions in psychology and neuroscience. At the same time, this shift in perspective establishes unexpected links with developmental and ecological psychology.},
  langid = {english},
  keywords = {evolution,experimental design,interpolation,learning,neural networks},
  file = {/home/luisaam/Zotero/storage/R7H8NSLD/Hasson et al. - 2020 - Direct Fit to Nature An Evolutionary Perspective .pdf;/home/luisaam/Zotero/storage/UXIJIA8T/S089662731931044X.html}
}

@article{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  journal = {arXiv:1512.03385 [cs]},
  eprint = {1512.03385},
  primaryclass = {cs},
  urldate = {2021-08-04},
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,ResNets},
  file = {/home/luisaam/Zotero/storage/5QIS54A4/He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;/home/luisaam/Zotero/storage/JRYTSF49/1512.html}
}

@inproceedings{heFilterPruningGeometric2019,
  title = {Filter {{Pruning}} via {{Geometric Median}} for {{Deep Convolutional Neural Networks Acceleration}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Yang and Liu, Ping and Wang, Ziwei and Hu, Zhilan and Yang, Yi},
  year = {2019},
  month = jun,
  pages = {4335--4344},
  issn = {2575-7075},
  doi = {10.1109/CVPR.2019.00447},
  abstract = {Previous works utilized ``smaller-norm-less-important'' criterion to prune filters with smaller norm values in a convolutional neural network. In this paper, we analyze this norm-based criterion and point out that its effectiveness depends on two requirements that are not always met: (1) the norm deviation of the filters should be large; (2) the minimum norm of the filters should be small. To solve this problem, we propose a novel filter pruning method, namely Filter Pruning via Geometric Median (FPGM), to compress the model regardless of those two requirements. Unlike previous methods, FPGM compresses CNN models by pruning filters with redundancy, rather than those with``relatively less'' importance. When applied to two image classification benchmarks, our method validates its usefulness and strengths. Notably, on CIFAR-10, FPGM reduces more than 52\% FLOPs on ResNet-110 with even 2.69\% relative accuracy improvement. Moreover, on ILSVRC-2012, FPGM reduces more than 42\% FLOPs on ResNet-101 without top-5 accuracy drop, which has advanced the state-of-the-art. Code is publicly available on GitHub: https://github.com/he-y/filter-pruning-geometric-median},
  keywords = {Deep Learning,filter similarity,Others,Pruning},
  file = {/home/luisaam/Zotero/storage/WKLQZF7E/He et al. - 2019 - Filter Pruning via Geometric Median for Deep Convo.pdf;/home/luisaam/Zotero/storage/I7WX4UPG/8953212.html}
}

@article{hendersonDeepReinforcementLearning2019,
  title = {Deep {{Reinforcement Learning}} That {{Matters}}},
  author = {Henderson, Peter and Islam, Riashat and Bachman, Philip and Pineau, Joelle and Precup, Doina and Meger, David},
  year = {2019},
  month = jan,
  journal = {arXiv:1709.06560 [cs, stat]},
  eprint = {1709.06560},
  primaryclass = {cs, stat},
  urldate = {2021-07-14},
  abstract = {In recent years, significant progress has been made in solving challenging problems across various domains using deep reinforcement learning (RL). Reproducing existing work and accurately judging the improvements offered by novel methods is vital to sustaining this progress. Unfortunately, reproducing results for state-of-the-art deep RL methods is seldom straightforward. In particular, non-determinism in standard benchmark environments, combined with variance intrinsic to the methods, can make reported results tough to interpret. Without significance metrics and tighter standardization of experimental reporting, it is difficult to determine whether improvements over the prior state-of-the-art are meaningful. In this paper, we investigate challenges posed by reproducibility, proper experimental techniques, and reporting procedures. We illustrate the variability in reported metrics and results when comparing against common baselines and suggest guidelines to make future results in deep RL more reproducible. We aim to spur discussion about how to ensure continued progress in the field by minimizing wasted effort stemming from results that are non-reproducible and easily misinterpreted.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Reliability of Reinforcement Learning,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/BQXYE5CM/Henderson et al. - 2019 - Deep Reinforcement Learning that Matters.pdf;/home/luisaam/Zotero/storage/8B4M6AVD/1709.html}
}

@inproceedings{hendrycksBenchmarkingNeuralNetwork2018,
  title = {Benchmarking {{Neural Network Robustness}} to {{Common Corruptions}} and {{Perturbations}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Hendrycks, Dan and Dietterich, Thomas},
  year = {2018},
  month = sep,
  urldate = {2021-08-23},
  abstract = {We propose ImageNet-C to measure classifier corruption robustness and ImageNet-P to measure perturbation robustness},
  langid = {english},
  keywords = {Robustness},
  file = {/home/luisaam/Zotero/storage/9GHH8JHE/Hendrycks y Dietterich - 2018 - Benchmarking Neural Network Robustness to Common C.pdf;/home/luisaam/Zotero/storage/YNVHJPUP/forum.html}
}

@article{hendrycksManyFacesRobustness2021,
  title = {The {{Many Faces}} of {{Robustness}}: {{A Critical Analysis}} of {{Out-of-Distribution Generalization}}},
  shorttitle = {The {{Many Faces}} of {{Robustness}}},
  author = {Hendrycks, Dan and Basart, Steven and Mu, Norman and Kadavath, Saurav and Wang, Frank and Dorundo, Evan and Desai, Rahul and Zhu, Tyler and Parajuli, Samyak and Guo, Mike and Song, Dawn and Steinhardt, Jacob and Gilmer, Justin},
  year = {2021},
  month = jul,
  journal = {arXiv:2006.16241 [cs, stat]},
  eprint = {2006.16241},
  primaryclass = {cs, stat},
  urldate = {2021-08-23},
  abstract = {We introduce four new real-world distribution shift datasets consisting of changes in image style, image blurriness, geographic location, camera operation, and more. With our new datasets, we take stock of previously proposed methods for improving out-of-distribution robustness and put them to the test. We find that using larger models and artificial data augmentations can improve robustness on real-world distribution shifts, contrary to claims in prior work. We find improvements in artificial robustness benchmarks can transfer to real-world distribution shifts, contrary to claims in prior work. Motivated by our observation that data augmentations can help with real-world distribution shifts, we also introduce a new data augmentation method which advances the state-of-the-art and outperforms models pretrained with 1000 times more labeled data. Overall we find that some methods consistently help with distribution shifts in texture and local image statistics, but these methods do not help with some other distribution shifts like geographic changes. Our results show that future research must study multiple distribution shifts simultaneously, as we demonstrate that no evaluated method consistently improves robustness.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Robustness,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/P2YJPUI6/Hendrycks et al. - 2021 - The Many Faces of Robustness A Critical Analysis .pdf;/home/luisaam/Zotero/storage/4GBQLD9P/2006.html}
}

@article{heNonlinearitiesActivationsSubstantially2020,
  title = {Nonlinearities in Activations Substantially Shape the Loss Surfaces of Neural Networks},
  author = {He, Fengxiang and Wang, Bohan and Tao, D.},
  year = {2020},
  journal = {undefined},
  urldate = {2021-06-28},
  abstract = {Understanding the loss surfaces of neural networks is fundamentally important to understanding deep learning. This paper presents how the nonlinearities in activations substantially shape the loss surfaces of neural networks. We first prove that the loss surface of every neural network has infinite spurious local minima, which are defined as the local minima with higher empirical risks than the global minima. Our result holds for any neural network with arbitrary depth and arbitrary piecewise linear activation functions (excluding linear functions) under most loss functions in practice. This result demonstrates that nonlinear networks possess substantial differences to the well-studied linear neural networks. Essentially, the underlying assumptions for the above result are consistent with most practical circumstances where the output layer is narrower than any hidden layer. We further prove a theorem that draws a big picture for the loss surfaces of nonlinear neural networks from the following respects. (1) Smooth and multilinear partition: the loss surface is partitioned into multiple smooth and multilinear open cells. (2) Local analogous convexity: within every cell, local minima are equally good, and equivalently, they are all global minima in the cell. (3) Local minima valley: some local minima are concentrated into a valley in some cell, sharing the same empirical risk. (4) Linear collapse: when all activations are linear, the partitioned loss surface collapses to one single cell, which includes linear neural networks as a simplified case. The second result holds for one-hidden-layer networks for regression under convex loss, while all others apply to networks of arbitrary depth.},
  langid = {english},
  keywords = {Why no global second order},
  file = {/home/luisaam/Zotero/storage/KQKSE3J8/fce76323018003ec025869fcc1260d33e0109b95.html}
}

@article{hernandezMeasuringAlgorithmicEfficiency2020,
  title = {Measuring the {{Algorithmic Efficiency}} of {{Neural Networks}}},
  author = {Hernandez, Danny and Brown, Tom B.},
  year = {2020},
  month = may,
  journal = {arXiv:2005.04305 [cs, stat]},
  eprint = {2005.04305},
  primaryclass = {cs, stat},
  urldate = {2021-12-15},
  abstract = {Three factors drive the advance of AI: algorithmic innovation, data, and the amount of compute available for training. Algorithmic progress has traditionally been more difficult to quantify than compute and data. In this work, we argue that algorithmic progress has an aspect that is both straightforward to measure and interesting: reductions over time in the compute needed to reach past capabilities. We show that the number of floating-point operations required to train a classifier to AlexNet-level performance on ImageNet has decreased by a factor of 44x between 2012 and 2019. This corresponds to algorithmic efficiency doubling every 16 months over a period of 7 years. By contrast, Moore's Law would only have yielded an 11x cost improvement. We observe that hardware and algorithmic efficiency gains multiply and can be on a similar scale over meaningful horizons, which suggests that a good model of AI progress should integrate measures from both.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Deep learning Efficiency,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/Z6W667BX/Hernandez y Brown - 2020 - Measuring the Algorithmic Efficiency of Neural Net.pdf;/home/luisaam/Zotero/storage/8F6KKF6W/2005.html}
}

@article{hintonDistillingKnowledgeNeural2015,
  title = {Distilling the Knowledge in a Neural Network},
  author = {Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  year = {2015},
  journal = {arXiv preprint arXiv:1503.02531},
  eprint = {1503.02531},
  abstract = {A very simple way to improve the performance of almost any machine learning algorithm is to train many different models on the same data and then to average their predictions [3]. Unfortunately, making predictions using a whole ensemble of models is cumbersome and may be too computationally expensive to allow de- ployment to a large number of users, especially if the individual models are large neural nets. Caruana and his collaborators [1] have shown that it is possible to compress the knowledge in an ensemble into a single model which is much eas- ier to deploy and we develop this approach further using a different compression technique. We achieve some surprising results on MNIST and we show that we can significantly improve the acoustic model of a heavily used commercial system by distilling the knowledge in an ensemble of models into a single model. We also introduce a new type of ensemble composed of one or more full models and many specialist models which learn to distinguish fine-grained classes that the full mod- els confuse. Unlike a mixture of experts, these specialist models can be trained rapidly and in parallel.},
  archiveprefix = {arxiv}
}

@article{hintonReducingDimensionalityData2006,
  title = {Reducing the {{Dimensionality}} of {{Data}} with {{Neural Networks}}},
  author = {Hinton, G. E. and Salakhutdinov, R. R.},
  year = {2006},
  month = jul,
  journal = {Science},
  volume = {313},
  number = {5786},
  pages = {504--507},
  publisher = {{American Association for the Advancement of Science}},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1127647},
  urldate = {2021-08-23},
  abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such ``autoencoder'' networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data. Neural networks can be used to reduce accurately high-dimensional data to lower dimensional representations for pattern recognition tasks. Neural networks can be used to reduce accurately high-dimensional data to lower dimensional representations for pattern recognition tasks.},
  chapter = {Report},
  copyright = {American Association for the Advancement of Science},
  langid = {english},
  pmid = {16873662},
  keywords = {examples},
  file = {/home/luisaam/Zotero/storage/MBDKJQ8C/504.html}
}

@article{hochreiterFlatMinima1997,
  title = {Flat {{Minima}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = jan,
  journal = {Neural Computation},
  volume = {9},
  number = {1},
  pages = {1--42},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.1.1},
  urldate = {2021-08-18},
  abstract = {We present a new algorithm for finding low-complexity neural networks with high generalization capability. The algorithm searches for a ``flat'' minimum of the error function. A flat minimum is a large connected region in weight space where the error remains approximately constant. An MDL-based, Bayesian argument suggests that flat minima correspond to ``simple'' networks and low expected overfitting. The argument is based on a Gibbs algorithm variant and a novel way of splitting generalization error into underfitting and overfitting error. Unlike many previous approaches, ours does not require gaussian assumptions and does not depend on a ``good'' weight prior. Instead we have a prior over input output functions, thus taking into account net architecture and training set. Although our algorithm requires the computation of second-order derivatives, it has backpropagation's order of complexity. Automatically, it effectively prunes units, weights, and input lines. Various experiments with feedforward and recurrent nets are described. In an application to stock market prediction, flat minimum search outperforms conventional backprop, weight decay, and ``optimal brain surgeon/optimal brain damage.''},
  keywords = {Sharp Minima},
  file = {/home/luisaam/Zotero/storage/NBMIX7EP/Hochreiter y Schmidhuber - 1997 - Flat Minima.pdf;/home/luisaam/Zotero/storage/WJXIWL4N/Flat-Minima.html}
}

@article{hochreiterLongShortTermMemory1997,
  title = {Long {{Short-Term Memory}}},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  issn = {0899-7667},
  doi = {10.1162/neco.1997.9.8.1735},
  urldate = {2022-01-31},
  abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  file = {/home/luisaam/Zotero/storage/PHS9SQPM/Long-Short-Term-Memory.html}
}

@article{hodDetectingModularityDeep2021,
  title = {Detecting {{Modularity}} in {{Deep Neural Networks}}},
  author = {Hod, Shlomi and Casper, Stephen and Filan, Daniel and Wild, Cody and Critch, Andrew and Russell, Stuart},
  year = {2021},
  month = oct,
  urldate = {2022-11-23},
  abstract = {A neural network is modular to the extent that parts of its computational graph (i.e. structure) can be represented as performing some comprehensible subtask relevant to the overall task (i.e. functionality). Are modern deep neural networks modular? How can this be quantified? In this paper, we consider the problem of assessing the modularity exhibited by a partitioning of a network's neurons. We propose two proxies for this: importance, which reflects how crucial sets of neurons are to network performance; and coherence, which reflects how consistently their neurons associate with features of the inputs. To measure these proxies, we develop a set of statistical methods based on techniques conventionally used to interpret individual neurons. We apply the proxies to partitionings generated by spectrally clustering a graph representation of the network's neurons with edges determined either by network weights or correlations of activations. We show that these partitionings, even ones based only on weights (i.e. strictly from non-runtime analysis), reveal groups of neurons that are important and coherent. These results suggest that graph-based partitioning can reveal modularity and help us understand how deep neural networks function.},
  langid = {english},
  keywords = {Explainability,Pruning},
  file = {/home/luisaam/Zotero/storage/6BHLDD2E/Hod et al. - 2021 - Detecting Modularity in Deep Neural Networks.pdf;/home/luisaam/Zotero/storage/44JVBDHY/forum.html}
}

@article{hoeflerSparsityDeepLearning,
  title = {Sparsity in {{Deep Learning}}: {{Pruning}} and Growth for Efficient Inference and Training in Neural Networks},
  author = {Hoefler, Torsten and Alistarh, Dan and {Ben-Nun}, Tal and Dryden, Nikoli},
  pages = {124},
  abstract = {The growing energy and performance costs of deep learning have driven the community to reduce the size of neural networks by selectively pruning components. Similarly to their biological counterparts, sparse networks generalize just as well, sometimes even better than, the original dense networks. Sparsity promises to reduce the memory footprint of regular networks to fit mobile devices, as well as shorten training time for ever growing networks. In this paper, we survey prior work on sparsity in deep learning and provide an extensive tutorial of sparsification for both inference and training. We describe approaches to remove and add elements of neural networks, different training strategies to achieve model sparsity, and mechanisms to exploit sparsity in practice. Our work distills ideas from more than 300 research papers and provides guidance to practitioners who wish to utilize sparsity today, as well as to researchers whose goal is to push the frontier forward. We include the necessary background on mathematical methods in sparsification, describe phenomena such as early structure adaptation, the intricate relations between sparsity and the training process, and show techniques for achieving acceleration on real hardware. We also define a metric of pruned parameter efficiency that could serve as a baseline for comparison of different sparse networks. We close by speculating on how sparsity can improve future workloads and outline major open problems in the field.},
  langid = {english},
  file = {/home/luisaam/Zotero/storage/CJ28T2RH/Hoeï¬er et al. - Sparsity in Deep Learning Pruning and growth for .pdf}
}

@book{hosmerAppliedLogisticRegression2013,
  title = {Applied {{Logistic Regression}}: {{Third Edition}}},
  author = {Hosmer, David W. and Lemeshow, Stanley and Sturdivant, Rodney X.},
  year = {2013},
  journal = {Applied Logistic Regression: Third Edition},
  doi = {10.1002/9781118548387},
  abstract = {A new edition of the definitive guide to logistic regression modelingfor health science and other applications This thoroughly expanded Third Edition provides an easily accessible introduction to the logistic regression (LR) model and highlights the power of this model by examining the relationship between a dichotomous outcome and a set of covariables. Applied Logistic Regression, Third Edition emphasizes applications in the health sciences and handpicks topics that best suit the use of modern statistical software. The book provides readers with state-of-the-art techniques for building, interpreting, and assessing the performance of LR models. New and updated features include: A chapter on the analysis of correlated outcome data A wealth of additional material for topics ranging from Bayesian methods to assessing model fit Rich data sets from real-world studies that demonstrate each method under discussion Detailed examples and interpretation of the presented results as well as exercises throughout Applied Logistic Regression, Third Edition is a must-have guide for professionals and researchers who need to model nominal or ordinal scaled outcome variables in public health, medicine, and the social sciences as well as a wide range of other fields and disciplines.},
  isbn = {978-1-118-54838-7},
  file = {/home/luisaam/Zotero/storage/RDI6MWLJ/(Wiley Series in Probability and Statistics) David W. Hosmer, Stanley Lemeshow, Rodney X. Sturdivant (auth.), Walter A. Shewhart, Samuel S. Wilks (eds.) - Applied Logistic Regression-Wiley (2013).pdf}
}

@misc{howardMobileNetsEfficientConvolutional2017,
  title = {{{MobileNets}}: {{Efficient Convolutional Neural Networks}} for {{Mobile Vision Applications}}},
  shorttitle = {{{MobileNets}}},
  author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  year = {2017},
  month = apr,
  number = {arXiv:1704.04861},
  eprint = {1704.04861},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1704.04861},
  urldate = {2022-12-18},
  abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,examples,small networks},
  file = {/home/luisaam/Zotero/storage/UPQQDWR5/Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf;/home/luisaam/Zotero/storage/PFL5GQJB/1704.html}
}

@article{HowArePolicy,
  title = {How Are Policy Gradient Methods Affected by the Limits of Control?. ({{arXiv}}:2206.06863v1 [Math.{{OC}}])},
  journal = {arXiv Optimization and Control},
  doi = {arXiv:2206.06863v1},
  abstract = {We study stochastic policy gradient methods from the perspective of control-theoretic limitations. Our main result is that ill-conditioned linear systems in the sense of Doyle inevitably lead to noisy gradient estimates. We also give an example of a class of stable systems in which policy gradient methods suffer from the curse of dimensionality. Our results apply to both state feedback and partially observed systems.},
  keywords = {Researcher App}
}

@article{HowFindRead,
  title = {How to Find, Read and Organize Papers},
  journal = {Nature},
  doi = {10.1038/d41586-022-01878-7},
  abstract = {Maya Gosztyla decided to rethink her approach to research papers after she had trouble keeping track of the published literature. Maya Gosztyla decided to rethink her approach to research papers after she had trouble keeping track of the published literature.},
  keywords = {Researcher App}
}

@article{huangGPipeEfficientTraining2019,
  title = {{{GPipe}}: {{Efficient Training}} of {{Giant Neural Networks}} Using {{Pipeline Parallelism}}},
  shorttitle = {{{GPipe}}},
  author = {Huang, Yanping and Cheng, Youlong and Bapna, Ankur and Firat, Orhan and Chen, Mia Xu and Chen, Dehao and Lee, HyoukJoong and Ngiam, Jiquan and Le, Quoc V. and Wu, Yonghui and Chen, Zhifeng},
  year = {2019},
  month = jul,
  journal = {arXiv:1811.06965 [cs]},
  eprint = {1811.06965},
  primaryclass = {cs},
  urldate = {2021-08-10},
  abstract = {Scaling up deep neural network capacity has been known as an effective approach to improving model quality for several different machine learning tasks. In many cases, increasing model capacity beyond the memory limit of a single accelerator has required developing special algorithms or infrastructure. These solutions are often architecture-specific and do not transfer to other tasks. To address the need for efficient and task-independent model parallelism, we introduce GPipe, a pipeline parallelism library that allows scaling any network that can be expressed as a sequence of layers. By pipelining different sub-sequences of layers on separate accelerators, GPipe provides the flexibility of scaling a variety of different networks to gigantic sizes efficiently. Moreover, GPipe utilizes a novel batch-splitting pipelining algorithm, resulting in almost linear speedup when a model is partitioned across multiple accelerators. We demonstrate the advantages of GPipe by training large-scale neural networks on two different tasks with distinct network architectures: (i) Image Classification: We train a 557-million-parameter AmoebaNet model and attain a top-1 accuracy of 84.4\% on ImageNet-2012, (ii) Multilingual Neural Machine Translation: We train a single 6-billion-parameter, 128-layer Transformer model on a corpus spanning over 100 languages and achieve better quality than all bilingual models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,examples},
  file = {/home/luisaam/Zotero/storage/J24CMLBG/Huang et al. - 2019 - GPipe Efficient Training of Giant Neural Networks.pdf;/home/luisaam/Zotero/storage/EQN8C6GU/1811.html}
}

@article{huangUnderstandingGeneralizationVisualizations2020,
  title = {Understanding {{Generalization}} through {{Visualizations}}},
  author = {Huang, W. Ronny and Emam, Zeyad and Goldblum, Micah and Fowl, Liam and Terry, Justin K. and Huang, Furong and Goldstein, Tom},
  year = {2020},
  month = nov,
  journal = {arXiv:1906.03291 [cs, stat]},
  eprint = {1906.03291},
  primaryclass = {cs, stat},
  urldate = {2021-06-28},
  abstract = {The power of neural networks lies in their ability to generalize to unseen data, yet the underlying reasons for this phenomenon remain elusive. Numerous rigorous attempts have been made to explain generalization, but available bounds are still quite loose, and analysis does not always lead to true understanding. The goal of this work is to make generalization more intuitive. Using visualization methods, we discuss the mystery of generalization, the geometry of loss landscapes, and how the curse (or, rather, the blessing) of dimensionality causes optimizers to settle into minima that generalize well.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Loss Landscape,Statistics - Machine Learning,Why no global second order},
  file = {/home/luisaam/Zotero/storage/9ETF9PKK/Huang et al. - 2020 - Understanding Generalization through Visualization.pdf;/home/luisaam/Zotero/storage/PGGDD2IN/1906.html}
}

@inproceedings{humbleSoftMaskingCostConstrained2022,
  title = {Soft {{Masking}} for {{Cost-Constrained Channel Pruning}}},
  booktitle = {Computer {{Vision}}{\textendash}{{ECCV}} 2022: 17th {{European Conference}}, {{Tel Aviv}}, {{Israel}}, {{October}} 23{\textendash}27, 2022, {{Proceedings}}, {{Part XI}}},
  author = {Humble, Ryan and Shen, Maying and Latorre, Jorge Albericio and Darve, Eric and Alvarez, Jose},
  year = {2022},
  pages = {641--657},
  publisher = {{Springer}},
  keywords = {Pruning,sparse neural networks}
}

@inproceedings{igelNeuroevolutionReinforcementLearning2003,
  title = {Neuroevolution for Reinforcement Learning Using Evolution Strategies},
  booktitle = {The 2003 {{Congress}} on {{Evolutionary Computation}}, 2003. {{CEC}} '03.},
  author = {Igel, C.},
  year = {2003},
  month = dec,
  volume = {4},
  pages = {2588-2595 Vol.4},
  doi = {10.1109/CEC.2003.1299414},
  abstract = {We apply the CMA-ES, an evolution strategy which efficiently adapts the covariance matrix of the mutation distribution, to the optimization of the weights of neural networks for solving reinforcement learning problems. It turns out that the topology of the networks considerably influences the time to find a suitable control strategy. Still, our results with fixed network topologies are significantly better than those reported for the best evolutionary method so far, which adapts both the weights and the structure of the networks.},
  keywords = {Covariance matrix,Delay,Evolutionary computation,Genetic mutations,Learning,Network topology,Neural networks,Optimization methods,Search methods,Stochastic processes},
  file = {/home/luisaam/Zotero/storage/HL9R3LCE/Igel - 2003 - Neuroevolution for reinforcement learning using ev.pdf}
}

@article{ImplicitSelfRegularizationDeep,
  title = {Implicit {{Self-Regularization}} in {{Deep Neural Networks}}: {{Evidence}} from {{Random Matrix Theory}} and {{Implications}} for {{Learning}}},
  journal = {Journal of Machine Learning Research},
  doi = {79.12958.9e20efa6-ae22-4757-9933-f6d8c1b18cb2.1628643350},
  abstract = {Random Matrix Theory (RMT) is applied to analyze the weight matrices of Deep Neural Networks (DNNs), including both production quality, pre-trained models such as AlexNet and Inception, and smaller models trained from scratch, such as LeNet5 and a miniature-AlexNet.  Empirical and theoretical results clearly indicate that the DNN training process itself implicitly implements a form of Self-Regularization, implicitly sculpting a more regularized energy or penalty landscape.  In particular, the empirical spectral density (ESD) of DNN layer matrices displays signatures of traditionally-regularized statistical models, even in the absence of exogenously specifying traditional forms of explicit regularization, such as Dropout or Weight Norm constraints.  Building on relatively recent results in RMT, most notably its extension to Universality classes of Heavy-Tailed matrices, and applying them to these empirical results, we develop a theory to identify 5+1 Phases of Training, corresponding to increasing amounts of Implicit Self-Regularization.  These phases can be observed during the training process as well as in the final learned DNNs.  For smaller and/or older DNNs, this Implicit Self-Regularization is like traditional Tikhonov regularization, in that there is a "size scale" separating signal from noise.  For state-of-the-art DNNs, however, we identify a novel form of Heavy-Tailed Self-Regularization, similar to the self-organization seen in the statistical physics of disordered systems (such as classical models of actual neural activity).  This results from correlations arising at all size scales, which for DNNs arises implicitly due to the training process itself.  This implicit Self-Regularization can depend strongly on the many knobs of the training process.  In particular, we demonstrate that we can cause a small model to exhibit all 5+1 phases of training simply by changing the batch size.  Our results suggest that large, well-trained DNN architectures should exhibit Heavy-Tailed Self-Regularization, and we discuss the theoretical and practical implications of this.},
  keywords = {Researcher App}
}

@article{InexactAugmentedLagrangian,
  title = {An {{Inexact Augmented Lagrangian Algorithm}} for {{Training Leaky ReLU Neural Network}} with {{Group Sparsity}}. ({{arXiv}}:2205.05428v1 [Math.{{OC}}])},
  journal = {arXiv Optimization and Control},
  doi = {arXiv:2205.05428v1},
  abstract = {The leaky ReLU network with a group sparse regularization term has been widely used in the recent years. However, training such a network yields a nonsmooth nonconvex optimization problem and there exists a lack of approaches to compute a stationary point deterministically. In this paper, we first resolve the multi-layer composite term in the original optimization problem by introducing auxiliary variables and additional constraints. We show the new model has a nonempty and bounded solution set and its feasible set satisfies the Mangasarian-Fromovitz constraint qualification. Moreover, we show the relationship between the new model and the original problem. Remarkably, we propose an inexact augmented Lagrangian algorithm for solving the new model and show the convergence of the algorithm to a KKT point. Numerical experiments demonstrate that our algorithm is more efficient for training sparse leaky ReLU neural networks than some well-known algorithms.},
  keywords = {Researcher App}
}

@article{InfiniteHorizonReachAvoidZeroSum,
  title = {Infinite-{{Horizon Reach-Avoid Zero-Sum Games}} via {{Deep Reinforcement Learning}}. ({{arXiv}}:2203.10142v1 [Eess.{{SY}}])},
  journal = {arXiv Optimization and Control},
  doi = {arXiv:2203.10142v1},
  abstract = {In this paper, we consider the infinite-horizon reach-avoid zero-sum game problem, where the goal is to find a set in the state space, referred to as the reach-avoid set, such that the system starting at a state therein could be controlled to reach a given target set without violating constraints under the worst-case disturbance. We address this problem by designing a new value function with a contracting Bellman backup, where the super-zero level set, i.e., the set of states where the value function is evaluated to be non-negative, recovers the reach-avoid set. Building upon this, we prove that the proposed method can be adapted to compute the viability kernel, or the set of states which could be controlled to satisfy given constraints, and the backward reachable set, or the set of states that could be driven towards a given target set. Finally, we propose to alleviate the curse of dimensionality issue in high-dimensional problems by extending Conservative Q-Learning, a deep reinforcement learning technique, to learn a value function such that the super-zero level set of the learned value function serves as a (conservative) approximation to the reach-avoid set. Our theoretical and empirical results suggest that the proposed method could learn reliably the reach-avoid set and the optimal control policy even with neural network approximation.},
  keywords = {Researcher App}
}

@article{InteroceptionMentalHealth,
  title = {Interoception and Mental Health: Integrating Mind Body and Brain Interactions},
  journal = {Research in Practice in Neuroscience},
  doi = {dBmbFS41hIfgI1IwH5r8},
  abstract = {OCT 24    Interoception and mental health: integrating mind body and brain interactions    04:00 pm BST/ 03:00 pm GMT      FREE          ~        In our next Researcher Live series, we will be focusing on `Brain-body Interactions and new treatments for psychiatric disorders' {\textendash} bringing you four fantastic speakers.~    ~    Join our~  first episode   on   24  th   October at 4pm BST / 3pm GMT for a talk with Dr. Sahib Khalsa discussing the critical role of interoception in mental health. Sign up   here   to receive email reminders for this series.      ~    What are we going to talk about in this episode?    ~    Our internal organs emit a cascade of signals which rhythmically propagate throughout the body before reaching the nervous system. However, these `interoceptive' signals do not usually affect our conscious day-to-day experiences except when our mental or physical health is disturbed; knowing whether the problem lies within the body or brain is thus one of the main unsolved challenges facing psychiatry and medicine. During this event Dr. Khalsa discusses how interoception is critical for understanding how crosstalk between the body and brain influences mental health.    ~    The slides for this event can be found   here  .    ~      Series programme:        24  th   October, 4 pm BST / 3 pm GMT - `  Interoception and mental health: integrating mind body and brain interactions  ' with Dr Sahib Khalsa, University of Tulsa      ~      24  th   October, 6 pm BST / 5 pm GMT {\textendash} `  The brain-body axis in neurodevelopmental conditions  '   with Dr Eleanor Palser, University of California, San Francisco~      ~      27  th   October, 4 pm BST / 3 pm GMT {\textendash} `  How the brain processes gut feelings  '   with Dr Ryan Smith, University of Tulsa      ~      28  th   October, 3 pm BST / 2 pm GMT {\textendash} `  Heartbeats Influence Perception and Motor Activity'   with Dr Esra AI,~Columbia University      ~      If you'd like to present at your own Researcher Live event, please email   kristine.lennie@researcher-app.com            Date and Time    Monday, October 24, 2022  04:00 pm BST/ 03:00 pm GMT          Speakers        Dr Sahib Khalsa    Dr. Khalsa graduated from the Medical Scientist Training Program at the University of Iowa, receiving M.D. and Ph.D. (neuroscience) degrees. He is currently the Director of Clinical Operations at the Laureate Institute for Brain Research, and an Associate Professor of Community Medicine at the University of Tulsa. Dr. Khalsa's research investigates the role of interoception in mental health, with a focus on understanding how changes in internal physiological states influence body perception and the functioning of the human nervous system. His studies utilize a variety of approaches to probe cardiovascular, respiratory, and gastrointestinal interoception including via pharmacological and non-pharmacological techniques, functional magnetic resonance imaging (fMRI), electroencephalography (EEG), and computational modeling.},
  keywords = {Researcher App}
}

@article{InterplayDepthNeural,
  title = {Interplay between Depth of Neural Networks and Locality of Target Functions. ({{arXiv}}:2201.12082v1 [Cs.{{LG}}])},
  journal = {arXiv Disordered Systems and Neural Networks},
  doi = {arXiv:2201.12082v1},
  abstract = {It has been recognized that heavily overparameterized deep neural networks (DNNs) exhibit surprisingly good generalization performance in various machine-learning tasks. Although benefits of depth have been investigated from different perspectives such as the approximation theory and the statistical learning theory, existing theories do not adequately explain the empirical success of overparameterized DNNs. In this work, we report a remarkable interplay between depth and locality of a target function. We introduce -local and -global functions, and find that depth is beneficial for learning local functions but detrimental to learning global functions. This interplay is not properly captured by the neural tangent kernel, which describes an infinitely wide neural network within the lazy learning regime.},
  keywords = {Researcher App}
}

@article{izmailovAveragingWeightsLeads2019,
  title = {Averaging {{Weights Leads}} to {{Wider Optima}} and {{Better Generalization}}},
  author = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  year = {2019},
  month = feb,
  journal = {arXiv:1803.05407 [cs, stat]},
  eprint = {1803.05407},
  primaryclass = {cs, stat},
  urldate = {2021-07-29},
  abstract = {Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much flatter solutions than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,flat optimizers,Optimization,small networks,Statistics - Machine Learning,stochastic pruning},
  file = {/home/luisaam/Zotero/storage/SYN54PHF/Izmailov et al. - 2019 - Averaging Weights Leads to Wider Optima and Better.pdf;/home/luisaam/Zotero/storage/ENGVW3SJ/1803.html}
}

@article{jadavOptimizingWeightsArtificial2012,
  title = {Optimizing {{Weights}} of {{Artificial Neural Networks}} Using {{Genetic Algorithms}}},
  author = {Jadav, Kinjal and Panchal, Mahesh},
  year = {2012},
  journal = {undefined},
  urldate = {2021-07-12},
  abstract = {Artificial Neural Networks have a number of properties which make them psuitable to solve complex pattern classification problems. Their applications to some real world problems has been adopted by the lack of a training algorithm. This algorithms finds a nearly globally optimal set of weights in a relatively short time. Back propagation is one of the training algorithm of the Artificial neural network. However, training the neural networks using backpropagation algorithm may cause two main drawbacks: trapping into local minima and converging slowly. In view of these limitations of back-propagation neural networks, global search technique such as Genetic algorithm have been presented to overcome these shortcomings. Genetic algorithms are a class of optimization procedures which are good at exploring a large and complex space in an intelligent way. It finds values close to the global optimum. Hence, they are well suited to the problem of training and optimize weights of Artificial Neural Networks. In this paper the use of Genetic algorithms to optimize weights of Artificial Neural Networks is shown.},
  langid = {english},
  file = {/home/luisaam/Zotero/storage/7AKCS87U/f52731379b763894dc378471e00a9dbe5ab7cda8.html}
}

@article{jaderbergPopulationBasedTraining2017,
  title = {Population {{Based Training}} of {{Neural Networks}}},
  author = {Jaderberg, Max and Dalibard, Valentin and Osindero, Simon and Czarnecki, Wojciech M. and Donahue, Jeff and Razavi, Ali and Vinyals, Oriol and Green, Tim and Dunning, Iain and Simonyan, Karen and Fernando, Chrisantha and Kavukcuoglu, Koray},
  year = {2017},
  month = nov,
  journal = {arXiv:1711.09846 [cs]},
  eprint = {1711.09846},
  primaryclass = {cs},
  urldate = {2021-06-03},
  abstract = {Neural networks dominate the modern machine learning landscape, but their training and success still suffer from sensitivity to empirical choices of hyperparameters such as model architecture, loss function, and optimisation algorithm. In this work we present {\textbackslash}emph\{Population Based Training (PBT)\}, a simple asynchronous optimisation algorithm which effectively utilises a fixed computational budget to jointly optimise a population of models and their hyperparameters to maximise performance. Importantly, PBT discovers a schedule of hyperparameter settings rather than following the generally sub-optimal strategy of trying to find a single fixed set to use for the whole course of training. With just a small modification to a typical distributed hyperparameter training framework, our method allows robust and reliable training of models. We demonstrate the effectiveness of PBT on deep reinforcement learning problems, showing faster wall-clock convergence and higher final performance of agents by optimising over a suite of hyperparameters. In addition, we show the same method can be applied to supervised learning for machine translation, where PBT is used to maximise the BLEU score directly, and also to training of Generative Adversarial Networks to maximise the Inception score of generated images. In all cases PBT results in the automatic discovery of hyperparameter schedules and model selection which results in stable training and better final performance.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/luisaam/Zotero/storage/49RQSCVE/Jaderberg et al. - 2017 - Population Based Training of Neural Networks.pdf;/home/luisaam/Zotero/storage/WUWEF858/1711.html}
}

@inproceedings{jaderbergSpeedingConvolutionalNeural2014,
  title = {Speeding up {{Convolutional Neural Networks}} with {{Low Rank Expansions}}},
  booktitle = {Proceedings of the {{British Machine Vision Conference}} 2014},
  author = {Jaderberg, Max and Vedaldi, Andrea and Zisserman, Andrew},
  year = {2014},
  pages = {88.1-88.13},
  publisher = {{British Machine Vision Association}},
  address = {{Nottingham}},
  doi = {10.5244/C.28.88},
  urldate = {2022-12-18},
  isbn = {978-1-901725-52-0},
  langid = {english},
  keywords = {examples,network compression},
  file = {/home/luisaam/Zotero/storage/4QLHLJJB/Jaderberg et al. - 2014 - Speeding up Convolutional Neural Networks with Low.pdf}
}

@article{jaeglePerceiverGeneralPerception2021,
  title = {Perceiver: {{General Perception}} with {{Iterative Attention}}},
  shorttitle = {Perceiver},
  author = {Jaegle, Andrew and Gimeno, Felix and Brock, Andrew and Zisserman, Andrew and Vinyals, Oriol and Carreira, Joao},
  year = {2021},
  month = jun,
  journal = {arXiv:2103.03206 [cs, eess]},
  eprint = {2103.03206},
  primaryclass = {cs, eess},
  urldate = {2021-08-13},
  abstract = {Biological systems perceive the world by simultaneously processing high-dimensional inputs from modalities as diverse as vision, audition, touch, proprioception, etc. The perception models used in deep learning on the other hand are designed for individual modalities, often relying on domain-specific assumptions such as the local grid structures exploited by virtually all existing vision models. These priors introduce helpful inductive biases, but also lock models to individual modalities. In this paper we introduce the Perceiver - a model that builds upon Transformers and hence makes few architectural assumptions about the relationship between its inputs, but that also scales to hundreds of thousands of inputs, like ConvNets. The model leverages an asymmetric attention mechanism to iteratively distill inputs into a tight latent bottleneck, allowing it to scale to handle very large inputs. We show that this architecture is competitive with or outperforms strong, specialized models on classification tasks across various modalities: images, point clouds, audio, video, and video+audio. The Perceiver obtains performance comparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly attending to 50,000 pixels. It is also competitive in all modalities in AudioSet.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Interactions},
  file = {/home/luisaam/Zotero/storage/SN38G7S6/Jaegle et al. - 2021 - Perceiver General Perception with Iterative Atten.pdf;/home/luisaam/Zotero/storage/BVB96E2G/2103.html}
}

@misc{jastrzebskiRelationSharpestDirections2019,
  title = {On the {{Relation Between}} the {{Sharpest Directions}} of {{DNN Loss}} and the {{SGD Step Length}}},
  author = {Jastrz{\k{e}}bski, Stanis{\l}aw and Kenton, Zachary and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
  year = {2019},
  month = dec,
  number = {arXiv:1807.05031},
  eprint = {1807.05031},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1807.05031},
  urldate = {2023-12-03},
  abstract = {Stochastic Gradient Descent (SGD) based training of neural networks with a large learning rate or a small batch-size typically ends in well-generalizing, flat regions of the weight space, as indicated by small eigenvalues of the Hessian of the training loss. However, the curvature along the SGD trajectory is poorly understood. An empirical investigation shows that initially SGD visits increasingly sharp regions, reaching a maximum sharpness determined by both the learning rate and the batch-size of SGD. When studying the SGD dynamics in relation to the sharpest directions in this initial phase, we find that the SGD step is large compared to the curvature and commonly fails to minimize the loss along the sharpest directions. Furthermore, using a reduced learning rate along these directions can improve training speed while leading to both sharper and better generalizing solutions compared to vanilla SGD. In summary, our analysis of the dynamics of SGD in the subspace of the sharpest directions shows that they influence the regions that SGD steers to (where larger learning rate or smaller batch size result in wider regions visited), the overall training speed, and the generalization ability of the final model.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Second Order,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/QEP7N8RU/JastrzÄbski et al. - 2019 - On the Relation Between the Sharpest Directions of.pdf;/home/luisaam/Zotero/storage/CGS48XRE/1807.html}
}

@article{JaxPrunerConciseLibrary,
  title = {{{JaxPruner}}: {{A}} Concise Library for Sparsity Research. ({{arXiv}}:2304.14082v1 [Cs.{{LG}}])},
  journal = {arXiv Computer Science},
  doi = {arXiv:2304.14082v1},
  abstract = {This paper introduces JaxPruner, an open-source JAX-based pruning and sparse training library for machine learning research. JaxPruner aims to accelerate research on sparse neural networks by providing concise implementations of popular pruning and sparse training algorithms with minimal memory and latency overhead. Algorithms implemented in JaxPruner use a common API and work seamlessly with the popular optimization library Optax, which, in turn, enables easy integration with existing JAX based libraries. We demonstrate this ease of integration by providing examples in four different codebases: Scenic, t5x, Dopamine and FedJAX and provide baseline experiments on popular benchmarks.},
  keywords = {Researcher App}
}

@article{jiangFantasticGeneralizationMeasures2019,
  title = {Fantastic {{Generalization Measures}} and {{Where}} to {{Find Them}}},
  author = {Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
  year = {2019},
  month = dec,
  journal = {arXiv:1912.02178 [cs, stat]},
  eprint = {1912.02178},
  primaryclass = {cs, stat},
  urldate = {2021-07-29},
  abstract = {Generalization of deep networks has been of great interest in recent years, resulting in a number of theoretically and empirically motivated complexity measures. However, most papers proposing such measures study only a small set of models, leaving open the question of whether the conclusion drawn from those experiments would remain valid in other settings. We present the first large scale study of generalization in deep networks. We investigate more then 40 complexity measures taken from both theoretical bounds and empirical studies. We train over 10,000 convolutional networks by systematically varying commonly used hyperparameters. Hoping to uncover potentially causal relationships between each measure and generalization, we analyze carefully controlled experiments and show surprising failures of some measures as well as promising measures for further research.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Diagnostic Measures,Generalization Measures,small networks,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/9CE2T4C2/Jiang et al. - 2019 - Fantastic Generalization Measures and Where to Fin.pdf;/home/luisaam/Zotero/storage/VSQ6QXEE/1912.html}
}

@article{jiangFantasticGeneralizationMeasures2019a,
  title = {Fantastic Generalization Measures and Where to Find Them},
  author = {Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
  year = {2019},
  journal = {arXiv preprint arXiv:1912.02178},
  eprint = {1912.02178},
  archiveprefix = {arxiv}
}

@article{jinHowEscapeSaddle2017,
  title = {How to {{Escape Saddle Points Efficiently}}},
  author = {Jin, Chi and Ge, Rong and Netrapalli, Praneeth and Kakade, Sham M. and Jordan, Michael I.},
  year = {2017},
  month = mar,
  journal = {arXiv:1703.00887 [cs, math, stat]},
  eprint = {1703.00887},
  primaryclass = {cs, math, stat},
  urldate = {2021-08-06},
  abstract = {This paper shows that a perturbed form of gradient descent converges to a second-order stationary point in a number iterations which depends only poly-logarithmically on dimension (i.e., it is almost "dimension-free"). The convergence rate of this procedure matches the well-known convergence rate of gradient descent to first-order stationary points, up to log factors. When all saddle points are non-degenerate, all second-order stationary points are local minima, and our result thus shows that perturbed gradient descent can escape saddle points almost for free. Our results can be directly applied to many machine learning applications, including deep learning. As a particular concrete example of such an application, we show that our results can be used directly to establish sharp global convergence rates for matrix factorization. Our results rely on a novel characterization of the geometry around saddle points, which may be of independent interest to the non-convex optimization community.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Saddle Points,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/2SRQTFIX/Jin et al. - 2017 - How to Escape Saddle Points Efficiently.pdf;/home/luisaam/Zotero/storage/FE2HQB9M/1703.html}
}

@misc{jozefowiczExploringLimitsLanguage2016,
  title = {Exploring the {{Limits}} of {{Language Modeling}}},
  author = {Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
  year = {2016},
  month = feb,
  number = {arXiv:1602.02410},
  eprint = {1602.02410},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1602.02410},
  urldate = {2022-11-07},
  abstract = {In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,example},
  file = {/home/luisaam/Zotero/storage/SMVMMVLU/Jozefowicz et al. - 2016 - Exploring the Limits of Language Modeling.pdf;/home/luisaam/Zotero/storage/I66E3WFZ/1602.html}
}

@article{karimi-mamaghanMachineLearningService2022,
  title = {Machine Learning at the Service of Meta-Heuristics for Solving Combinatorial Optimization Problems: {{A}} State-of-the-Art},
  shorttitle = {Machine Learning at the Service of Meta-Heuristics for Solving Combinatorial Optimization Problems},
  author = {{Karimi-Mamaghan}, Maryam and Mohammadi, Mehrdad and Meyer, Patrick and {Karimi-Mamaghan}, Amir Mohammad and Talbi, El-Ghazali},
  year = {2022},
  month = jan,
  journal = {European Journal of Operational Research},
  volume = {296},
  number = {2},
  pages = {393--422},
  issn = {0377-2217},
  doi = {10.1016/j.ejor.2021.04.032},
  urldate = {2022-07-06},
  abstract = {In recent years, there has been a growing research interest in integrating machine learning techniques into meta-heuristics for solving combinatorial optimization problems. This integration aims to lead meta-heuristics toward an efficient, effective, and robust search and improve their performance in terms of solution quality, convergence rate, and robustness. Since various integration methods with different purposes have been developed, there is a need to review the recent advances in using machine learning techniques to improve meta-heuristics. To the best of our knowledge, the literature is deprived of having a comprehensive yet technical review. To fill this gap, this paper provides such a review on the use of machine learning techniques in the design of different elements of meta-heuristics for different purposes including algorithm selection, fitness evaluation, initialization, evolution, parameter setting, and cooperation. First, we describe the key concepts and preliminaries of each of these ways of integration. Then, the recent advances in each way of integration are reviewed and classified based on a proposed unified taxonomy. Finally, we provide a technical discussion on the advantages, limitations, requirements, and challenges of implementing each of these integration ways, followed by promising future research directions.},
  langid = {english},
  keywords = {Combinatorial optimization problems,Machine learning,Meta-heuristics,State-of-the-art},
  file = {/home/luisaam/Zotero/storage/2PBBZAWE/Karimi-Mamaghan et al. - 2022 - Machine learning at the service of meta-heuristics.pdf}
}

@article{karninSimpleProcedurePruning1990,
  title = {A Simple Procedure for Pruning Back-Propagation Trained Neural Networks},
  author = {Karnin, E.D.},
  year = {1990},
  month = jun,
  journal = {IEEE Transactions on Neural Networks},
  volume = {1},
  number = {2},
  pages = {239--242},
  issn = {1941-0093},
  doi = {10.1109/72.80236},
  abstract = {The sensitivity of the global error (cost) function to the inclusion/exclusion of each synapse in the artificial neural network is estimated. Introduced are shadow arrays which keep track of the incremental changes to the synaptic weights during a single pass of back-propagating learning. The synapses are then ordered by decreasing sensitivity numbers so that the network can be efficiently pruned by discarding the last items of the sorted list. Unlike previous approaches, this simple procedure does not require a modification of the cost function, does not interfere with the learning process, and demands a negligible computational overhead.{$<>$}},
  keywords = {Artificial neural networks,Cities and towns,Computational efficiency,Computer networks,Cost function,Learning systems,Logistics,Neural networks,Neurons,Pruning,Training data},
  file = {/home/luisaam/Zotero/storage/NHQL3DK5/Karnin - 1990 - A simple procedure for pruning back-propagation tr.pdf;/home/luisaam/Zotero/storage/7FKEIQ25/80236.html}
}

@inproceedings{kawaguchiDeepLearningPoor2016,
  title = {Deep {{Learning}} without {{Poor Local Minima}}},
  booktitle = {{{NIPS}}},
  author = {Kawaguchi, Kenji},
  year = {2016},
  abstract = {In this paper, we prove a conjecture published in 1989 and also partially address an open problem announced at the Conference on Learning Theory (COLT) 2015. With no unrealistic assumption, we first prove the following statements for the squared loss function of deep linear neural networks with any depth and any widths: 1) the function is non-convex and non-concave, 2) every local minimum is a global minimum, 3) every critical point that is not a global minimum is a saddle point, and 4) there exist "bad" saddle points (where the Hessian has no negative eigenvalue) for the deeper networks (with more than three layers), whereas there is no bad saddle point for the shallow networks (with three layers). Moreover, for deep nonlinear neural networks, we prove the same four statements via a reduction to a deep linear model under the independence assumption adopted from recent work. As a result, we present an instance, for which we can answer the following question: how difficult is it to directly train a deep model in theory? It is more difficult than the classical machine learning models (because of the non-convexity), but not too difficult (because of the nonexistence of poor local minima). Furthermore, the mathematically proven existence of bad saddle points for deeper models would suggest a possible open problem. We note that even though we have advanced the theoretical foundations of deep learning and non-convex optimization, there is still a gap between theory and practice.},
  keywords = {Why no global second order},
  file = {/home/luisaam/Zotero/storage/S4RQIAK2/Kawaguchi - 2016 - Deep Learning without Poor Local Minima.pdf}
}

@article{KeepGradientsFlowing,
  title = {Keep the {{Gradients Flowing}}: {{Using Gradient Flow}} to {{Study Sparse Network Optimization}}. ({{arXiv}}:2102.01670v2 [Cs.{{LG}}] {{UPDATED}})},
  journal = {arXiv Computer Vision and Pattern Recognition},
  doi = {arXiv:2102.01670v2},
  abstract = {Training sparse networks to converge to the same performance as dense neural architectures has proven to be elusive. Recent work suggests that initialization is the key. However, while this direction of research has had some success, focusing on initialization alone appears to be inadequate. In this paper, we take a broader view of training sparse networks and consider the role of regularization, optimization, and architecture choices on sparse models. We propose a simple experimental framework, Same Capacity Sparse vs Dense Comparison (SC-SDC), that allows for a fair comparison of sparse and dense networks. Furthermore, we propose a new measure of gradient flow, Effective Gradient Flow (EGF), that better correlates to performance in sparse networks. Using top-line metrics, SC-SDC and EGF, we show that default choices of optimizers, activation functions and regularizers used for dense networks can disadvantage sparse networks. Based upon these findings, we show that gradient flow in sparse networks can be improved by reconsidering aspects of the architecture design and the training regime. Our work suggests that initialization is only one piece of the puzzle and taking a wider view of tailoring optimization to sparse networks yields promising results.},
  keywords = {Pruning,Researcher App},
  file = {/home/luisaam/Zotero/storage/F4EX29L9/Keep the Gradients Flowing Using Gradient Flow to.pdf}
}

@article{keskarLargeBatchTrainingDeep2017,
  title = {On {{Large-Batch Training}} for {{Deep Learning}}: {{Generalization Gap}} and {{Sharp Minima}}},
  shorttitle = {On {{Large-Batch Training}} for {{Deep Learning}}},
  author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  year = {2017},
  month = feb,
  journal = {arXiv:1609.04836 [cs, math]},
  eprint = {1609.04836},
  primaryclass = {cs, math},
  urldate = {2021-07-29},
  abstract = {The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say \$32\$-\$512\$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,generalization,Mathematics - Optimization and Control,small networks},
  file = {/home/luisaam/Zotero/storage/J3T3FWF2/Keskar et al. - 2017 - On Large-Batch Training for Deep Learning General.pdf;/home/luisaam/Zotero/storage/48WNQKDB/1609.html}
}

@article{keupOrigamiDimensionsHow2022,
  title = {Origami in {{N}} Dimensions: {{How}} Feed-Forward Networks Manufacture Linear Separability},
  shorttitle = {Origami in {{N}} Dimensions},
  author = {Keup, Christian and Helias, Moritz},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.11355 [cond-mat, stat]},
  eprint = {2203.11355},
  primaryclass = {cond-mat, stat},
  urldate = {2022-04-05},
  abstract = {Neural networks can implement arbitrary functions. But, mechanistically, what are the tools at their disposal to construct the target? For classification tasks, the network must transform the data classes into a linearly separable representation in the final hidden layer. We show that a feed-forward architecture has one primary tool at hand to achieve this separability: progressive folding of the data manifold in unoccupied higher dimensions. The operation of folding provides a useful intuition in low-dimensions that generalizes to high ones. We argue that an alternative method based on shear, requiring very deep architectures, plays only a small role in real-world networks. The folding operation, however, is powerful as long as layers are wider than the data dimensionality, allowing efficient solutions by providing access to arbitrary regions in the distribution, such as data points of one class forming islands within the other classes. We argue that a link exists between the universal approximation property in ReLU networks and the fold-and-cut theorem (Demaine et al., 1998) dealing with physical paper folding. Based on the mechanistic insight, we predict that the progressive generation of separability is necessarily accompanied by neurons showing mixed selectivity and bimodal tuning curves. This is validated in a network trained on the poker hand task, showing the emergence of bimodal tuning curves during training. We hope that our intuitive picture of the data transformation in deep networks can help to provide interpretability, and discuss possible applications to the theory of convolutional networks, loss landscapes, and generalization. TL;DR: Shows that the internal processing of deep networks can be thought of as literal folding operations on the data distribution in the N-dimensional activation space. A link to a well-known theorem in origami theory is provided.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Interactions,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/2U7SRXII/Keup y Helias - 2022 - Origami in N dimensions How feed-forward networks.pdf;/home/luisaam/Zotero/storage/ZN3GN68A/2203.html}
}

@inproceedings{khalilLearningCombinatorialOptimization2017,
  title = {Learning {{Combinatorial Optimization Algorithms}} over {{Graphs}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Khalil, Elias and Dai, Hanjun and Zhang, Yuyu and Dilkina, Bistra and Song, Le},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-07-12},
  abstract = {The design of good heuristics or approximation algorithms for NP-hard combinatorial optimization problems often requires significant specialized knowledge and trial-and-error. Can we automate this challenging, tedious process, and learn the algorithms instead? In many real-world applications, it is typically the case that the same optimization problem is solved again and again on a regular basis, maintaining the same problem structure but differing in the data. This provides an opportunity for learning heuristic algorithms that exploit the structure of such recurring problems.  In this paper, we propose a unique combination of reinforcement learning and graph embedding to address this challenge. The learned greedy policy behaves like a meta-algorithm that incrementally constructs a solution, and the action is determined by the output of a graph embedding network capturing the current state of the solution. We show that our framework can be applied to a diverse range of optimization problems over graphs, and learns effective algorithms for the Minimum Vertex Cover, Maximum Cut and Traveling Salesman problems.},
  file = {/home/luisaam/Zotero/storage/CK74XWAA/Khalil et al. - 2017 - Learning Combinatorial Optimization Algorithms ove.pdf}
}

@article{kimDeadPixelTest2023,
  title = {Dead Pixel Test Using Effective Receptive Field},
  author = {Kim, Bum Jun and Choi, Hyeyeon and Jang, Hyeonah and Lee, Dong Gu and Jeong, Wonseok and Kim, Sang Woo},
  year = {2023},
  month = mar,
  journal = {Pattern Recognition Letters},
  volume = {167},
  pages = {149--156},
  issn = {0167-8655},
  doi = {10.1016/j.patrec.2023.02.018},
  urldate = {2023-12-04},
  abstract = {Deep neural networks have been used in various fields, but their internal behavior in how they understand images is not well known. In this study, we discuss two counterintuitive properties of convolutional neural networks (CNNs). First, we evaluated the size of the receptive field of CNNs with their classification accuracy. Previous studies have attempted to increase the size of the receptive field for performance gain. However, we observed that some CNNs with a smaller receptive field can achieve higher classification accuracy. In this regard, we claim that a larger receptive field does not guarantee improved classification accuracy. Second, using the effective receptive field, we examined the contribution of each pixel to the output of CNN. Intuitively, each pixel is expected to equally contribute to the final output, but we found that there exist pixels in a partially dead state with little contribution to the output. We reveal that the reason for dead pixels lies in even stride operations with odd-sized kernels in CNN and propose a kernel padding method to remove the dead pixels. We demonstrated the vulnerability of CNNs with dead pixels when we detect a noise or small box that is on dead pixels. Our findings on dead pixels should be understood and considered in practical applications of CNN.},
  keywords = {Computer vision,Convolutional kernel,Convolutional neural networks,Deep learning,Model interpretation,Receptive field,Receptive Field},
  file = {/home/luisaam/Zotero/storage/UG4F522C/Kim et al. - 2023 - Dead pixel test using effective receptive field.pdf;/home/luisaam/Zotero/storage/D4Z92LXG/S0167865523000478.html}
}

@techreport{kingmaADAMMETHODSTOCHASTIC,
  title = {{{ADAM}}: {{A METHOD FOR STOCHASTIC OPTIMIZATION}}},
  author = {Kingma, Diederik P and Lei Ba, Jimmy},
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  file = {/home/luisaam/Zotero/storage/WKUZDY6D/Kingma, Lei Ba - Unknown - ADAM A METHOD FOR STOCHASTIC OPTIMIZATION.pdf}
}

@article{kingmaAdamMethodStochastic2014,
  title = {Adam: {{A}} Method for Stochastic Optimization},
  author = {Kingma, Diederik P and Ba, Jimmy},
  year = {2014},
  journal = {arXiv preprint arXiv:1412.6980},
  eprint = {1412.6980},
  archiveprefix = {arxiv}
}

@inproceedings{kisselSobolevTrainingApproximated2020,
  title = {Sobolev {{Training}} with {{Approximated Derivatives}} for {{Black-Box Function Regression}} with {{Neural Networks}}},
  booktitle = {Machine {{Learning}} and {{Knowledge Discovery}} in {{Databases}}},
  author = {Kissel, Matthias and Diepold, Klaus},
  editor = {Brefeld, Ulf and Fromont, Elisa and Hotho, Andreas and Knobbe, Arno and Maathuis, Marloes and Robardet, C{\'e}line},
  year = {2020},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {399--414},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-46147-8_24},
  abstract = {With Sobolev Training, neural networks are trained to fit target output values as well as target derivatives with respect to the inputs. This leads to better generalization and fewer required training examples for certain problems. In this paper, we present a training pipeline that enables Sobolev Training for regression problems where target derivatives are not directly available. Thus, we propose to use a least-squares estimate of the target derivatives based on function values of neighboring training samples. We show for a variety of black-box function regression tasks that our training pipeline achieves smaller test errors compared to the traditional training method. Since our method has no additional requirements on the data collection process, it has great potential to improve the results for various regression tasks.},
  isbn = {978-3-030-46147-8},
  langid = {english},
  keywords = {gradients training,Machine Learning,Neural networks,Sobolev training},
  file = {/home/luisaam/Zotero/storage/2BPJYZBX/Kissel y Diepold - 2020 - Sobolev Training with Approximated Derivatives for.pdf}
}

@inproceedings{knyazevParameterPredictionUnseen2021,
  title = {Parameter {{Prediction}} for {{Unseen Deep Architectures}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Knyazev, Boris and Drozdzal, Michal and Taylor, Graham W and Romero Soriano, Adriana},
  year = {2021},
  volume = {34},
  pages = {29433--29448},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-10-17},
  abstract = {Deep learning has been successful in automating the design of features in machine learning pipelines. However, the algorithms optimizing neural network parameters remain largely hand-designed and computationally inefficient. We study if we can use deep learning to directly predict these parameters by exploiting the past knowledge of training other networks. We introduce a large-scale dataset of diverse computational graphs of neural architectures - DeepNets-1M - and use it to explore parameter prediction on CIFAR-10 and ImageNet. By leveraging advances in graph neural networks, we propose a hypernetwork that can predict performant parameters in a single forward pass taking a fraction of a second, even on a CPU. The proposed model achieves surprisingly good performance on unseen and diverse networks. For example, it is able to predict all 24 million parameters of a ResNet-50 achieving a 60\% accuracy on CIFAR-10. On ImageNet, top-5 accuracy of some of our networks approaches 50\%. Our task along with the model and results can potentially lead to a new, more computationally efficient paradigm of training networks. Our model also learns a strong representation of neural architectures enabling their analysis.},
  keywords = {Graph neural network,hypernetworks,Pruning,weight prediction},
  file = {/home/luisaam/Zotero/storage/KMPIHXXI/Knyazev et al. - 2021 - Parameter Prediction for Unseen Deep Architectures.pdf}
}

@misc{kobayashiInterpretationResNetVisualization2020,
  title = {Interpretation of {{ResNet}} by {{Visualization}} of {{Preferred Stimulus}} in {{Receptive Fields}}},
  author = {Kobayashi, Genta and Shouno, Hayaru},
  year = {2020},
  month = jul,
  number = {arXiv:2006.01645},
  eprint = {2006.01645},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.01645},
  urldate = {2023-11-26},
  abstract = {One of the methods used in image recognition is the Deep Convolutional Neural Network (DCNN). DCNN is a model in which the expressive power of features is greatly improved by deepening the hidden layer of CNN. The architecture of CNNs is determined based on a model of the visual cortex of mammals. There is a model called Residual Network (ResNet) that has a skip connection. ResNet is an advanced model in terms of the learning method, but it has not been interpreted from a biological viewpoint. In this research, we investigate the receptive fields of a ResNet on the classification task in ImageNet. We find that ResNet has orientation selective neurons and double opponent color neurons. In addition, we suggest that some inactive neurons in the first layer of ResNet affect the classification task.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/luisaam/Zotero/storage/Y52UCYVN/Kobayashi and Shouno - 2020 - Interpretation of ResNet by Visualization of Prefe.pdf;/home/luisaam/Zotero/storage/G6K2755U/2006.html}
}

@misc{kobayashiInterpretationResNetVisualization2020a,
  title = {Interpretation of {{ResNet}} by {{Visualization}} of {{Preferred Stimulus}} in {{Receptive Fields}}},
  author = {Kobayashi, Genta and Shouno, Hayaru},
  year = {2020},
  month = jul,
  number = {arXiv:2006.01645},
  eprint = {2006.01645},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2006.01645},
  urldate = {2023-12-03},
  abstract = {One of the methods used in image recognition is the Deep Convolutional Neural Network (DCNN). DCNN is a model in which the expressive power of features is greatly improved by deepening the hidden layer of CNN. The architecture of CNNs is determined based on a model of the visual cortex of mammals. There is a model called Residual Network (ResNet) that has a skip connection. ResNet is an advanced model in terms of the learning method, but it has not been interpreted from a biological viewpoint. In this research, we investigate the receptive fields of a ResNet on the classification task in ImageNet. We find that ResNet has orientation selective neurons and double opponent color neurons. In addition, we suggest that some inactive neurons in the first layer of ResNet affect the classification task.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Receptive Field},
  file = {/home/luisaam/Zotero/storage/8ZLQWJEM/Kobayashi and Shouno - 2020 - Interpretation of ResNet by Visualization of Prefe.pdf;/home/luisaam/Zotero/storage/CQMF88LU/2006.html}
}

@article{kondaActorCriticAlgorithms1999,
  title = {Actor-{{Critic Algorithms}}},
  author = {Konda, Vijay and Tsitsiklis, John},
  year = {1999},
  journal = {Advances in Neural Information Processing Systems},
  volume = {12},
  urldate = {2021-07-10},
  langid = {english},
  file = {/home/luisaam/Zotero/storage/7SB9IKAZ/Konda y Tsitsiklis - 1999 - Actor-Critic Algorithms.pdf;/home/luisaam/Zotero/storage/SJN8VMT4/6449f44a102fde848669bdd9eb6b76fa-Abstract.html}
}

@misc{kornblithSimilarityNeuralNetwork2019,
  title = {Similarity of {{Neural Network Representations Revisited}}},
  author = {Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  year = {2019},
  month = jul,
  number = {arXiv:1905.00414},
  eprint = {1905.00414},
  primaryclass = {cs, q-bio, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1905.00414},
  urldate = {2023-12-03},
  abstract = {Recent work has sought to understand the behavior of neural networks by comparing representations between layers and between different trained models. We examine methods for comparing neural network representations based on canonical correlation analysis (CCA). We show that CCA belongs to a family of statistics for measuring multivariate similarity, but that neither CCA nor any other statistic that is invariant to invertible linear transformation can measure meaningful similarities between representations of higher dimension than the number of data points. We introduce a similarity index that measures the relationship between representational similarity matrices and does not suffer from this limitation. This similarity index is equivalent to centered kernel alignment (CKA) and is also closely connected to CCA. Unlike CCA, CKA can reliably identify correspondences between representations in networks trained from different initializations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition,Receptive Field,Similarity,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/QFEXM9WV/Kornblith et al. - 2019 - Similarity of Neural Network Representations Revis.pdf;/home/luisaam/Zotero/storage/BA2KM6C4/1905.html}
}

@article{kurticOptimalBERTSurgeon2022,
  title = {The {{Optimal BERT Surgeon}}: {{Scalable}} and {{Accurate Second-Order Pruning}} for {{Large Language Models}}},
  shorttitle = {The {{Optimal BERT Surgeon}}},
  author = {Kurtic, Eldar and Campos, Daniel and Nguyen, Tuan and Frantar, Elias and Kurtz, Mark and Fineran, Benjamin and Goin, Michael and Alistarh, Dan},
  year = {2022},
  month = mar,
  journal = {arXiv:2203.07259 [cs]},
  eprint = {2203.07259},
  primaryclass = {cs},
  urldate = {2022-04-12},
  abstract = {Pre-trained Transformer-based language models have become a key building block for natural language processing (NLP) tasks. While these models are extremely accurate, they can be too large and computationally intensive to run on standard deployments. A variety of compression methods, including distillation, quantization, structured and unstructured pruning are known to be applicable to decrease model size and increase inference speed. In this context, this paper's contributions are two-fold. We begin with an in-depth study of the accuracy-compression trade-off for unstructured weight pruning in the context of BERT models, and introduce Optimal BERT Surgeon (O-BERT-S), an efficient and accurate weight pruning method based on approximate second-order information, which we show to yield state-of-the-art results in terms of the compression/accuracy trade-off. Specifically, Optimal BERT Surgeon extends existing work on second-order pruning by allowing for pruning blocks of weights, and by being applicable at BERT scale. Second, we investigate the impact of this pruning method when compounding compression approaches for Transformer-based models, which allows us to combine state-of-the-art structured and unstructured pruning together with quantization, in order to obtain highly compressed, but accurate models. The resulting compression framework is powerful, yet general and efficient: we apply it to both the fine-tuning and pre-training stages of language tasks, to obtain state-of-the-art results on the accuracy-compression trade-off with relatively simple compression recipes. For example, we obtain 10x model size compression with {$<$} 1\% relative drop in accuracy to the dense BERT-base, 10x end-to-end CPU-inference speedup with {$<$} 2\% relative drop in accuracy, and 29x inference speedups with {$<$} 7.5\% relative accuracy drop.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Hessian,Neural Magic,Pruning},
  file = {/home/luisaam/Zotero/storage/KU8KYHUP/Kurtic et al. - 2022 - The Optimal BERT Surgeon Scalable and Accurate Se.pdf;/home/luisaam/Zotero/storage/I7CD5RA8/2203.html}
}

@misc{kurticSparseFinetuningInference2023,
  title = {Sparse {{Fine-tuning}} for {{Inference Acceleration}} of {{Large Language Models}}},
  author = {Kurtic, Eldar and Kuznedelev, Denis and Frantar, Elias and Goin, Michael and Alistarh, Dan},
  year = {2023},
  month = oct,
  number = {arXiv:2310.06927},
  eprint = {2310.06927},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2310.06927},
  urldate = {2023-10-23},
  abstract = {We consider the problem of accurate sparse fine-tuning of large language models (LLMs), that is, fine-tuning pretrained LLMs on specialized tasks, while inducing sparsity in their weights. On the accuracy side, we observe that standard loss-based fine-tuning may fail to recover accuracy, especially at high sparsities. To address this, we perform a detailed study of distillation-type losses, determining an L2-based distillation approach we term SquareHead which enables accurate recovery even at higher sparsities, across all model types. On the practical efficiency side, we show that sparse LLMs can be executed with speedups by taking advantage of sparsity, for both CPU and GPU runtimes. While the standard approach is to leverage sparsity for computational reduction, we observe that in the case of memory-bound LLMs sparsity can also be leveraged for reducing memory bandwidth. We exhibit end-to-end results showing speedups due to sparsity, while recovering accuracy, on T5 (language translation), Whisper (speech translation), and open GPT-type (MPT for text generation). For MPT text generation, we show for the first time that sparse fine-tuning can reach 75\% sparsity without accuracy drops, provide notable end-to-end speedups for both CPU and GPU inference, and highlight that sparsity is also compatible with quantization approaches. Models and software for reproducing our results are provided in Section 6.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Dynamical Sparse Training,Pruning},
  file = {/home/luisaam/Zotero/storage/EGNHNNZ5/Kurtic et al. - 2023 - Sparse Fine-tuning for Inference Acceleration of L.pdf;/home/luisaam/Zotero/storage/MS8CRQG9/2310.html}
}

@inproceedings{kurtzInducingExploitingActivation2020,
  title = {Inducing and Exploiting Activation Sparsity for Fast Inference on Deep Neural Networks},
  booktitle = {Proceedings of the 37th International Conference on Machine Learning},
  author = {Kurtz, Mark and Kopinsky, Justin and Gelashvili, Rati and Matveev, Alexander and Carr, John and Goin, Michael and Leiserson, William and Moore, Sage and Nell, Bill and Shavit, Nir and Alistarh, Dan},
  editor = {III, Hal Daum{\'e} and Singh, Aarti},
  year = {2020},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {119},
  pages = {5533--5543},
  publisher = {{PMLR}},
  address = {{Virtual}},
  abstract = {Optimizing convolutional neural networks for fast inference has recently become an extremely active area of research. One of the go-to solutions in this context is weight pruning, which aims to reduce computational and memory footprint by removing large subsets of the connections in a neural network. Surprisingly, much less attention has been given to exploiting sparsity in the activation maps, which tend to be naturally sparse in many settings thanks to the structure of rectified linear (ReLU) activation functions. In this paper, we present an in-depth analysis of methods for maximizing the sparsity of the activations in a trained neural network, and show that, when coupled with an efficient sparse-input convolution algorithm, we can leverage this sparsity for significant performance gains. To induce highly sparse activation maps without accuracy loss, we introduce a new regularization technique, coupled with a new threshold-based sparsification method based on a parameterized activation function called Forced-Activation-Threshold Rectified Linear Unit (FATReLU). We examine the impact of our methods on popular image classification models, showing that most architectures can adapt to significantly sparser activation maps without any accuracy loss. Our second contribution is showing that these these compression gains can be translated into inference speedups: we provide a new algorithm to enable fast convolution operations over networks with sparse activations, and show that it can enable significant speedups for end-to-end inference on a range of popular models on the large-scale ImageNet image classification task on modern Intel CPUs, with little or no retraining cost.},
  pdf = {http://proceedings.mlr.press/v119/kurtz20a/kurtz20a.pdf},
  keywords = {Neural Magic,Pruning,sparse NN}
}

@article{kurtzInducingExploitingActivation2020a,
  title = {Inducing and {{Exploiting Activation Sparsity}} for {{Fast Neural Network Inference}}},
  author = {Kurtz, Mark and Kopinsky, Justin and Gelashvili, Rati and Matveev, Alexander and Carr, John and Goin, Michael and Leiserson, William and Moore, Sage and Nell, Bill and Shavit, Nir and Alistarh, Dan},
  year = {2020},
  pages = {119},
  abstract = {Optimizing deep neural networks for inference has recently become an extremely active area of research. One of the go-to solutions in this context is weight pruning, which aims to reduce computational and memory footprint by removing large subsets of the connections in a neural network. Surprisingly, much less attention has been given to exploiting sparsity in the activation maps, which tend to be naturally sparse in many settings thanks to the structure of rectified linear (ReLU) activation functions. In this paper, we present an analysis of methods for maximizing the sparsity of the activations in a trained neu-ral network, and show that, when coupled with an efficient sparse-input convolution algorithm, we can leverage this sparsity for significant performance gains. To induce highly sparse activation maps without accuracy loss, we introduce a new regularization technique, coupled with a new threshold-based sparsification method based on a parameterized activation function called Forced-Activation-Threshold Rectified Linear Unit (FA-TReLU). We examine the impact of our methods on popular image classification models, showing that most architectures can adapt to significantly sparser activation maps without any accuracy loss. Our second contribution is showing that these these compression gains can be translated into inference speedups: we provide a new algorithm to enable fast convolution operations over networks with sparse activations, and show that it can enable significant speedups for end-to-end inference on a range of popular models on the large-scale ImageNet image classification task on modern Intel CPUs, with relatively low retraining cost. * Equal contribution 1 Neural Magic 2 IST Austria. Correspondence to: Dan Alistarh {\textbackslash}textlessdan@neuralmagic.com{\textbackslash}textgreater.},
  file = {/home/luisaam/Zotero/storage/QSFLISIY/6820-Paper.pdf}
}

@article{LearningCosmologyClustering,
  title = {Learning Cosmology and Clustering with Cosmic Graphs. ({{arXiv}}:2204.13713v1 [Astro-Ph.{{CO}}])},
  journal = {arXiv Astrophysics of Galaxies},
  doi = {arXiv:2204.13713v1},
  abstract = {We train deep learning models on thousands of galaxy catalogues from the state-of-the-art hydrodynamic simulations of the CAMELS project to perform regression and inference. We employ Graph Neural Networks (GNNs), architectures designed to work with irregular and sparse data, like the distribution of galaxies in the Universe. We first show that GNNs can learn to compute the power spectrum of galaxy catalogues with a few percent accuracy. We then train GNNs to perform likelihood-free inference at the galaxy-field level. Our models are able to infer the value of  with a  accuracy just from the positions of  galaxies in a volume of  at  while accounting for astrophysical uncertainties as modelled in CAMELS. Incorporating information from galaxy properties, such as stellar mass, stellar metallicity, and stellar radius, increases the accuracy to . Our models are built to be translational and rotational invariant, and they can extract information from any scale larger than the minimum distance between two galaxies. However, our models are not completely robust: testing on simulations run with a different subgrid physics than the ones used for training does not yield as accurate results.},
  keywords = {Researcher App}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  journal = {nature},
  volume = {521},
  number = {7553},
  pages = {436--444},
  publisher = {{Nature Publishing Group}}
}

@inproceedings{lecunOptimalBrainDamage1989,
  title = {Optimal {{Brain Damage}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {LeCun, Yann and Denker, John and Solla, Sara},
  year = {1989},
  volume = {2},
  publisher = {{Morgan-Kaufmann}},
  urldate = {2022-03-24},
  keywords = {Hessian,Pruning},
  file = {/home/luisaam/Zotero/storage/4H79QPRU/LeCun et al. - 1989 - Optimal Brain Damage.pdf}
}

@inproceedings{leeGradientDescentOnly2016,
  title = {Gradient {{Descent Only Converges}} to {{Minimizers}}},
  booktitle = {Conference on {{Learning Theory}}},
  author = {Lee, Jason D. and Simchowitz, Max and Jordan, Michael I. and Recht, Benjamin},
  year = {2016},
  month = jun,
  pages = {1246--1257},
  publisher = {{PMLR}},
  issn = {1938-7228},
  urldate = {2021-06-25},
  abstract = {We show that gradient descent converges to a local minimizer, almost surely with random initial- ization. This is proved by applying the Stable Manifold Theorem from dynamical systems theory.},
  langid = {english},
  keywords = {Why no global second order},
  file = {/home/luisaam/Zotero/storage/PN35G4H2/Lee et al. - 2016 - Gradient Descent Only Converges to Minimizers.pdf;/home/luisaam/Zotero/storage/GAZ39E6X/lee16.html}
}

@inproceedings{leeLayeradaptiveSparsityMagnitudebased2022,
  title = {Layer-Adaptive {{Sparsity}} for the {{Magnitude-based Pruning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Lee, Jaeho and Park, Sejun and Mo, Sangwoo and Ahn, Sungsoo and Shin, Jinwoo},
  year = {2022},
  month = feb,
  urldate = {2022-11-24},
  abstract = {Recent discoveries on neural network pruning reveal that, with a carefully chosen layerwise sparsity, a simple magnitude-based pruning achieves state-of-the-art tradeoff between sparsity and performance. However, without a clear consensus on ``how to choose,'' the layerwise sparsities are mostly selected algorithm-by-algorithm, often resorting to handcrafted heuristics or an extensive hyperparameter search. To fill this gap, we propose a novel importance score for global pruning, coined layer-adaptive magnitude-based pruning (LAMP) score; the score is a rescaled version of weight magnitude that incorporates the model-level \${\textbackslash}ell\_2\$ distortion incurred by pruning, and does not require any hyperparameter tuning or heavy computation. Under various image classification setups, LAMP consistently outperforms popular existing schemes for layerwise sparsity selection. Furthermore, we observe that LAMP continues to outperform baselines even in weight-rewinding setups, while the connectivity-oriented layerwise sparsity (the strongest baseline overall) performs worse than a simple global magnitude-based pruning in this case. Code: https://github.com/jaeho-lee/layer-adaptive-sparsity},
  langid = {english},
  keywords = {layer-wise purning,Pruning},
  file = {/home/luisaam/Zotero/storage/XWBJB7RI/Lee et al. - 2022 - Layer-adaptive Sparsity for the Magnitude-based Pr.pdf;/home/luisaam/Zotero/storage/YVDZBE2B/forum.html}
}

@inproceedings{leeSNIPSINGLESHOTNETWORK2018,
  title = {{{SNIP}}: {{SINGLE-SHOT NETWORK PRUNING BASED ON CONNECTION SENSITIVITY}}},
  shorttitle = {{{SNIP}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Lee, Namhoon and Ajanthan, Thalaiyasingam and Torr, Philip},
  year = {2018},
  month = sep,
  urldate = {2021-08-22},
  abstract = {We present a new approach, SNIP, that is simple, versatile and interpretable; it prunes irrelevant connections for a given task at single-shot prior to training and is applicable to a variety of...},
  langid = {english},
  keywords = {Pruning},
  file = {/home/luisaam/Zotero/storage/I5FQ2YTI/Lee et al. - 2018 - SNIP SINGLE-SHOT NETWORK PRUNING BASED ON CONNECT.pdf;/home/luisaam/Zotero/storage/S9ZNY7GJ/forum.html}
}

@inproceedings{leOptimizationMethodsDeep2011,
  title = {On Optimization Methods for Deep Learning},
  booktitle = {Proceedings of the 28th {{International Conference}} on {{Machine Learning}}, {{ICML}} 2011},
  author = {Le, Quoc V. and Ngiam, Jiquan and Coates, Adam and Lahiri, Abhik and Prochnow, Bobby and Ng, Andrew Y.},
  year = {2011},
  abstract = {The predominant methodology in training deep learning advocates the use of stochastic gradient descent methods (SGDs). Despite its ease of implementation, SGDs are difficult to tune and parallelize. These problems make it challenging to develop, debug and scale up deep learning algorithms with SGDs. In this paper, we show that more sophisticated off-the-shelf optimization methods such as Limited memory BFGS (L-BFGS) and Conjugate gradient (CG) with line search can significantly simplify and speed up the process of pretraining deep algorithms. In our experiments, the difference between L-BFGS/CG and SGDs are more pronounced if we consider algorithmic extensions (e.g., sparsity regularization) and hardware extensions (e.g., GPUs or computer clusters). Our experiments with distributed optimization support the use of L-BFGS with locally connected networks and convolutional neural networks. Using L-BFGS, our convolutional network model achieves 0.69\% on the standard MNIST dataset. This is a state-of-the-art result on MNIST among algorithms that do not use distortions or pretraining. Copyright 2011 by the author(s)/owner(s).},
  isbn = {978-1-4503-0619-5},
  keywords = {Optimization Methods}
}

@inproceedings{lianXDeepFMCombiningExplicit2018,
  title = {{{xDeepFM}}: {{Combining Explicit}} and {{Implicit Feature Interactions}} for {{Recommender Systems}}},
  shorttitle = {{{xDeepFM}}},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Lian, Jianxun and Zhou, Xiaohuan and Zhang, Fuzheng and Chen, Zhongxia and Xie, Xing and Sun, Guangzhong},
  year = {2018},
  month = jul,
  eprint = {1803.05170},
  primaryclass = {cs},
  pages = {1754--1763},
  doi = {10.1145/3219819.3220023},
  urldate = {2022-06-17},
  abstract = {Combinatorial features are essential for the success of many commercial models. Manually crafting these features usually comes with high cost due to the variety, volume and velocity of raw data in web-scale systems. Factorization based models, which measure interactions in terms of vector product, can learn patterns of combinatorial features automatically and generalize to unseen features as well. With the great success of deep neural networks (DNNs) in various fields, recently researchers have proposed several DNN-based factorization model to learn both low- and high-order feature interactions. Despite the powerful ability of learning an arbitrary function from data, plain DNNs generate feature interactions implicitly and at the bit-wise level. In this paper, we propose a novel Compressed Interaction Network (CIN), which aims to generate feature interactions in an explicit fashion and at the vector-wise level. We show that the CIN share some functionalities with convolutional neural networks (CNNs) and recurrent neural networks (RNNs). We further combine a CIN and a classical DNN into one unified model, and named this new model eXtreme Deep Factorization Machine (xDeepFM). On one hand, the xDeepFM is able to learn certain bounded-degree feature interactions explicitly; on the other hand, it can learn arbitrary low- and high-order feature interactions implicitly. We conduct comprehensive experiments on three real-world datasets. Our results demonstrate that xDeepFM outperforms state-of-the-art models. We have released the source code of xDeepFM at {\textbackslash}url\{https://github.com/Leavingseason/xDeepFM\}.},
  archiveprefix = {arxiv},
  keywords = {click through prediction,Computer Science - Information Retrieval,Computer Science - Machine Learning,feature interaction},
  file = {/home/luisaam/Zotero/storage/DEC6CEQS/Lian et al. - 2018 - xDeepFM Combining Explicit and Implicit Feature I.pdf;/home/luisaam/Zotero/storage/XYI8Y242/1803.html}
}

@misc{liDHPDifferentiableMeta2020,
  title = {{{DHP}}: {{Differentiable Meta Pruning}} via {{HyperNetworks}}},
  shorttitle = {{{DHP}}},
  author = {Li, Yawei and Gu, Shuhang and Zhang, Kai and Van Gool, Luc and Timofte, Radu},
  year = {2020},
  month = aug,
  number = {arXiv:2003.13683},
  eprint = {2003.13683},
  primaryclass = {cs, eess},
  publisher = {{arXiv}},
  urldate = {2023-08-11},
  abstract = {Network pruning has been the driving force for the acceleration of neural networks and the alleviation of model storage/transmission burden. With the advent of AutoML and neural architecture search (NAS), pruning has become topical with automatic mechanism and searching based architecture optimization. Yet, current automatic designs rely on either reinforcement learning or evolutionary algorithm. Due to the non-differentiability of those algorithms, the pruning algorithm needs a long searching stage before reaching the convergence. To circumvent this problem, this paper introduces a differentiable pruning method via hypernetworks for automatic network pruning. The specifically designed hypernetworks take latent vectors as input and generate the weight parameters of the backbone network. The latent vectors control the output channels of the convolutional layers in the backbone network and act as a handle for the pruning of the layers. By enforcing \${\textbackslash}ell\_1\$ sparsity regularization to the latent vectors and utilizing proximal gradient solver, sparse latent vectors can be obtained. Passing the sparsified latent vectors through the hypernetworks, the corresponding slices of the generated weight parameters can be removed, achieving the effect of network pruning. The latent vectors of all the layers are pruned together, resulting in an automatic layer configuration. Extensive experiments are conducted on various networks for image classification, single image super-resolution, and denoising. And the experimental results validate the proposed method.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Hyper Networks,Pruning},
  file = {/home/luisaam/Zotero/storage/REX6QH6U/Li et al. - 2020 - DHP Differentiable Meta Pruning via HyperNetworks.pdf;/home/luisaam/Zotero/storage/8B8SMGFU/2003.html}
}

@inproceedings{liEagleeyeFastSubnet2020,
  title = {Eagleeye: {{Fast}} Sub-Net Evaluation for Efficient Neural Network Pruning},
  booktitle = {Computer {{Vision}}{\textendash}{{ECCV}} 2020: 16th {{European Conference}}, {{Glasgow}}, {{UK}}, {{August}} 23{\textendash}28, 2020, {{Proceedings}}, {{Part II}} 16},
  author = {Li, Bailin and Wu, Bowen and Su, Jiang and Wang, Guangrun},
  year = {2020},
  pages = {639--654},
  publisher = {{Springer}},
  keywords = {Pruning}
}

@article{liGroupSparsityHinge,
  title = {Group {{Sparsity}}: {{The Hinge Between Filter Pruning}} and {{Decomposition}} for {{Network Compression}}. ({{arXiv}}:2003.08935v1 [Cs.{{CV}}])},
  author = {Li, Yawei and Gu, Shuhang and Mayer, Christoph and Gool, Luc Van and Timofte, Radu},
  journal = {arXiv Computer Science},
  doi = {arXiv:2003.08935v1},
  abstract = {In this paper, we analyze two popular network compression techniques, i.e. filter pruning and low-rank decomposition, in a unified sense. By simply changing the way the sparsity regularization is enforced, filter pruning and low-rank decomposition can be derived accordingly. This provides another flexible choice for network compression because the techniques complement each other. For example, in popular network architectures with shortcut connections (e.g. ResNet), filter pruning cannot deal with the last convolutional layer in a ResBlock while the low-rank decomposition methods can. In addition, we propose to compress the whole network jointly instead of in a layer-wise manner. Our approach proves its potential as it compares favorably to the state-of-the-art on several benchmarks.},
  keywords = {Pruning,Researcher App,to read}
}

@inproceedings{liHessianBasedAnalysis2020,
  title = {Hessian Based Analysis of Sgd for Deep Nets: {{Dynamics}} and Generalization},
  booktitle = {Proceedings of the 2020 {{SIAM}} International Conference on Data Mining},
  author = {Li, Xinyan and Gu, Qilong and Zhou, Yingxue and Chen, Tiancong and Banerjee, Arindam},
  year = {2020},
  pages = {190--198},
  organization = {{SIAM}},
  keywords = {Hessian}
}

@inproceedings{liMeasuringIntrinsicDimension2022,
  title = {Measuring the {{Intrinsic Dimension}} of {{Objective Landscapes}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and Yosinski, Jason},
  year = {2022},
  month = feb,
  urldate = {2022-09-25},
  abstract = {We train in random subspaces of parameter space to measure how many dimensions are really needed to find a solution.},
  langid = {english},
  keywords = {Pruning},
  file = {/home/luisaam/Zotero/storage/CXVJ7HQU/Li et al. - 2022 - Measuring the Intrinsic Dimension of Objective Lan.pdf;/home/luisaam/Zotero/storage/NZLRFIX4/forum.html}
}

@article{LinearConvergencePolicy,
  title = {Linear Convergence of a Policy Gradient Method for Finite Horizon Continuous Time Stochastic Control Problems. ({{arXiv}}:2203.11758v1 [Math.{{OC}}])},
  journal = {arXiv Probability},
  doi = {arXiv:2203.11758v1},
  abstract = {Despite its popularity in the reinforcement learning community, a provably convergent policy gradient method for general continuous space-time stochastic control problems has been elusive. This paper closes the gap by proposing a proximal gradient algorithm for feedback controls of finite-time horizon stochastic control problems. The state dynamics are continuous time nonlinear diffusions with controlled drift and possibly degenerate noise, and the objectives are nonconvex in the state and nonsmooth in the control. We prove under suitable conditions that the algorithm converges linearly to a stationary point of the control problem, and is stable with respect to policy updates by approximate gradient steps. The convergence result justifies the recent reinforcement learning heuristics that adding entropy regularization to the optimization objective accelerates the convergence of policy gradient methods. The proof exploits careful regularity estimates of backward stochastic differential equations.},
  keywords = {Researcher App}
}

@article{linParetoSetLearning2022,
  title = {Pareto Set Learning for Neural Multi-Objective Combinatorial Optimization},
  author = {Lin, Xi and Yang, Zhiyuan and Zhang, Qingfu},
  year = {2022},
  journal = {arXiv preprint arXiv:2203.15386},
  eprint = {2203.15386},
  archiveprefix = {arxiv},
  keywords = {learning in optimization}
}

@article{liOverParameterizedDeepNeural2018,
  title = {Over-{{Parameterized Deep Neural Networks Have No Strict Local Minima For Any Continuous Activations}}},
  author = {Li, Dawei and Ding, Tian and Sun, Ruoyu},
  year = {2018},
  journal = {ArXiv},
  abstract = {In this paper, we study the loss surface of the over-parameterized fully connected deep neural networks. We prove that for any continuous activation functions, the loss function has no bad strict local minimum, both in the regular sense and in the sense of sets. This result holds for any convex and differentiable loss function, and the data samples are only required to be distinct in at least one dimension. Furthermore, we show that bad local minima do exist for a class of activation functions, so without further assumptions it is impossible to prove every local minimum is a global minimum.},
  keywords = {Why no global second order},
  file = {/home/luisaam/Zotero/storage/PKK2M57U/Li et al. - 2018 - Over-Parameterized Deep Neural Networks Have No St.pdf}
}

@inproceedings{liPruningFiltersEfficient2017,
  title = {Pruning {{Filters}} for {{Efficient ConvNets}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
  year = {2017},
  file = {/home/luisaam/Zotero/storage/ZT8DWM7J/Li et al. - 2017 - Pruning Filters for Efficient ConvNets.pdf}
}

@article{liTranslationBasedSequentialRecommendation2020,
  title = {Translation-{{Based Sequential Recommendation}} for {{Complex Users}} on {{Sparse Data}}},
  author = {Li, Hui and Liu, Ye and Mamoulis, Nikos and Rosenblum, David S.},
  year = {2020},
  month = aug,
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {32},
  number = {8},
  pages = {1639--1651},
  issn = {1558-2191},
  doi = {10.1109/TKDE.2019.2906180},
  abstract = {Sequential recommendation is one of the main tasks in recommender systems, where the next action (e.g., purchase, visit, and click) of the user is predicted based on his/her past sequence of actions. Translating Embeddings is a knowledge graph completion approach which was recently adapted to a translation-based sequential recommendation (TransRec) method. We observe a flaw of TransRec when handling complex translations, which hinders it from generating accurate suggestions. In view of this, we propose a translation-based recommender for complex users (CTransRec), which utilizes category-specific projection and temporal dynamic relaxation. Using our proposed Margin-based Pairwise Bayesian Personalized Ranking and Time-Aware Negative Sampling, CTransRec outperforms state-of-the-art methods for sequential recommendation on extremely sparse data. The superiority of CTransRec, which is confirmed by our extensive experiments on both public data and real data obtained from the industry, comes from not only the additional information used in training but also the fact that CTransRec makes good use of this additional information to model the complex translations.},
  keywords = {Bayes methods,Data models,Recommender systems,Recurrent neural networks,sequential behavior,sequential recommendation,Task analysis,Training},
  file = {/home/luisaam/Zotero/storage/JA5JQXKY/8669829.html}
}

@inproceedings{liuDeepEnsemblingNo2021,
  title = {Deep {{Ensembling}} with {{No Overhead}} for Either {{Training}} or {{Testing}}: {{The All-Round Blessings}} of {{Dynamic Sparsity}}},
  shorttitle = {Deep {{Ensembling}} with {{No Overhead}} for Either {{Training}} or {{Testing}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Liu, Shiwei and Chen, Tianlong and Atashgahi, Zahra and Chen, Xiaohan and Sokar, Ghada and Mocanu, Elena and Pechenizkiy, Mykola and Wang, Zhangyang and Mocanu, Decebal Constantin},
  year = {2021},
  month = sep,
  urldate = {2022-06-03},
  abstract = {The success of deep ensembles on improving predictive performance, uncertainty estimation, and out-of-distribution robustness has been extensively studied in the machine learning literature. Albeit...},
  langid = {english},
  keywords = {dynamical sparse training,enembling,good references},
  file = {/home/luisaam/Zotero/storage/4AQT8L9S/Liu et al. - 2021 - Deep Ensembling with No Overhead for either Traini.pdf;/home/luisaam/Zotero/storage/RPTY9NW4/forum.html}
}

@inproceedings{liuDynamicSparseTraining2020,
  title = {Dynamic {{Sparse Training}}: {{Find Efficient Sparse Network From Scratch With Trainable Masked Layers}}},
  shorttitle = {Dynamic {{Sparse Training}}},
  booktitle = {Eighth {{International Conference}} on {{Learning Representations}}},
  author = {Liu, Junjie and Xu, Zhe and Shi, Runbin and Cheung, Ray C. C. and So, Hayden K. H.},
  year = {2020},
  month = apr,
  urldate = {2021-11-15},
  abstract = {We present a novel network pruning algorithm called Dynamic Sparse Training that can jointly find the optimal network parameters and sparse network structure in a unified optimization process with trainable pruning thresholds. These thresholds can have fine-grained layer-wise adjustments dynamically via backpropagation. We demonstrate that our dynamic sparse training algorithm can easily train very sparse neural network models with little performance loss using the same training epochs as dense models. Dynamic Sparse Training achieves prior art performance compared with other sparse training algorithms on various network architectures. Additionally, we have several surprising observations that provide strong evidence to the effectiveness and efficiency of our algorithm. These observations reveal the underlying problems of traditional three-stage pruning algorithms and present the potential guidance provided by our algorithm to the design of more compact network architectures.},
  langid = {english},
  keywords = {dynamical sparse training,Optimization,Pruning},
  file = {/home/luisaam/Zotero/storage/VLUFDEDU/Liu et al. - 2020 - Dynamic Sparse Training Find Efficient Sparse Net.pdf;/home/luisaam/Zotero/storage/5JLDJA6D/poster_SJlbGJrtDB.html}
}

@article{liuGameTheorybasedNetwork2019,
  title = {A Game Theory-Based Network Rumor Spreading Model: Based on Game Experiments},
  author = {Liu, Fengming and Li, Mingcai},
  year = {2019},
  journal = {International Journal of Machine Learning and Cybernetics},
  volume = {10},
  number = {6},
  pages = {1449--1457},
  publisher = {{Springer Berlin Heidelberg}},
  issn = {1868808X},
  doi = {10.1007/s13042-018-0826-5},
  abstract = {This paper considers the relation between a rumor maker and many disseminators as a game and sets up Explosion-Trust (ET) Game Model. This model regards rumor explosion degree and trust degree of source node as influential factors of rumor making and spreading. The purpose of on both sides of a game is to obtain a maximum benefit. Based on ET model, we make experiments imitating the mode of information dissemination in social networks. We find rumor makers don't make rumors with quite a high explosion degree because others will doubt the truth of such rumor bringing a low transmission rate. This paper finds an optimal value for rumor spreading by experiments and on what condition trust degree will have an important impact on rumor spreading. Based on experiment results, the authors sum up the universal characteristics of rumors spread widely. Conclusions will be used to detect rumors.},
  isbn = {0123456789},
  keywords = {Game theory,Network rumor,Rumor,Rumor spreading},
  file = {/home/luisaam/Zotero/storage/72CLHSYX/Liu, Li - 2019 - A game theory-based network rumor spreading model based on game experiments.pdf}
}

@article{liuSparseEvolutionaryDeep2021,
  title = {Sparse Evolutionary {{Deep Learning}} with over One Million Artificial Neurons on Commodity Hardware},
  author = {Liu, Shiwei and Mocanu, Decebal Constantin and Matavalam, Amarsagar Reddy Ramapuram and Pei, Yulong and Pechenizkiy, Mykola},
  year = {2021},
  month = jan,
  journal = {arXiv:1901.09181 [cs, stat]},
  eprint = {1901.09181},
  primaryclass = {cs, stat},
  urldate = {2022-03-24},
  abstract = {Artificial Neural Networks (ANNs) have emerged as hot topics in the research community. Despite the success of ANNs, it is challenging to train and deploy modern ANNs on commodity hardware due to the ever-increasing model size and the unprecedented growth in the data volumes. Particularly for microarray data, the very-high dimensionality and the small number of samples make it difficult for machine learning techniques to handle. Furthermore, specialized hardware such as Graphics Processing Unit (GPU) is expensive. Sparse neural networks are the leading approaches to address these challenges. However, off-the-shelf sparsity inducing techniques either operate from a pre-trained model or enforce the sparse structure via binary masks. The training efficiency of sparse neural networks cannot be obtained practically. In this paper, we introduce a technique allowing us to train truly sparse neural networks with fixed parameter count throughout training. Our experimental results demonstrate that our method can be applied directly to handle high dimensional data, while achieving higher accuracy than the traditional two phases approaches. Moreover, we have been able to create truly sparse MultiLayer Perceptrons (MLPs) models with over one million neurons and to train them on a typical laptop without GPU, this being way beyond what is possible with any state-of-the-art techniques.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,dynamical sparse training,Pruning,sparse neural networks,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/JBB5TG3L/Liu et al. - 2021 - Sparse evolutionary Deep Learning with over one mi.pdf;/home/luisaam/Zotero/storage/ZLLN2QBC/1901.html}
}

@inproceedings{liuUnreasonableEffectivenessRandom2022,
  title = {The {{Unreasonable Effectiveness}} of {{Random Pruning}}: {{Return}} of the {{Most Naive Baseline}} for {{Sparse Training}}},
  shorttitle = {The {{Unreasonable Effectiveness}} of {{Random Pruning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Shen, Li and Mocanu, Decebal Constantin and Wang, Zhangyang and Pechenizkiy, Mykola},
  year = {2022},
  month = mar,
  urldate = {2022-12-15},
  abstract = {Random pruning is arguably the most naive way to attain sparsity in neural networks, but has been deemed uncompetitive by either post-training pruning or sparse training. In this paper, we focus on sparse training and highlight a perhaps counter-intuitive finding, that random pruning at initialization can be quite powerful for the sparse training of modern neural networks. Without any delicate pruning criteria or carefully pursued sparsity structures, we empirically demonstrate that sparsely training a randomly pruned network from scratch can match the performance of its dense equivalent. There are two key factors that contribute to this revival: (i) \$the network sizes matter\$: as the original dense networks grow wider and deeper, the performance of training a randomly pruned sparse network will quickly grow to matching that of its dense equivalent, even at high sparsity ratios; (ii) \$appropriate layer-wise sparsity ratios\$ can be pre-chosen for sparse training, which shows to be another important performance booster. Simple as it looks, a randomly pruned subnetwork of Wide ResNet-50 can be sparsely trained to outperforming a dense Wide ResNet-50, on ImageNet. We also observed such randomly pruned networks outperform dense counterparts in other favorable aspects, such as out-of-distribution detection, uncertainty estimation, and adversarial robustness. Overall, our results strongly suggest there is larger-than-expected room for sparse training at scale, and the benefits of sparsity might be more universal beyond carefully designed pruning. Our source code can be found at https://github.com/VITA-Group/Random\_Pruning.},
  langid = {english},
  keywords = {dynamical sparse training,Pruning,Random Pruning},
  file = {/home/luisaam/Zotero/storage/LLN3P42U/Liu et al. - 2022 - The Unreasonable Effectiveness of Random Pruning .pdf}
}

@inproceedings{liuUnreasonableEffectivenessRandom2022a,
  title = {The {{Unreasonable Effectiveness}} of {{Random Pruning}}: {{Return}} of the {{Most Naive Baseline}} for {{Sparse Training}}},
  shorttitle = {The {{Unreasonable Effectiveness}} of {{Random Pruning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Liu, Shiwei and Chen, Tianlong and Chen, Xiaohan and Shen, Li and Mocanu, Decebal Constantin and Wang, Zhangyang and Pechenizkiy, Mykola},
  year = {2022},
  month = jan,
  urldate = {2023-05-25},
  abstract = {Random pruning is arguably the most naive way to attain sparsity in neural networks, but has been deemed uncompetitive by either post-training pruning or sparse training. In this paper, we focus on sparse training and highlight a perhaps counter-intuitive finding, that random pruning at initialization can be quite powerful for the sparse training of modern neural networks. Without any delicate pruning criteria or carefully pursued sparsity structures, we empirically demonstrate that sparsely training a randomly pruned network from scratch can match the performance of its dense equivalent. There are two key factors that contribute to this revival: (i) \$the network sizes matter\$: as the original dense networks grow wider and deeper, the performance of training a randomly pruned sparse network will quickly grow to matching that of its dense equivalent, even at high sparsity ratios; (ii) \$appropriate layer-wise sparsity ratios\$ can be pre-chosen for sparse training, which shows to be another important performance booster. Simple as it looks, a randomly pruned subnetwork of Wide ResNet-50 can be sparsely trained to outperforming a dense Wide ResNet-50, on ImageNet. We also observed such randomly pruned networks outperform dense counterparts in other favorable aspects, such as out-of-distribution detection, uncertainty estimation, and adversarial robustness. Overall, our results strongly suggest there is larger-than-expected room for sparse training at scale, and the benefits of sparsity might be more universal beyond carefully designed pruning. Our source code can be found at https://github.com/VITA-Group/Random\_Pruning.},
  langid = {english},
  keywords = {dynamical sparse training,Pruning,Random Pruning},
  file = {/home/luisaam/Zotero/storage/TQGJZ9UK/Liu et al. - 2022 - The Unreasonable Effectiveness of Random Pruning .pdf}
}

@article{liVisualizingLossLandscape2018,
  title = {Visualizing the {{Loss Landscape}} of {{Neural Nets}}},
  author = {Li, Hao and Xu, Zheng and Taylor, Gavin and Studer, Christoph and Goldstein, Tom},
  year = {2018},
  month = nov,
  journal = {arXiv:1712.09913 [cs, stat]},
  eprint = {1712.09913},
  primaryclass = {cs, stat},
  urldate = {2021-04-29},
  abstract = {Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Loss Landscape,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/JD8EZKKV/Li et al. - 2018 - Visualizing the Loss Landscape of Neural Nets.pdf;/home/luisaam/Zotero/storage/97F97W7X/1712.html}
}

@article{louatiDeepLearningbasedMultiagent2020,
  title = {A Deep Learning-Based Multi-Agent System for Intrusion Detection},
  author = {Louati, Faten and Ktata, Farah Barika},
  year = {2020},
  month = mar,
  journal = {SN Applied Sciences},
  volume = {2},
  number = {4},
  pages = {675},
  issn = {2523-3971},
  doi = {10.1007/s42452-020-2414-z},
  urldate = {2022-03-16},
  abstract = {Intrusion detection systems play an important role in preventing attacks which have been increased rapidly due to the dependence on network and Internet connectivity. Deep learning algorithms are promising techniques, which have been used in many classification problems. In the same way, multi-agent systems become a new useful approach in intrusion detection field. In this paper, we propose a deep learning-based multi-agent system for intrusion detection which combines the desired features of multi-agent system approach with the precision of deep learning algorithms. Therefore, we created a number of autonomous, intelligent and adaptive agents that implanted three algorithms, namely autoencoder, multilayer perceptron and k-nearest neighbors. Autoencoder is used as features reduction tool, and multilayer perceptron and k-nearest neighbors are used as classifiers. The performance of our model is compared against traditional machine learning approaches and other multi-agent system-based systems. The experiments have shown that our hybrid distributed intrusion detection system achieves the detection with better accuracy rate and it reduces considerably the time of detection.},
  langid = {english},
  keywords = {pratical application},
  file = {/home/luisaam/Zotero/storage/BLN5JG4M/Louati y Ktata - 2020 - A deep learning-based multi-agent system for intru.pdf}
}

@inproceedings{lubanaGradientFlowFramework2021,
  title = {A Gradient Flow Framework for Analyzing Network Pruning},
  booktitle = {International Conference on Learning Representations},
  author = {Lubana, Ekdeep Singh and Dick, Robert},
  year = {2021},
  keywords = {Pruning,to read},
  file = {/home/luisaam/Zotero/storage/JBTVASXA/Lubana and Dick - 2021 - A gradient flow framework for analyzing network pr.pdf}
}

@inproceedings{lubanaGradientFlowFramework2022,
  title = {A {{Gradient Flow Framework For Analyzing Network Pruning}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Lubana, Ekdeep Singh and Dick, Robert},
  year = {2022},
  month = feb,
  urldate = {2022-12-21},
  abstract = {Recent network pruning methods focus on pruning models early-on in training. To estimate the impact of removing a parameter, these methods use importance measures that were originally designed to prune trained models. Despite lacking justification for their use early-on in training, such measures result in surprisingly low accuracy loss. To better explain this behavior, we develop a general framework that uses gradient flow to unify state-of-the-art importance measures through the norm of model parameters. We use this framework to determine the relationship between pruning measures and evolution of model parameters, establishing several results related to pruning models early-on in training: (i) magnitude-based pruning removes parameters that contribute least to reduction in loss, resulting in models that converge faster than magnitude-agnostic methods; (ii) loss-preservation based pruning preserves first-order model evolution dynamics and its use is therefore justified for pruning minimally trained models; and (iii) gradient-norm based pruning affects second-order model evolution dynamics, such that increasing gradient norm via pruning can produce poorly performing models. We validate our claims on several VGG-13, MobileNet-V1, and ResNet-56 models trained on CIFAR-10/CIFAR-100.},
  langid = {english},
  keywords = {gradient flow,Pruning},
  file = {/home/luisaam/Zotero/storage/L4UMKNZJ/Lubana y Dick - 2022 - A Gradient Flow Framework For Analyzing Network Pr.pdf}
}

@article{lucasAnalyzingMonotonicLinear2021,
  title = {Analyzing Monotonic Linear Interpolation in Neural Network Loss Landscapes},
  author = {Lucas, James and Bae, Juhan and Zhang, Michael R. and Fort, Stanislav and Zemel, Richard S. and Grosse, Roger B.},
  year = {2021},
  journal = {CoRR},
  volume = {abs/2104.11044},
  cdate = {1609459200000},
  publtype = {informal},
  keywords = {linear interpolation}
}

@article{luDepthCreatesNo2017,
  title = {Depth {{Creates No Bad Local Minima}}},
  author = {Lu, Haihao and Kawaguchi, Kenji},
  year = {2017},
  month = may,
  journal = {arXiv:1702.08580 [cs, math, stat]},
  eprint = {1702.08580},
  primaryclass = {cs, math, stat},
  urldate = {2021-06-28},
  abstract = {In deep learning, {\textbackslash}textit\{depth\}, as well as {\textbackslash}textit\{nonlinearity\}, create non-convex loss surfaces. Then, does depth alone create bad local minima? In this paper, we prove that without nonlinearity, depth alone does not create bad local minima, although it induces non-convex loss surface. Using this insight, we greatly simplify a recently proposed proof to show that all of the local minima of feedforward deep linear neural networks are global minima. Our theoretical results generalize previous results with fewer assumptions, and this analysis provides a method to show similar results beyond square loss in deep linear models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Mathematics - Optimization and Control,Statistics - Machine Learning,Why no global second order},
  file = {/home/luisaam/Zotero/storage/XL26D5LK/Lu y Kawaguchi - 2017 - Depth Creates No Bad Local Minima.pdf;/home/luisaam/Zotero/storage/CBYD2QW7/1702.html}
}

@inproceedings{luoUnderstandingEffectiveReceptive2016,
  title = {Understanding the {{Effective Receptive Field}} in {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Luo, Wenjie and Li, Yujia and Urtasun, Raquel and Zemel, Richard},
  year = {2016},
  volume = {29},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-11-20},
  abstract = {We study characteristics of receptive fields of units in deep convolutional networks. The receptive field size is a crucial issue in many visual tasks, as the output must respond to large enough areas in the image to capture information about large objects. We introduce the notion of an effective receptive field size, and show that it both has a Gaussian distribution and   only occupies a fraction of the full theoretical receptive field size. We analyze the effective receptive field in several architecture designs, and the effect of sub-sampling, skip connections, dropout and nonlinear activations on it. This leads to suggestions for ways to address its tendency to be too small.},
  keywords = {Receptive Field},
  file = {/home/luisaam/Zotero/storage/NBCQTAZL/Luo et al. - 2016 - Understanding the Effective Receptive Field in Dee.pdf}
}

@article{lyuGradientDescentTwolayer2021,
  title = {Gradient {{Descent}} on {{Two-layer Nets}}: {{Margin Maximization}} and {{Simplicity Bias}}},
  shorttitle = {Gradient {{Descent}} on {{Two-layer Nets}}},
  author = {Lyu, Kaifeng and Li, Zhiyuan and Wang, Runzhe and Arora, Sanjeev},
  year = {2021},
  month = nov,
  journal = {arXiv:2110.13905 [cs]},
  eprint = {2110.13905},
  primaryclass = {cs},
  urldate = {2021-11-16},
  abstract = {The generalization mystery of overparametrized deep nets has motivated efforts to understand how gradient descent (GD) converges to low-loss solutions that generalize well. Real-life neural networks are initialized from small random values and trained with cross-entropy loss for classification (unlike the "lazy" or "NTK" regime of training where analysis was more successful), and a recent sequence of results (Lyu and Li, 2020; Chizat and Bach, 2020; Ji and Telgarsky, 2020) provide theoretical evidence that GD may converge to the "max-margin" solution with zero loss, which presumably generalizes well. However, the global optimality of margin is proved only in some settings where neural nets are infinitely or exponentially wide. The current paper is able to establish this global optimality for two-layer Leaky ReLU nets trained with gradient flow on linearly separable and symmetric data, regardless of the width. The analysis also gives some theoretical justification for recent empirical findings (Kalimeris et al., 2019) on the so-called simplicity bias of GD towards linear or other "simple" classes of solutions, especially early in training. On the pessimistic side, the paper suggests that such results are fragile. A simple data manipulation can make gradient flow converge to a linear classifier with suboptimal margin.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Pruning},
  file = {/home/luisaam/Zotero/storage/7N6Y4W7T/Lyu et al. - 2021 - Gradient Descent on Two-layer Nets Margin Maximiz.pdf;/home/luisaam/Zotero/storage/WUHP7SAL/2110.html}
}

@article{mairesseUsingLinguisticCues2007,
  title = {Using {{Linguistic Cues}} for the {{Automatic Recognition}} of {{Personality}} in {{Conversation}} and {{Text}}},
  author = {Mairesse, F. and Walker, M. A. and Mehl, M. R. and Moore, R. K.},
  year = {2007},
  month = nov,
  journal = {Journal of Artificial Intelligence Research},
  volume = {30},
  pages = {457--500},
  issn = {1076-9757},
  doi = {10.1613/jair.2349},
  urldate = {2023-07-07},
  abstract = {It is well known that utterances convey a great deal of information about the speaker in addition to their semantic content.  One such type of information consists of cues to the speaker's personality traits, the most fundamental dimension of variation between humans.  Recent work explores the automatic detection of other types of pragmatic variation in text and conversation, such as emotion, deception, speaker charisma, dominance, point of view, subjectivity, opinion and sentiment. Personality affects these other aspects of linguistic production, and thus personality recognition may be useful for these tasks, in addition to many other potential applications.  However, to date, there is little work on the automatic recognition of personality traits.  This article reports experimental results for recognition of all Big Five personality traits, in both conversation and text, utilising both self and observer ratings of personality.  While other work reports classification results, we experiment with classification, regression and ranking models. For each model, we analyse the effect of different feature sets on accuracy. Results show that for some traits, any type of statistical model performs significantly better than the baseline, but ranking models perform best overall. We also present an experiment suggesting that ranking models are more accurate than multi-class classifiers for modelling personality. In addition, recognition models trained on observed personality perform better than models trained using self-reports, and the optimal feature set depends on the personality trait. A qualitative analysis of the learned models confirms previous findings linking language and personality, while revealing many new linguistic markers.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {Big 5,NLP,Psycholinguistics},
  file = {/home/luisaam/Zotero/storage/6CPXH54I/Mairesse et al. - 2007 - Using Linguistic Cues for the Automatic Recognitio.pdf}
}

@phdthesis{malialisDistributedReinforcementLearning2014,
  title = {Distributed {{Reinforcement Learning}} for {{Network Intrusion Response}}},
  author = {Malialis, Kleanthis},
  year = {2014},
  month = sep,
  urldate = {2022-03-18},
  abstract = {The increasing adoption of technologies and the exponential growth of networks has made the area of information technology an integral part of our lives, where network security plays a vital role. One of the most serious threats in the current Internet is posed by distributed denial of service (DDoS) attacks, which target the availability of the victim system. Such an attack is designed to exhaust a server's resources or congest a network's infrastructure, and therefore renders the victim incapable of providing services to its legitimate users or customers. To tackle the distributed nature of these attacks, a distributed and coordinated defence mechanism is necessary, where many defensive nodes, across different locations cooperate in order to stop or reduce the flood. This thesis investigates the applicability of distributed reinforcement learning to intrusion response, specifically, DDoS response. We propose a novel approach to respond to DDoS attacks called Multiagent Router Throttling. Multiagent Router Throttling provides an agent-based distributed response to the DDoS problem, where multiple reinforcement learning agents are installed on a set of routers and learn to rate-limit or throttle traffic towards a victim server. One of the novel characteristics of the proposed approach is that it has a decentralised architecture and provides a decentralised coordinated response to the DDoS problem, thus being resilient to the attacks themselves. Scalability constitutes a critical aspect of a defence system since a non-scalable mechanism will never be considered, let alone adopted, for wide deployment by a company or organisation. We propose Coordinated Team Learning (CTL) which is a novel design to the original Multiagent Router Throttling approach based on the divide-and-conquer paradigm, that uses task decomposition and coordinated team rewards. To better scale-up CTL is combined with a form of reward shaping. The scalability of the proposed system is successfully demonstrated in experiments involving up to 1000 reinforcement learning agents. The significant improvements on scalability and learning speed lay the foundations for a potential real-world deployment.},
  copyright = {cc\_by\_nc\_nd},
  langid = {english},
  school = {University of York},
  keywords = {cybersecurity,multi-agent system,Reinforcement Learning},
  file = {/home/luisaam/Zotero/storage/KBC62J65/Malialis - 2014 - Distributed Reinforcement Learning for Network Int.pdf;/home/luisaam/Zotero/storage/N62ZTBT6/8109.html}
}

@article{mandischerComparisonEvolutionStrategies2002,
  title = {A Comparison of Evolution Strategies and Backpropagation for Neural Network Training},
  author = {Mandischer, M.},
  year = {2002},
  month = jan,
  journal = {Neurocomputing},
  series = {Evolutionary Neural Systems},
  volume = {42},
  number = {1},
  pages = {87--117},
  issn = {0925-2312},
  doi = {10.1016/S0925-2312(01)00596-3},
  urldate = {2021-07-12},
  abstract = {This report investigates evolution strategies (ESs, a subclass of evolutionary algorithms) as an alternative to gradient-based neural network training. Based on an empirical comparison of population- and gradient-based search, we derive hints for parameterization and draw conclusions about the usefulness of evolution strategies for this purpose. We will see that ESs can only compete with gradient-based search in the case of small problems and that ESs are good for training neural networks with a non-differentiable activation function. Insights into how evolution strategies behave in search spaces generated by neural networks are offered. Here, we see that for this class of objective functions, the dimensionality of the problem is critical. With increasing numbers of decision variables, the learning becomes more and more difficult for ESs, and an ``efficient'' parameterization becomes crucial.},
  langid = {english},
  keywords = {Backpropagation,Evolution strategy,Gradient search,Learning,Neural networks,Weight space},
  file = {/home/luisaam/Zotero/storage/VRAYDKJD/Mandischer - 2002 - A comparison of evolution strategies and backpropa.pdf;/home/luisaam/Zotero/storage/X596MBDK/S0925231201005963.html}
}

@article{mandischerComparisonEvolutionStrategies2002a,
  title = {A Comparison of Evolution Strategies and Backpropagation for Neural Network Training},
  author = {Mandischer, M.},
  year = {2002},
  month = jan,
  journal = {Neurocomputing},
  series = {Evolutionary Neural Systems},
  volume = {42},
  number = {1},
  pages = {87--117},
  issn = {0925-2312},
  doi = {10.1016/S0925-2312(01)00596-3},
  urldate = {2022-01-31},
  abstract = {This report investigates evolution strategies (ESs, a subclass of evolutionary algorithms) as an alternative to gradient-based neural network training. Based on an empirical comparison of population- and gradient-based search, we derive hints for parameterization and draw conclusions about the usefulness of evolution strategies for this purpose. We will see that ESs can only compete with gradient-based search in the case of small problems and that ESs are good for training neural networks with a non-differentiable activation function. Insights into how evolution strategies behave in search spaces generated by neural networks are offered. Here, we see that for this class of objective functions, the dimensionality of the problem is critical. With increasing numbers of decision variables, the learning becomes more and more difficult for ESs, and an ``efficient'' parameterization becomes crucial.},
  langid = {english},
  keywords = {Backpropagation,Evolution strategy,Gradient search,Learning,Neural networks,Weight space},
  file = {/home/luisaam/Zotero/storage/F288J8DA/S0925231201005963.html}
}

@article{mardiaMeasuresMultivariateSkewness1970,
  title = {Measures of Multivariate Skewness and Kurtosis with Applications},
  author = {MARDIA, K. V.},
  year = {1970},
  month = dec,
  journal = {Biometrika},
  volume = {57},
  number = {3},
  eprint = {https://academic.oup.com/biomet/article-pdf/57/3/519/702615/57-3-519.pdf},
  pages = {519--530},
  issn = {0006-3444},
  doi = {10.1093/biomet/57.3.519},
  abstract = {Measures of multivariate skewness and kurtosis are developed by extending certain studies on robustness of the t statistic. These measures are shown to possess desirable properties. The asymptotic distributions of the measures for samples from a multivariate normal population are derived and a test of multivariate normality is proposed. The effect of nonnormality on the size of the one-sample Hotelling's T2 test is studied empirically with the help of these measures, and it is found that Hotelling's T2 test is more sensitive to the measure of skewness than to the measure of kurtosis.}
}

@inproceedings{martensOptimizingNeuralNetworks2015,
  title = {Optimizing {{Neural Networks}} with {{Kronecker-factored Approximate Curvature}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Martens, James and Grosse, Roger},
  year = {2015},
  month = jun,
  pages = {2408--2417},
  publisher = {{PMLR}},
  issn = {1938-7228},
  urldate = {2021-08-21},
  langid = {english},
  keywords = {neural network,second order},
  file = {/home/luisaam/Zotero/storage/C52GFYGH/Martens y Grosse - 2015 - Optimizing Neural Networks with Kronecker-factored.pdf}
}

@article{mengSphericalTextEmbedding2019,
  title = {Spherical {{Text Embedding}}},
  author = {Meng, Yu and Huang, Jiaxin and Wang, Guangyuan and Zhang, Chao and Zhuang, Honglei and Kaplan, Lance and Han, Jiawei},
  year = {2019},
  number = {NeurIPS},
  pages = {1--10},
  issn = {10495258},
  abstract = {Unsupervised text embedding has shown great power in a wide range of NLP tasks. While text embeddings are typically learned in the Euclidean space, directional similarity is often more effective in tasks such as word similarity and document clustering, which creates a gap between the training stage and usage stage of text embedding. To close this gap, we propose a spherical generative model based on which unsupervised word and paragraph embeddings are jointly learned. To learn text embeddings in the spherical space, we develop an efficient optimization algorithm with convergence guarantee based on Riemannian optimization. Our model enjoys high efficiency and achieves state-of-the-art performances on various text embedding tasks including word similarity and document clustering.},
  file = {/home/luisaam/Zotero/storage/894NCJNY/9031-spherical-text-embedding.pdf}
}

@article{mhaskarApproximationPropertiesMultilayered1993,
  title = {Approximation Properties of a Multilayered Feedforward Artificial Neural Network},
  author = {Mhaskar, H. N.},
  year = {1993},
  month = feb,
  journal = {Advances in Computational Mathematics},
  volume = {1},
  number = {1},
  pages = {61--80},
  issn = {1572-9044},
  doi = {10.1007/BF02070821},
  urldate = {2021-07-02},
  abstract = {We prove that an artificial neural network with multiple hidden layers and akth-order sigmoidal response function can be used to approximate any continuous function on any compact subset of a Euclidean space so as to achieve the Jackson rate of approximation. Moreover, if the function to be approximated has an analytic extension, then a nearly geometric rate of approximation can be achieved. We also discuss the problem of approximation of a compact subset of a Euclidean space with such networks with a classical sigmoidal response function.},
  langid = {english},
  keywords = {NN as universal approximators},
  file = {/home/luisaam/Zotero/storage/CV6Y3PZ3/Mhaskar - 1993 - Approximation properties of a multilayered feedfor.pdf}
}

@article{mhaskarDeepVsShallow2016,
  title = {Deep vs. Shallow Networks : {{An}} Approximation Theory Perspective},
  shorttitle = {Deep vs. Shallow Networks},
  author = {Mhaskar, Hrushikesh and Poggio, Tomaso},
  year = {2016},
  month = aug,
  journal = {arXiv:1608.03287 [cs, math]},
  eprint = {1608.03287},
  primaryclass = {cs, math},
  urldate = {2021-07-02},
  abstract = {The paper briefy reviews several recent results on hierarchical architectures for learning from examples, that may formally explain the conditions under which Deep Convolutional Neural Networks perform much better in function approximation problems than shallow, one-hidden layer architectures. The paper announces new results for a non-smooth activation function - the ReLU function - used in present-day neural networks, as well as for the Gaussian networks. We propose a new definition of relative dimension to encapsulate different notions of sparsity of a function class that can possibly be exploited by deep networks but not by shallow ones to drastically reduce the complexity required for approximation and learning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Functional Analysis,NN as universal approximators},
  file = {/home/luisaam/Zotero/storage/WHR6HY5Y/Mhaskar y Poggio - 2016 - Deep vs. shallow networks  An approximation theor.pdf;/home/luisaam/Zotero/storage/HNVSIXR7/1608.html}
}

@misc{microsoftresearchEfficientSecondorderOptimization,
  title = {Efficient {{Second-order Optimization}} for {{Machine Learning}}},
  author = {{Microsoft Research}},
  urldate = {2021-07-12},
  abstract = {Stochastic gradient-based methods are the state-of-the-art in large-scale machine learning optimization due to their extremely efficient per-iteration computational cost. Second-order methods, that use the second derivative of the optimization objective, are known to enable faster convergence. However, the latter has been much less explored due to the high cost of computing the second-order information. We will present second-order stochastic methods for (convex and non-convex) optimization problems arising in machine learning that match the per-iteration cost of gradient-based methods, yet enjoy the faster convergence properties of second-order optimization overall leading to faster algorithms than the best known gradient-based methods. See more at https://www.microsoft.com/en-us/resea...}
}

@article{mnihPlayingAtariDeep2013,
  title = {Playing {{Atari}} with {{Deep Reinforcement Learning}}},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  year = {2013},
  month = dec,
  journal = {arXiv:1312.5602 [cs]},
  eprint = {1312.5602},
  primaryclass = {cs},
  urldate = {2021-07-10},
  abstract = {We present the first deep learning model to successfully learn control policies directly from high-dimensional sensory input using reinforcement learning. The model is a convolutional neural network, trained with a variant of Q-learning, whose input is raw pixels and whose output is a value function estimating future rewards. We apply our method to seven Atari 2600 games from the Arcade Learning Environment, with no adjustment of the architecture or learning algorithm. We find that it outperforms all previous approaches on six of the games and surpasses a human expert on three of them.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning},
  file = {/home/luisaam/Zotero/storage/5HUGYPIX/Mnih et al. - 2013 - Playing Atari with Deep Reinforcement Learning.pdf;/home/luisaam/Zotero/storage/MPCX3SN5/1312.html}
}

@article{mocanuScalableTrainingArtificial2018,
  title = {Scalable Training of Artificial Neural Networks with Adaptive Sparse Connectivity Inspired by Network Science},
  author = {Mocanu, Decebal Constantin and Mocanu, Elena and Stone, Peter and Nguyen, Phuong H. and Gibescu, Madeleine and Liotta, Antonio},
  year = {2018},
  month = jun,
  journal = {Nature Communications},
  volume = {9},
  number = {1},
  pages = {2383},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-04316-3},
  urldate = {2022-03-02},
  abstract = {Through the success of deep learning in various domains, artificial neural networks are currently among the most used artificial intelligence methods. Taking inspiration from the network properties of biological neural networks (e.g. sparsity, scale-freeness), we argue that (contrary to general practice) artificial neural networks, too, should not have fully-connected layers. Here we propose sparse evolutionary training of artificial neural networks, an algorithm which evolves an initial sparse topology (Erd{\H o}s{\textendash}R{\'e}nyi random graph) of two consecutive layers of neurons into a scale-free topology, during learning. Our method replaces artificial neural networks fully-connected layers with sparse ones before training, reducing quadratically the number of parameters, with no decrease in accuracy. We demonstrate our claims on restricted Boltzmann machines, multi-layer perceptrons, and convolutional neural networks for unsupervised and supervised learning on 15 datasets. Our approach has the potential to enable artificial neural networks to scale up beyond what is currently possible.},
  copyright = {2018 The Author(s)},
  langid = {english},
  keywords = {Complex networks,Computer science,dynamical sparse training,Machine learning},
  file = {/home/luisaam/Zotero/storage/VG6CSPKI/Mocanu et al. - 2018 - Scalable training of artificial neural networks wi.pdf;/home/luisaam/Zotero/storage/MUPGAB8B/s41467-018-04316-3.html}
}

@misc{molchanovPruningConvolutionalNeural2017,
  title = {Pruning {{Convolutional Neural Networks}} for {{Resource Efficient Inference}}},
  author = {Molchanov, Pavlo and Tyree, Stephen and Karras, Tero and Aila, Timo and Kautz, Jan},
  year = {2017},
  month = jun,
  number = {arXiv:1611.06440},
  eprint = {1611.06440},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1611.06440},
  urldate = {2022-12-18},
  abstract = {We propose a new formulation for pruning convolutional kernels in neural networks to enable efficient inference. We interleave greedy criteria-based pruning with fine-tuning by backpropagation - a computationally efficient procedure that maintains good generalization in the pruned network. We propose a new criterion based on Taylor expansion that approximates the change in the cost function induced by pruning network parameters. We focus on transfer learning, where large pretrained networks are adapted to specialized tasks. The proposed criterion demonstrates superior performance compared to other criteria, e.g. the norm of kernel weights or feature map activation, for pruning large CNNs after adaptation to fine-grained classification tasks (Birds-200 and Flowers-102) relaying only on the first order gradient information. We also show that pruning can lead to more than 10x theoretical (5x practical) reduction in adapted 3D-convolutional filters with a small drop in accuracy in a recurrent gesture classifier. Finally, we show results for the large-scale ImageNet dataset to emphasize the flexibility of our approach.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Pruning,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/5U44NUFW/Molchanov et al. - 2017 - Pruning Convolutional Neural Networks for Resource.pdf;/home/luisaam/Zotero/storage/EMMQVN3G/1611.html}
}

@article{montanaTrainingFeedforwardNeural,
  title = {Training {{Feedforward Neural Networks Using Genetic Algorithms}}},
  author = {Montana, David J and Davis, Lawrence and St, Mouiton},
  pages = {6},
  abstract = {Multilayered feedforward neural networks possess a number of properties which make them particularly suited to complex pattern classification problems. However, their application to some realworld problems has been hampered by the lack of a training algonthm which reliably finds a nearly globally optimal set of weights in a relatively short time. Genetic algorithms are a class of optimization procedures which are good at exploring a large and complex space in an intelligent way to find values close to the global optimum. Hence, they are well suited to the problem of training feedforward networks. In this paper, we describe a set of experiments performed on data from a sonar image classification problem. These experiments both 1) illustrate the improvements gained by using a genetic algorithm rather than backpropagation and 2) chronicle the evolution of the performance of the genetic algorithm as we added more and more domain-specific knowledge into it.},
  langid = {english},
  file = {/home/luisaam/Zotero/storage/9NR4ACRC/Montana et al. - Training Feedforward Neural Networks Using Genetic.pdf}
}

@book{montavonNeuralNetworksTricks2012,
  title = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}},
  author = {Montavon, Gr{\'e}goire and Orr, Gb Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert and LeCun, Y and Bottou, L and Orr, Gb Genevi{\`e}ve B. and Muller, Kr and Montavon, Gr{\'e}goire and Orr, Gb Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  journal = {Springer Lecture Notes in Computer Sciences},
  issn = {0302-9743},
  abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observedb y practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposedin serious technical publications. This paper gives some of those tricks, ando.ers explanations of why they work. Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most ``classical'' second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
  isbn = {3-540-65311-2},
  pmid = {8514134}
}

@misc{morcosInsightsRepresentationalSimilarity2018,
  title = {Insights on Representational Similarity in Neural Networks with Canonical Correlation},
  author = {Morcos, Ari S. and Raghu, Maithra and Bengio, Samy},
  year = {2018},
  month = oct,
  number = {arXiv:1806.05759},
  eprint = {1806.05759},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1806.05759},
  urldate = {2023-09-28},
  abstract = {Comparing different neural network representations and determining how representations evolve over time remain challenging open questions in our understanding of the function of neural networks. Comparing representations in neural networks is fundamentally difficult as the structure of representations varies greatly, even across groups of networks trained on identical tasks, and over the course of training. Here, we develop projection weighted CCA (Canonical Correlation Analysis) as a tool for understanding neural networks, building off of SVCCA, a recently proposed method (Raghu et al., 2017). We first improve the core method, showing how to differentiate between signal and noise, and then apply this technique to compare across a group of CNNs, demonstrating that networks which generalize converge to more similar representations than networks which memorize, that wider networks converge to more similar solutions than narrow networks, and that trained networks with identical topology but different learning rates converge to distinct clusters with diverse representations. We also investigate the representational dynamics of RNNs, across both training and sequential timesteps, finding that RNNs converge in a bottom-up pattern over the course of training and that the hidden state is highly variable over the course of a sequence, even when accounting for linear transforms. Together, these results provide new insights into the function of CNNs and RNNs, and demonstrate the utility of using CCA to understand representations.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Neural Network Representation,Similarity,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/24XJL4PM/Morcos et al. - 2018 - Insights on representational similarity in neural .pdf;/home/luisaam/Zotero/storage/6DLNQRXF/1806.html}
}

@article{MoreLessInducing,
  title = {More Is {{Less}}: {{Inducing Sparsity}} via {{Overparameterization}}. ({{arXiv}}:2112.11027v2 [Math.{{OC}}] {{UPDATED}})},
  journal = {arXiv Optimization and Control},
  doi = {arXiv:2112.11027v2},
  abstract = {In deep learning it is common to overparameterize neural networks, that is, to use more parameters than training samples. Quite surprisingly training the neural network via (stochastic) gradient descent leads to models that generalize very well, while classical statistics would suggest overfitting. In order to gain understanding of this implicit bias phenomenon we study the special case of sparse recovery (compressed sensing) which is of interest on its own. More precisely, in order to reconstruct a vector from underdetermined linear measurements, we introduce a corresponding overparameterized square loss functional, where the vector to be reconstructed is deeply factorized into several vectors. We show that, if there exists an exact solution, vanilla gradient flow for the overparameterized loss functional converges to a good approximation of the solution of minimal -norm. The latter is well-known to promote sparse solutions. As a by-product, our results significantly improve the sample complexity for compressed sensing via gradient flow/descent on overparameterized models derived in previous works. The theory accurately predicts the recovery rate in numerical experiments. Our proof relies on a analyzing a certain Bregman divergence of the flow. This bypasses the obstacles caused by non-convexity and should be of independent interest.},
  keywords = {Researcher App}
}

@article{morenoCanPersonalityTraits2021,
  title = {Can Personality Traits Be Measured Analyzing Written Language? {{A}} Meta-Analytic Study on Computational Methods},
  shorttitle = {Can Personality Traits Be Measured Analyzing Written Language?},
  author = {Moreno, Jos{\'e} David and {Mart{\'i}nez-Huertas}, Jos{\'e} {\'A}. and Olmos, Ricardo and {Jorge-Botana}, Guillermo and Botella, Juan},
  year = {2021},
  month = jul,
  journal = {Personality and Individual Differences},
  volume = {177},
  pages = {110818},
  issn = {0191-8869},
  doi = {10.1016/j.paid.2021.110818},
  urldate = {2023-07-07},
  abstract = {In the last two decades, empirical evidence has shown that personality traits could be related to the characteristics of written language. This study describes a meta-analysis that synthesizes 23 independent estimates of the correlations between the Big Five major personality traits, and some computationally obtained indicators from written language. The results show significant combined estimates of the correlations, albeit small to moderate according to Cohen's conventions to interpret effect sizes, for the five traits (between r~=~0.26 for agreeableness and neuroticism, and 0.30 for openness). These estimates are moderated by the type of information in the texts, the use of prediction mechanisms, and the source of publication of the primary studies. Generally, the same effective moderators operate for the five traits. It is concluded that written language analyzed through computational methods could be used to extract relevant information of personality. But further research is still needed to consider it as predictive or explanatory tool for individual differences.},
  langid = {english},
  keywords = {Big 5,Big five,Computational models of language,Language,Meta-analysis,Personality,Psycholinguistics},
  file = {/home/luisaam/Zotero/storage/9942RJ74/Moreno et al. - 2021 - Can personality traits be measured analyzing writt.pdf;/home/luisaam/Zotero/storage/M7MBPLIM/S0191886921001938.html}
}

@inproceedings{morseSimpleEvolutionaryOptimization2016,
  title = {Simple {{Evolutionary Optimization Can Rival Stochastic Gradient Descent}} in {{Neural Networks}}},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}} 2016},
  author = {Morse, Gregory and Stanley, Kenneth O.},
  year = {2016},
  month = jul,
  series = {{{GECCO}} '16},
  pages = {477--484},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/2908812.2908916},
  urldate = {2021-07-03},
  abstract = {While evolutionary algorithms (EAs) have long offered an alternative approach to optimization, in recent years backpropagation through stochastic gradient descent (SGD) has come to dominate the fields of neural network optimization and deep learning. One hypothesis for the absence of EAs in deep learning is that modern neural networks have become so high dimensional that evolution with its inexact gradient cannot match the exact gradient calculations of backpropagation. Furthermore, the evaluation of a single individual in evolution on the big data sets now prevalent in deep learning would present a prohibitive obstacle towards efficient optimization. This paper challenges these views, suggesting that EAs can be made to run significantly faster than previously thought by evaluating individuals only on a small number of training examples per generation. Surprisingly, using this approach with only a simple EA (called the limited evaluation EA or LEEA) is competitive with the performance of the state-of-the-art SGD variant RMSProp on several benchmarks with neural networks with over 1,000 weights. More investigation is warranted, but these initial results suggest the possibility that EAs could be the first viable training alternative for deep learning outside of SGD, thereby opening up deep learning to all the tools of evolutionary computation.},
  isbn = {978-1-4503-4206-3},
  keywords = {artificial intelligence,deep learning,machine learning,neural networks,pattern recognition and classification,Why ES in neural networks},
  file = {/home/luisaam/Zotero/storage/SX8D88HG/Morse y Stanley - 2016 - Simple Evolutionary Optimization Can Rival Stochas.pdf}
}

@inproceedings{morseSimpleEvolutionaryOptimization2016a,
  title = {Simple {{Evolutionary Optimization Can Rival Stochastic Gradient Descent}} in {{Neural Networks}}},
  booktitle = {Proceedings of the {{Genetic}} and {{Evolutionary Computation Conference}} 2016},
  author = {Morse, Gregory and Stanley, Kenneth O.},
  year = {2016},
  month = jul,
  pages = {477--484},
  publisher = {{ACM}},
  address = {{Denver Colorado USA}},
  doi = {10.1145/2908812.2908916},
  urldate = {2021-05-03},
  abstract = {While evolutionary algorithms (EAs) have long offered an alternative approach to optimization, in recent years backpropagation through stochastic gradient descent (SGD) has come to dominate the fields of neural network optimization and deep learning. One hypothesis for the absence of EAs in deep learning is that modern neural networks have become so high dimensional that evolution with its inexact gradient cannot match the exact gradient calculations of backpropagation. Furthermore, the evaluation of a single individual in evolution on the big data sets now prevalent in deep learning would present a prohibitive obstacle towards efficient optimization. This paper challenges these views, suggesting that EAs can be made to run significantly faster than previously thought by evaluating individuals only on a small number of training examples per generation. Surprisingly, using this approach with only a simple EA (called the limited evaluation EA or LEEA) is competitive with the performance of the state-of-the-art SGD variant RMSProp on several benchmarks with neural networks with over 1,000 weights. More investigation is warranted, but these initial results suggest the possibility that EAs could be the first viable training alternative for deep learning outside of SGD, thereby opening up deep learning to all the tools of evolutionary computation.},
  isbn = {978-1-4503-4206-3},
  langid = {english},
  file = {/home/luisaam/Zotero/storage/KKSHVY4A/Morse y Stanley - 2016 - Simple Evolutionary Optimization Can Rival Stochas.pdf}
}

@article{mostafaParameterEfficientTraining2019,
  title = {Parameter {{Efficient Training}} of {{Deep Convolutional Neural Networks}} by {{Dynamic Sparse Reparameterization}}},
  author = {Mostafa, Hesham and Wang, Xin},
  year = {2019},
  month = may,
  journal = {arXiv:1902.05967 [cs, stat]},
  eprint = {1902.05967},
  primaryclass = {cs, stat},
  urldate = {2022-03-08},
  abstract = {Modern deep neural networks are typically highly overparameterized. Pruning techniques are able to remove a significant fraction of network parameters with little loss in accuracy. Recently, techniques based on dynamic reallocation of non-zero parameters have emerged, allowing direct training of sparse networks without having to pre-train a large dense model. Here we present a novel dynamic sparse reparameterization method that addresses the limitations of previous techniques such as high computational cost and the need for manual configuration of the number of free parameters allocated to each layer. We evaluate the performance of dynamic reallocation methods in training deep convolutional networks and show that our method outperforms previous static and dynamic reparameterization methods, yielding the best accuracy for a fixed parameter budget, on par with accuracies obtained by iteratively pruning a pre-trained dense model. We further investigated the mechanisms underlying the superior generalization performance of the resultant sparse networks. We found that neither the structure, nor the initialization of the non-zero parameters were sufficient to explain the superior performance. Rather, effective learning crucially depended on the continuous exploration of the sparse network structure space during training. Our work suggests that exploring structural degrees of freedom during training is more effective than adding extra parameters to the network.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,dynamical sparse training,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/REVLA3TW/Mostafa y Wang - 2019 - Parameter Efficient Training of Deep Convolutional.pdf;/home/luisaam/Zotero/storage/9VBAEPDS/1902.html}
}

@inproceedings{mozerSkeletonizationTechniqueTrimming1988,
  title = {Skeletonization: {{A Technique}} for {{Trimming}} the {{Fat}} from a {{Network}} via {{Relevance Assessment}}},
  shorttitle = {Skeletonization},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Mozer, Michael C and Smolensky, Paul},
  year = {1988},
  volume = {1},
  publisher = {{Morgan-Kaufmann}},
  urldate = {2022-12-17},
  abstract = {This paper proposes a means of using the knowledge in a network to  determine the functionality or relevance of individual units, both for  the purpose of understanding the network's behavior and improving its  performance. The basic idea is to iteratively train the network to a cer(cid:173) tain performance criterion, compute a measure of relevance that identi(cid:173) fies which input or hidden units are most critical to performance, and  automatically trim the least relevant units. This skeletonization tech(cid:173) nique can be used to simplify networks by eliminating units that con(cid:173) vey redundant information; to improve learning performance by first  learning with spare hidden units and then trimming the unnecessary  ones away, thereby constraining generalization; and to understand the  behavior of networks in terms of minimal "rules."},
  keywords = {Pruning},
  file = {/home/luisaam/Zotero/storage/Y9I6V9AU/Mozer y Smolensky - 1988 - Skeletonization A Technique for Trimming the Fat .pdf}
}

@article{mummadiGroupPruningUsing,
  title = {Group {{Pruning}} Using a {{Bounded-Lp}} Norm for {{Group Gating}} and {{Regularization}}. ({{arXiv}}:1908.03463v1 [Stat.{{ML}}])},
  author = {Mummadi, Chaithanya Kumar and Genewein, Tim and Zhang, Dan and Brox, Thomas and Fischer, Volker},
  journal = {arXiv Computer Science},
  doi = {arXiv:1908.03463v1},
  abstract = {Deep neural networks achieve state-of-the-art results on several tasks while increasing in complexity. It has been shown that neural networks can be pruned during training by imposing sparsity inducing regularizers. In this paper, we investigate two techniques for group-wise pruning during training in order to improve network efficiency. We propose a gating factor after every convolutional layer to induce channel level sparsity, encouraging insignificant channels to become exactly zero. Further, we introduce and analyse a bounded variant of the L1 regularizer, which interpolates between L1 and L0-norms to retain performance of the network at higher pruning rates. To underline effectiveness of the proposed methods,we show that the number of parameters of ResNet-164, DenseNet-40 and MobileNetV2 can be reduced down by 30\%, 69\% and 75\% on CIFAR100 respectively without a significant drop in accuracy. We achieve state-of-the-art pruning results for ResNet-50 with higher accuracy on ImageNet. Furthermore, we show that the light weight MobileNetV2 can further be compressed on ImageNet without a significant drop in performance.},
  keywords = {Researcher App}
}

@book{nabiomidvarCooperativeCoevolutionDelta,
  title = {Cooperative {{Co-evolution}} with {{Delta Grouping}} for {{Large Scale Non-separable Function Optimization}}},
  author = {Nabi Omidvar, Mohammad and Li, Xiaodong and Member, Senior and Yao, Xin},
  abstract = {Many evolutionary algorithms have been proposed for large scale optimization. Parameter interaction in non-separable problems is a major source of performance loss specially on large scale problems. Cooperative Co-evolution(CC) has been proposed as a natural solution for large scale optimization problems, but lack of a systematic way of decomposing large scale non-separable problems is a major obstacle for CC frameworks. The aim of this paper is to propose a systematic way of capturing interacting variables for a more effective problem decomposition suitable for cooperative co-evolutionary frameworks. Grouping interacting variables in different subcomponents in a CC framework imposes a limit to the extent interacting variables can be optimized to their optimum values, in other words it limits the improvement interval of interacting variables. This is the central idea of the newly proposed technique which is called delta method. Delta method measures the averaged difference in a certain variable across the entire population and uses it for identifying interacting variables. The experimental results show that this new technique is more effective than the existing random grouping method.},
  isbn = {978-1-4244-8126-2},
  file = {/home/luisaam/Zotero/storage/YNFM43U4/Nabi Omidvar et al. - Unknown - Cooperative Co-evolution with Delta Grouping for Large Scale Non-separable Function Optimization.pdf}
}

@article{nesterovMethodSolvingConvex1983,
  title = {A Method for Solving the Convex Programming Problem with Convergence Rate {{O}}(1/K\^2)},
  author = {Nesterov, Y.},
  year = {1983},
  journal = {Proceedings of the USSR Academy of Sciences},
  volume = {269},
  pages = {543--547}
}

@article{NeuralNetworkClass,
  title = {Neural Network for a Class of Sparse Optimization with\,{$<$}math{$>$}{{L0}}{$<$}/Math{$>$}-Regularization},
  journal = {Neural Networks},
  doi = {8.1621.ab008f57-cac2-4a7c-8dac-0aab14b5a3e3.1649320179},
  abstract = {Sparse optimization involving the\,  L0  -norm function as the regularization in objective function has a wide application in many fields. In this paper, we propose a projected neural network modeled by a differential equation to solve a class of these optimization problems, in which the objective function is the sum of a nonsmooth convex loss function and the regularization defined by the\,  L0  -norm function. This optimization problem is not only nonconvex, but also discontinuous. To simplify the structure of the proposed network and let it own better convergence properties, we use the smoothing method, where the new constructed smoothing function for the regularization term plays a key role. We prove that the solution to the proposed network is globally existent and unique, and any accumulation point of it is a critical point of the continuous relaxation model. Except for a special case, which can be easily justified, any critical point is a local minimizer of the considered sparse optimization problem. It is an interesting thing that all critical points own a promising lower bound property, which is satisfied by all global minimizers of the considered problem, but is not by all local minimizers. Finally, we use some numerical experiments to illustrate the efficiency and good performance of the proposed method for solving this class of sparse optimization problems, which include the most widely used models in feature selection of classification learning.},
  keywords = {Researcher App}
}

@article{NeuralNetworkCompression,
  title = {Neural {{Network Compression}} by {{Joint Sparsity Promotion}} and {{Redundancy Reduction}}. ({{arXiv}}:2210.07451v1 [Cs.{{CV}}])},
  journal = {arXiv Computer Science},
  doi = {arXiv:2210.07451v1},
  abstract = {Compression of convolutional neural network models has recently been dominated by pruning approaches. A class of previous works focuses solely on pruning the unimportant filters to achieve network compression. Another important direction is the design of sparsity-inducing constraints which has also been explored in isolation. This paper presents a novel training scheme based on composite constraints that prune redundant filters and minimize their effect on overall network learning via sparsity promotion. Also, as opposed to prior works that employ pseudo-norm-based sparsity-inducing constraints, we propose a sparse scheme based on gradient counting in our framework. Our tests on several pixel-wise segmentation benchmarks show that the number of neurons and the memory footprint of networks in the test phase are significantly reduced without affecting performance. MobileNetV3 and UNet, two well-known architectures, are used to test the proposed scheme. Our network compression method not only results in reduced parameters but also achieves improved performance compared to MobileNetv3, which is an already optimized architecture.},
  keywords = {NN compression,Researcher App}
}

@article{NewNetworkPruning,
  title = {A {{New Network Pruning Framework Based}} on {{Rewind}}},
  journal = {International Journal of Pattern Recognition and Artificial Intelligence},
  doi = {10.1142/S0218001422590285},
  abstract = {Model pruning is one of the main methods of deep neural network model compression. However, the existing model pruning methods are inefficient, there is still a lot of redundancy in the network, and the pruning has a great impact on the accuracy. In addition, the traditional pruning process usually needs to fine-tune the network to restore accuracy, and fine-tuning has a limited effect on accuracy recovery, so it is difficult to achieve a high level of accuracy. In this paper, we propose a new neural network pruning framework: the channel sparsity is realized by introducing a scale factor, and the sparse network is pruned by setting a global threshold, which greatly improves the efficiency of pruning. After pruning, this paper proposes a rewind method to restore the accuracy, that is, save the weight after training, and then reload it on the network for training to restore the accuracy. In addition, we also study the best rewind point of the three networks. The experimental results show that our method significantly reduces the number of parameters and FLOPs without affecting or even improving the accuracy, and the rewind method proposed by us achieves a better accuracy recovery effect than fine-tuning. At the same time, we find that the epoch with the highest accuracy is the best rewind point, and the accuracy is the highest after saving its corresponding weight and retraining the model.},
  keywords = {Pruning,Researcher App}
}

@inproceedings{neyshaburPathSGDPathNormalizedOptimization2015,
  title = {Path-{{SGD}}: {{Path-Normalized Optimization}} in {{Deep Neural Networks}}},
  shorttitle = {Path-{{SGD}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Neyshabur, Behnam and Salakhutdinov, Russ R and Srebro, Nati},
  year = {2015},
  volume = {28},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-12-16},
  abstract = {We revisit the choice of SGD for training deep neural networks by reconsidering the appropriate geometry in which to optimize the weights.  We argue for a geometry invariant to rescaling of weights that does not affect the output of the network, and suggest Path-SGD, which is an approximate steepest descent method with respect to a path-wise regularizer related to max-norm regularization.  Path-SGD is easy and efficient to implement and leads to empirical gains over SGD and AdaGrad.},
  keywords = {neural network optimization},
  file = {/home/luisaam/Zotero/storage/T73VMRZH/Neyshabur et al. - 2015 - Path-SGD Path-Normalized Optimization in Deep Neu.pdf}
}

@inproceedings{neyshaburRoleOverparametrizationGeneralization2018,
  title = {The Role of Over-Parametrization in Generalization of Neural Networks},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Neyshabur, Behnam and Li, Zhiyuan and Bhojanapalli, Srinadh and LeCun, Yann and Srebro, Nathan},
  year = {2018},
  month = sep,
  urldate = {2021-08-26},
  abstract = {We suggest a generalization bound that could partly explain the improvement in generalization with over-parametrization.},
  langid = {english},
  keywords = {over-parametrized neural networks},
  file = {/home/luisaam/Zotero/storage/UFD72BME/Neyshabur et al. - 2018 - The role of over-parametrization in generalization.pdf;/home/luisaam/Zotero/storage/WVKWKZXZ/forum.html}
}

@article{nguyenLossLandscapeClass2018,
  title = {On the Loss Landscape of a Class of Deep Neural Networks with No Bad Local Valleys},
  author = {Nguyen, Quynh and Mukkamala, Mahesh Chandra and Hein, Matthias},
  year = {2018},
  month = dec,
  journal = {arXiv:1809.10749 [cs, stat]},
  eprint = {1809.10749},
  primaryclass = {cs, stat},
  urldate = {2021-06-28},
  abstract = {We identify a class of over-parameterized deep neural networks with standard activation functions and cross-entropy loss which provably have no bad local valley, in the sense that from any point in parameter space there exists a continuous path on which the cross-entropy loss is non-increasing and gets arbitrarily close to zero. This implies that these networks have no sub-optimal strict local minima.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning,Why no global second order},
  file = {/home/luisaam/Zotero/storage/9LDG86UX/Nguyen et al. - 2018 - On the loss landscape of a class of deep neural ne.pdf;/home/luisaam/Zotero/storage/BLFXSLWA/1809.html}
}

@incollection{nocedalTrustRegionMethods2006,
  title = {Trust-{{Region Methods}}},
  booktitle = {Numerical {{Optimization}}},
  editor = {Nocedal, Jorge and Wright, Stephen J.},
  year = {2006},
  series = {Springer {{Series}} in {{Operations Research}} and {{Financial Engineering}}},
  pages = {66--100},
  publisher = {{Springer}},
  address = {{New York, NY}},
  doi = {10.1007/978-0-387-40065-5_4},
  urldate = {2021-08-21},
  isbn = {978-0-387-40065-5},
  langid = {english},
  keywords = {examples,Global Convergence,Newton Step,Superlinear Convergence,Trust Region,Unconstrained Minimizer}
}

@book{nocedealjorgeNumericalOptimization2006,
  title = {Numerical {{Optimization}}},
  author = {Nocedeal,Jorge and Wright,Stephen},
  year = {2006},
  series = {Operations {{Research}}},
  edition = {2},
  publisher = {{Springer}},
  address = {{USA}},
  isbn = {0-387-30303-0},
  langid = {english},
  file = {/home/luisaam/Zotero/storage/BJW7SM93/_.pdf}
}

@article{NonsmoothImplicitDifferentiation,
  title = {Nonsmooth {{Implicit Differentiation}} for {{Machine Learning}} and {{Optimization}}. ({{arXiv}}:2106.04350v2 [Cs.{{LG}}] {{CROSS LISTED}})},
  journal = {arXiv Machine Learning (Computer Science)},
  doi = {arXiv:2106.04350v2},
  abstract = {In view of training increasingly complex learning architectures, we establish a nonsmooth implicit function theorem with an operational calculus. Our result applies to most practical problems (i.e., definable problems) provided that a nonsmooth form of the classical invertibility condition is fulfilled. This approach allows for formal subdifferentiation: for instance, replacing derivatives by Clarke Jacobians in the usual differentiation formulas is fully justified for a wide class of nonsmooth problems. Moreover this calculus is entirely compatible with algorithmic differentiation (e.g., backpropagation). We provide several applications such as training deep equilibrium networks, training neural nets with conic optimization layers, or hyperparameter-tuning for nonsmooth Lasso-type models. To show the sharpness of our assumptions, we present numerical experiments showcasing the extremely pathological gradient dynamics one can encounter when applying implicit algorithmic differentiation without any hypothesis.},
  keywords = {Researcher App}
}

@article{NovelMethodFinancial,
  title = {A Novel Method for Financial Distress Prediction Based on Sparse Neural Networks with \$\${{L}}\_\{1/2\}\$\$ {{L}} 1 / 2 Regularization},
  journal = {International Journal of Machine Learning and Cybernetics},
  doi = {10.1007/s13042-022-01566-y},
  abstract = {Corporate financial distress is related to the interests of the enterprise and stakeholders. Therefore, its accurate prediction is of great significance to avoid huge losses from them. Despite significant effort and progress in this field, the existing prediction methods are either limited by the number of input variables or restricted to those financial predictors. To alleviate those issues, both financial variables and non-financial variables are screened out from the existing accounting and finance theory to use as financial distress predictors. In addition, a novel method for financial distress prediction (FDP) based on sparse neural networks is proposed, namely FDP-SNN, in which the weight of the hidden layer is constrained with   {\textbackslash}(L\_\{1/2\}{\textbackslash})   regularization to achieve the sparsity, so as to select relevant and important predictors, improving the predicted accuracy. It also provides support for the interpretability of the model. The results show that non-financial variables, such as investor protection and governance structure, play a key role in financial distress prediction than those financial ones, especially when the forecast period grows longer. By comparing those classic models proposed by predominant researchers in accounting and finance, the proposed model outperforms in terms of accuracy, precision, and AUC performance.},
  keywords = {Researcher App}
}

@article{NoveltyDetectionUsing,
  title = {Novelty {{Detection Using Sparse Auto-Encoders}} to {{Characterize Structural Vibration Responses}}},
  journal = {Arabian Journal for Science and Engineering},
  doi = {10.1007/s13369-022-06732-6},
  abstract = {Deep learning techniques have been increasingly popular for detecting structural novelties in recent years. The deep learning notion originates from the theory of neural networks, and it comprises several machine learning approaches that were primarily created to solve high-dimensional and nonlinear problems due to their great data mapping capabilities. Although the basic ideas of such algorithms were established in the 1960s, their use in damage detection situations is still relatively new. In so doing, the current study assesses the Sparse Auto-Encoder (SAE) deep learning method when applied to the characterization of structural anomalies. The fundamental concept is to employ the SAE to extract significant features from monitored signals and the well-known Support Vector Machine (SVM) to classify those features within the framework of a Structural Health Monitoring (SHM) program. The proposed method is evaluated using vibration data from a numerical beam model and a highway viaduct in Brazil. The results demonstrate that the SAE can extract relevant properties from dynamic data, making it valuable for SHM applications.},
  keywords = {Researcher App}
}

@article{OmniXAILibraryExplainable,
  title = {{{OmniXAI}}: {{A Library}} for {{Explainable AI}}. ({{arXiv}}:2206.01612v3 [Cs.{{LG}}] {{UPDATED}})},
  journal = {arXiv Computer Vision and Pattern Recognition},
  doi = {arXiv:2206.01612v3},
  abstract = {We introduce OmniXAI (short for Omni eXplainable AI), an open-source Python library of eXplainable AI (XAI), which offers omni-way explainable AI capabilities and various interpretable machine learning techniques to address the pain points of understanding and interpreting the decisions made by machine learning (ML) in practice. OmniXAI aims to be a one-stop comprehensive library that makes explainable AI easy for data scientists, ML researchers and practitioners who need explanation for various types of data, models and explanation methods at different stages of ML process (data exploration, feature engineering, model development, evaluation, and decision-making, etc). In particular, our library includes a rich family of explanation methods integrated in a unified interface, which supports multiple data types (tabular data, images, texts, time-series), multiple types of ML models (traditional ML in Scikit-learn and deep learning models in PyTorch/TensorFlow), and a range of diverse explanation methods including "model-specific" and "model-agnostic" ones (such as feature-attribution explanation, counterfactual explanation, gradient-based explanation, etc). For practitioners, the library provides an easy-to-use unified interface to generate the explanations for their applications by only writing a few lines of codes, and also a GUI dashboard for visualization of different explanations for more insights about decisions. In this technical report, we present OmniXAI's design principles, system architectures, and major functionalities, and also demonstrate several example use cases across different types of data, tasks, and models.},
  keywords = {Researcher App}
}

@article{OnDeviceTraining256KB,
  title = {On-{{Device Training Under 256KB Memory}}. ({{arXiv}}:2206.15472v2 [Cs.{{CV}}] {{UPDATED}})},
  journal = {arXiv Computer Science},
  doi = {arXiv:2206.15472v2},
  abstract = {On-device training enables the model to adapt to new data collected from the sensors by fine-tuning a pre-trained model. However, the training memory consumption is prohibitive for IoT devices that have tiny memory resources. We propose an algorithm-system co-design framework to make on-device training possible with only 256KB of memory. On-device training faces two unique challenges: (1) the quantized graphs of neural networks are hard to optimize due to mixed bit-precision and the lack of normalization; (2) the limited hardware resource (memory and computation) does not allow full backward computation. To cope with the optimization difficulty, we propose Quantization-Aware Scaling to calibrate the gradient scales and stabilize quantized training. To reduce the memory footprint, we propose Sparse Update to skip the gradient computation of less important layers and sub-tensors. The algorithm innovation is implemented by a lightweight training system, Tiny Training Engine, which prunes the backward computation graph to support sparse updates and offloads the runtime auto-differentiation to compile time. Our framework is the first practical solution for on-device transfer learning of visual recognition on tiny IoT devices (e.g., a microcontroller with only 256KB SRAM), using less than 1/100 of the memory of existing frameworks while matching the accuracy of cloud training+edge deployment for the tinyML application VWW. Our study enables IoT devices to not only perform inference but also continuously adapt to new data for on-device lifelong learning.},
  keywords = {Researcher App,TinyML}
}

@article{oordWaveNetGenerativeModel2016,
  title = {{{WaveNet}}: {{A Generative Model}} for {{Raw Audio}}},
  shorttitle = {{{WaveNet}}},
  author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  year = {2016},
  month = sep,
  journal = {arXiv:1609.03499 [cs]},
  eprint = {1609.03499},
  primaryclass = {cs},
  urldate = {2021-07-01},
  abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,examples},
  file = {/home/luisaam/Zotero/storage/NZAPJXX7/Oord et al. - 2016 - WaveNet A Generative Model for Raw Audio.pdf;/home/luisaam/Zotero/storage/965GVEX2/1609.html}
}

@inproceedings{oordWaveNetGenerativeModel2016a,
  title = {{{WaveNet}}: {{A Generative Model}} for {{Raw Audio}}},
  shorttitle = {{{WaveNet}}},
  booktitle = {Arxiv},
  author = {van den Oord, A{\"a}ron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alexander and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  year = {2016},
  urldate = {2022-11-07}
}

@book{oppenheimOppenheimSegnalesSistemas1998,
  title = {Oppenheim {{Segnales}} y {{Sistemas}}.Pdf},
  author = {Oppenheim, A. V. and Willsky, A.S. and Nawab, S. H.},
  year = {1998},
  file = {/home/luisaam/Zotero/storage/2DVWLFFV/Oppenheim, Willsky, Nawab - 1998 - Oppenheim Segnales y Sistemas.pdf(2).pdf;/home/luisaam/Zotero/storage/TA4YFEIN/Oppenheim, Willsky, Nawab - 1998 - Oppenheim Segnales y Sistemas.pdf.pdf}
}

@misc{OptimizingNeuralNetworks,
  title = {Optimizing Neural Networks with {{Kronecker-factored}} Approximate Curvature | {{Proceedings}} of the 32nd {{International Conference}} on {{International Conference}} on {{Machine Learning}} - {{Volume}} 37},
  urldate = {2021-08-21},
  howpublished = {https://dl.acm.org/doi/10.5555/3045118.3045374}
}

@article{orhanSkipConnectionsEliminate2018,
  title = {Skip {{Connections Eliminate Singularities}}},
  author = {Orhan, A. Emin and Pitkow, Xaq},
  year = {2018},
  month = mar,
  journal = {arXiv:1701.09175 [cs]},
  eprint = {1701.09175},
  primaryclass = {cs},
  urldate = {2021-07-22},
  abstract = {Skip connections made the training of very deep networks possible and have become an indispensable component in a variety of neural architectures. A completely satisfactory explanation for their success remains elusive. Here, we present a novel explanation for the benefits of skip connections in training very deep networks. The difficulty of training deep networks is partly due to the singularities caused by the non-identifiability of the model. Several such singularities have been identified in previous works: (i) overlap singularities caused by the permutation symmetry of nodes in a given layer, (ii) elimination singularities corresponding to the elimination, i.e. consistent deactivation, of nodes, (iii) singularities generated by the linear dependence of the nodes. These singularities cause degenerate manifolds in the loss landscape that slow down learning. We argue that skip connections eliminate these singularities by breaking the permutation symmetry of nodes, by reducing the possibility of node elimination and by making the nodes less linearly dependent. Moreover, for typical initializations, skip connections move the network away from the "ghosts" of these singularities and sculpt the landscape around them to alleviate the learning slow-down. These hypotheses are supported by evidence from simplified models, as well as from experiments with deep networks trained on real-world datasets.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/luisaam/Zotero/storage/LPWQDTHM/Orhan y Pitkow - 2018 - Skip Connections Eliminate Singularities.pdf;/home/luisaam/Zotero/storage/W7EJ7UUD/1701.html}
}

@inproceedings{orhanSkipConnectionsEliminate2018a,
  title = {Skip {{Connections Eliminate Singularities}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Orhan, Emin and Pitkow, Xaq},
  year = {2018},
  month = feb,
  urldate = {2021-07-12},
  abstract = {Degenerate manifolds arising from the non-identifiability of the model slow down learning in deep networks; skip connections help by breaking degeneracies.},
  langid = {english},
  keywords = {skip connections},
  file = {/home/luisaam/Zotero/storage/EW9Y8R8W/Orhan y Pitkow - 2018 - Skip Connections Eliminate Singularities.pdf;/home/luisaam/Zotero/storage/CYCLQVGR/forum.html}
}

@article{OrigamiDimensionsHow,
  title = {Origami in {{N}} Dimensions: {{How}} Feed-Forward Networks Manufacture Linear Separability. ({{arXiv}}:2203.11355v1 [Cs.{{LG}}])},
  journal = {arXiv Disordered Systems and Neural Networks},
  doi = {arXiv:2203.11355v1},
  abstract = {Neural networks can implement arbitrary functions. But, mechanistically, what are the tools at their disposal to construct the target? For classification tasks, the network must transform the data classes into a linearly separable representation in the final hidden layer. We show that a feed-forward architecture has one primary tool at hand to achieve this separability: progressive folding of the data manifold in unoccupied higher dimensions. The operation of folding provides a useful intuition in low-dimensions that generalizes to high ones. We argue that an alternative method based on shear, requiring very deep architectures, plays only a small role in real-world networks. The folding operation, however, is powerful as long as layers are wider than the data dimensionality, allowing efficient solutions by providing access to arbitrary regions in the distribution, such as data points of one class forming islands within the other classes. We argue that a link exists between the universal approximation property in ReLU networks and the fold-and-cut theorem (Demaine et al., 1998) dealing with physical paper folding. Based on the mechanistic insight, we predict that the progressive generation of separability is necessarily accompanied by neurons showing mixed selectivity and bimodal tuning curves. This is validated in a network trained on the poker hand task, showing the emergence of bimodal tuning curves during training. We hope that our intuitive picture of the data transformation in deep networks can help to provide interpretability, and discuss possible applications to the theory of convolutional networks, loss landscapes, and generalization.     TL;DR: Shows that the internal processing of deep networks can be thought of as literal folding operations on the data distribution in the N-dimensional activation space. A link to a well-known theorem in origami theory is provided.},
  keywords = {Researcher App}
}

@inproceedings{osawaLargeScaleDistributedSecondOrder2019,
  title = {Large-{{Scale Distributed Second-Order Optimization Using Kronecker-Factored Approximate Curvature}} for {{Deep Convolutional Neural Networks}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Osawa, Kazuki and Tsuji, Yohei and Ueno, Yuichiro and Naruse, Akira and Yokota, Rio and Matsuoka, Satoshi},
  year = {2019},
  month = jun,
  pages = {12351--12359},
  issn = {2575-7075},
  doi = {10.1109/CVPR.2019.01264},
  abstract = {Large-scale distributed training of deep neural networks suffers from the generalization gap caused by the increase in the effective mini-batch size. Previous approaches try to solve this problem by varying the learning rate and batch size over epochs and layers, or some ad hoc modification of the batch normalization. We propose an alternative approach using a second order optimization method that shows similar generalization capability to first order methods, but converges faster and can handle larger mini-batches. To test our method on a benchmark where highly optimized first order methods are available as references, we train ResNet-50 on ImageNet-1K. We converged to 75\% Top-1 validation accuracy in 35 epochs for mini-batch sizes under 16,384, and achieved 75\% even with a mini-batch size of 131,072, which took only 978 iterations.},
  keywords = {Big Data,Deep Learning,Large Scale Methods,Optimization Methods},
  file = {/home/luisaam/Zotero/storage/JM3VXVHD/Osawa et al. - 2019 - Large-Scale Distributed Second-Order Optimization .pdf;/home/luisaam/Zotero/storage/KJ4UI9KT/Osawa et al. - 2019 - Large-Scale Distributed Second-Order Optimization .pdf;/home/luisaam/Zotero/storage/7EME8A5S/8953771.html}
}

@inproceedings{osawaLargeScaleDistributedSecondOrder2019a,
  title = {Large-{{Scale Distributed Second-Order Optimization Using Kronecker-Factored Approximate Curvature}} for {{Deep Convolutional Neural Networks}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Osawa, Kazuki and Tsuji, Yohei and Ueno, Yuichiro and Naruse, Akira and Yokota, Rio and Matsuoka, Satoshi},
  year = {2019},
  month = jun,
  pages = {12351--12359},
  issn = {2575-7075},
  doi = {10.1109/CVPR.2019.01264},
  abstract = {Large-scale distributed training of deep neural networks suffers from the generalization gap caused by the increase in the effective mini-batch size. Previous approaches try to solve this problem by varying the learning rate and batch size over epochs and layers, or some ad hoc modification of the batch normalization. We propose an alternative approach using a second order optimization method that shows similar generalization capability to first order methods, but converges faster and can handle larger mini-batches. To test our method on a benchmark where highly optimized first order methods are available as references, we train ResNet-50 on ImageNet-1K. We converged to 75\% Top-1 validation accuracy in 35 epochs for mini-batch sizes under 16,384, and achieved 75\% even with a mini-batch size of 131,072, which took only 978 iterations.},
  keywords = {Big Data,Deep Learning,Large Scale Methods,Optimization Methods,second order},
  file = {/home/luisaam/Zotero/storage/3J3NRNPA/Osawa et al. - 2019 - Large-Scale Distributed Second-Order Optimization .pdf;/home/luisaam/Zotero/storage/CSV73CAZ/8953771.html}
}

@article{PACBayesianLifelongLearning,
  title = {{{PAC-Bayesian Lifelong Learning For Multi-Armed Bandits}}. ({{arXiv}}:2203.03303v1 [Cs.{{LG}}] {{CROSS LISTED}})},
  journal = {arXiv Machine Learning (Statistics)},
  doi = {arXiv:2203.03303v1},
  abstract = {We present a PAC-Bayesian analysis of lifelong learning. In the lifelong learning problem, a sequence of learning tasks is observed one-at-a-time, and the goal is to transfer information acquired from previous tasks to new learning tasks. We consider the case when each learning task is a multi-armed bandit problem. We derive lower bounds on the expected average reward that would be obtained if a given multi-armed bandit algorithm was run in a new task with a particular prior and for a set number of steps. We propose lifelong learning algorithms that use our new bounds as learning objectives. Our proposed algorithms are evaluated in several lifelong multi-armed bandit problems and are found to perform better than a baseline method that does not use generalisation bounds.},
  keywords = {Researcher App}
}

@article{PACBayesianLifelongLearninga,
  title = {{{PAC-Bayesian}} Lifelong Learning for Multi-Armed Bandits},
  journal = {Data Mining and Knowledge Discovery},
  doi = {10.1007/s10618-022-00825-4},
  abstract = {We present a PAC-Bayesian analysis of lifelong learning. In the lifelong learning problem, a sequence of learning tasks is observed one-at-a-time, and the goal is to transfer information acquired from previous tasks to new learning tasks. We consider the case when each learning task is a multi-armed bandit problem. We derive lower bounds on the expected average reward that would be obtained if a given multi-armed bandit algorithm was run in a new task with a particular prior and for a set number of steps. We propose lifelong learning algorithms that use our new bounds as learning objectives. Our proposed algorithms are evaluated in several lifelong multi-armed bandit problems and are found to perform better than a baseline method that does not use generalisation bounds.},
  keywords = {Researcher App}
}

@article{panditLearningSparseNeural2022,
  title = {Learning {{Sparse Neural Networks Using Non-Convex Regularization}}},
  author = {Pandit, Mohammad Khalid and Naaz, Roohie and Chishti, Mohammad Ahsan},
  year = {2022},
  month = apr,
  journal = {IEEE Transactions on Emerging Topics in Computational Intelligence},
  volume = {6},
  number = {2},
  pages = {287--299},
  issn = {2471-285X},
  doi = {10.1109/TETCI.2021.3058672},
  abstract = {Deep Neural Networks (DNNs) is the computing paradigm that has achieved remarkable success in various fields of engineering in recent years, primarily visual recognition. DNNs owe its success to the presence of large number of weight parameters (and increased depth), which led to huge computation and memory costs for implementation. These limiting factors hinder the scalability of such algorithms on resource constraint devices (like IoT devices). In general, DNNs are believed to be over parametrized i.e., the parameters are highly redundant, thus can be structurally removed without significant loss of performance. To solve these issues, we propose to use non-convex T\textsubscript{1} regularizer along with the additional effect of sparse group lasso to completely remove the redundant neurons/filters that is, to introduce structured sparsity. The network has been trained using the proximal gradient method, which is useful in optimizing functions with the combination of smooth and non-smooth terms. We show that proposed regularizer manages to achieve competitive performances as well as extremely compact networks. Detailed experiments are performed on several benchmark datasets that illustrate the efficiency of the approach. On the ImageNet dataset, our approach removes more than 50\% of parameters of convolutional layers and 85\% parameters of fully connected layers of Alexnet with no drop in accuracy.},
  keywords = {Benchmark testing,Biological neural networks,Deep neural networks,Feature extraction,gradients training,Internet of Things,Jamming,Memory management,Neurons,regularization,Sobolev training,sparse group lasso},
  file = {/home/luisaam/Zotero/storage/FY3J9CL9/Pandit et al. - 2022 - Learning Sparse Neural Networks Using Non-Convex R.pdf;/home/luisaam/Zotero/storage/BK6TB3XW/9372948.html}
}

@misc{PapersCodePerformance,
  title = {Papers with {{Code}} - {{Performance}} Is Not Enough: A Story of the {{Rashomon}}'s Quartet},
  shorttitle = {Papers with {{Code}} - {{Performance}} Is Not Enough},
  urldate = {2023-03-08},
  abstract = {Implemented in one code library.},
  howpublished = {https://paperswithcode.com/paper/performance-is-not-enough-a-story-of-the},
  langid = {english}
}

@misc{papyanMeasurementsThreeLevelHierarchical2019,
  title = {Measurements of {{Three-Level Hierarchical Structure}} in the {{Outliers}} in the {{Spectrum}} of {{Deepnet Hessians}}},
  author = {Papyan, Vardan},
  year = {2019},
  month = jan,
  number = {arXiv:1901.08244},
  eprint = {1901.08244},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1901.08244},
  urldate = {2022-11-08},
  abstract = {We consider deep classifying neural networks. We expose a structure in the derivative of the logits with respect to the parameters of the model, which is used to explain the existence of outliers in the spectrum of the Hessian. Previous works decomposed the Hessian into two components, attributing the outliers to one of them, the so-called Covariance of gradients. We show this term is not a Covariance but a second moment matrix, i.e., it is influenced by means of gradients. These means possess an additive two-way structure that is the source of the outliers in the spectrum. This structure can be used to approximate the principal subspace of the Hessian using certain "averaging" operations, avoiding the need for high-dimensional eigenanalysis. We corroborate this claim across different datasets, architectures and sample sizes.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,eigenvalues,Hessian,second order,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/66Q7CKMN/Papyan - 2019 - Measurements of Three-Level Hierarchical Structure.pdf;/home/luisaam/Zotero/storage/78DZLXXV/1901.html}
}

@misc{parkGenerativeAgentsInteractive2023,
  title = {Generative {{Agents}}: {{Interactive Simulacra}} of {{Human Behavior}}},
  shorttitle = {Generative {{Agents}}},
  author = {Park, Joon Sung and O'Brien, Joseph C. and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
  year = {2023},
  month = apr,
  number = {arXiv:2304.03442},
  eprint = {2304.03442},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.03442},
  urldate = {2023-06-08},
  abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,simulated workd},
  file = {/home/luisaam/Zotero/storage/QNB5KYHV/Park et al. - 2023 - Generative Agents Interactive Simulacra of Human .pdf;/home/luisaam/Zotero/storage/5PYFVLUB/2304.html}
}

@inproceedings{pascanuRevisitingNaturalGradient2013,
  title = {Revisiting Natural Gradient for Deep Networks},
  booktitle = {1st {{International Conference}} on {{Learning Representations}}, {{ICLR}} 2013 - {{Workshop Track Proceedings}}},
  author = {Pascanu, Razvan and Bengio, Yoshua},
  year = {2013},
  abstract = {The aim of this paper is three-fold. First we show that Hessian-Free (Martens, 2010) and Krylov Subspace Descent (Vinyals and Povey, 2012) can be described as implementations of natural gradient descent due to their use of the extended Gauss-Newton approximation of the Hessian. Secondly we re-derive natural gradient from basic principles, contrasting the difference between two versions of the algorithm found in the neural network literature, as well as highlighting a few differences between natural gradient and typical second order methods. Lastly we show empirically that natural gradient can be robust to overfitting and particularly it can be robust to the order in which the training data is presented to the model.}
}

@incollection{paszkePyTorchImperativeStyle2019,
  title = {{{PyTorch}}: {{An}} Imperative Style, High-Performance Deep Learning Library},
  booktitle = {Advances in Neural Information Processing Systems 32},
  author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and {dAlch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {8024--8035},
  publisher = {{Curran Associates, Inc.}}
}

@misc{PDFSpuriousLocal,
  title = {[{{PDF}}] {{Spurious Local Minima Exist}} for {{Almost All Over-parameterized Neural Networks}} | {{Semantic Scholar}}},
  urldate = {2021-06-28},
  howpublished = {https://www.semanticscholar.org/paper/Spurious-Local-Minima-Exist-for-Almost-All-Neural-Ding-Li/3249bb3087a2c6bbda040d7afbd658c83cf17ac5}
}

@article{PersisterCellsThat,
  title = {Persister Cells That Survive Chemotherapy Are Pinpointed},
  journal = {Nature},
  doi = {10.1038/d41586-022-01866-x},
  abstract = {A close look at the cells that drive cancer growth after chemotherapy, and thereby contribute to fatal tumour progression, provides new insights into the identity of the cells that manage to survive treatment. Population of cells that drive tumour relapse identified.},
  keywords = {Researcher App}
}

@article{phamMetaPseudoLabels2021,
  title = {Meta {{Pseudo Labels}}},
  author = {Pham, Hieu and Dai, Zihang and Xie, Qizhe and Luong, Minh-Thang and Le, Quoc V.},
  year = {2021},
  month = mar,
  journal = {arXiv:2003.10580 [cs, stat]},
  eprint = {2003.10580},
  primaryclass = {cs, stat},
  urldate = {2021-08-04},
  abstract = {We present Meta Pseudo Labels, a semi-supervised learning method that achieves a new state-of-the-art top-1 accuracy of 90.2\% on ImageNet, which is 1.6\% better than the existing state-of-the-art. Like Pseudo Labels, Meta Pseudo Labels has a teacher network to generate pseudo labels on unlabeled data to teach a student network. However, unlike Pseudo Labels where the teacher is fixed, the teacher in Meta Pseudo Labels is constantly adapted by the feedback of the student's performance on the labeled dataset. As a result, the teacher generates better pseudo labels to teach the student. Our code will be available at https://github.com/google-research/google-research/tree/master/meta\_pseudo\_labels.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Destilling Networks,small networks,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/2MX6ABE4/Pham et al. - 2021 - Meta Pseudo Labels.pdf;/home/luisaam/Zotero/storage/SRI45NT8/2003.html}
}

@article{PixelatedButterflySimple,
  title = {Pixelated {{Butterfly}}: {{Simple}} and {{Efficient Sparse}} Training for {{Neural Network Models}}. ({{arXiv}}:2112.00029v2 [Cs.{{LG}}] {{UPDATED}})},
  journal = {arXiv Computer Science},
  doi = {arXiv:2112.00029v2},
  abstract = {Overparameterized neural networks generalize well but are expensive to train. Ideally, one would like to reduce their computational cost while retaining their generalization benefits. Sparse model training is a simple and promising approach to achieve this, but there remain challenges as existing methods struggle with accuracy loss, slow training runtime, or difficulty in sparsifying all model components. The core problem is that searching for a sparsity mask over a discrete set of sparse matrices is difficult and expensive. To address this, our main insight is to optimize over a continuous superset of sparse matrices with a fixed structure known as products of butterfly matrices. As butterfly matrices are not hardware efficient, we propose simple variants of butterfly (block and flat) to take advantage of modern hardware. Our method (Pixelated Butterfly) uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers (e.g., attention, MLP). We empirically validate that Pixelated Butterfly is 3x faster than butterfly and speeds up training to achieve favorable accuracy--efficiency tradeoffs. On the ImageNet classification and WikiText-103 language modeling tasks, our sparse models train up to 2.5x faster than the dense MLP-Mixer, Vision Transformer, and GPT-2 medium with no drop in accuracy.},
  keywords = {dynamical sparse training,Researcher App,to read}
}

@article{poggioLossLandscapeSGD,
  title = {Loss Landscape: {{SGD}} Has a Better View},
  author = {Poggio, Tomaso and Cooper, Yaim},
  pages = {6},
  langid = {english},
  keywords = {to read},
  file = {/home/luisaam/Zotero/storage/KMSD2VZT/Poggio y Cooper - Loss landscape SGD has a better view.pdf}
}

@article{polyakMethodsSpeedingConvergence1964,
  title = {Some Methods of Speeding up the Convergence of Iteration Methods},
  author = {Polyak, B. T.},
  year = {1964},
  month = jan,
  journal = {USSR Computational Mathematics and Mathematical Physics},
  volume = {4},
  number = {5},
  pages = {1--17},
  issn = {0041-5553},
  doi = {10.1016/0041-5553(64)90137-5},
  urldate = {2021-06-24},
  abstract = {For the solution of the functional equation P (x) = 0 (1) (where P is an operator, usually linear, from B into B, and B is a Banach space) iteration methods are generally used. These consist of the construction of a series x0, {\ldots}, xn, {\ldots}, which converges to the solution (see, for example [1]). Continuous analogues of these methods are also known, in which a trajectory x(t), 0 {$\leqslant$} t {$\leqslant$} {$\infty$} is constructed, which satisfies the ordinary differential equation in B and is such that x(t) approaches the solution of (1) as t {\textrightarrow} {$\infty$} (see [2]). We shall call the method a k-step method if for the construction of each successive iteration xn+1 we use k previous iterations xn, {\ldots}, xn-k+1. The same term will also be used for continuous methods if x(t) satisfies a differential equation of the k-th order or k-th degree. Iteration methods which are more widely used are one-step (e.g. methods of successive approximations). They are generally simple from the calculation point of view but often converge very slowly. This is confirmed both by the evaluation of the speed of convergence and by calculation in practice (for more details see below). Therefore the question of the rate of convergence is most important. Some multistep methods, which we shall consider further, which are only slightly more complicated than the corresponding one-step methods, make it possible to speed up the convergence substantially. Note that all the methods mentioned below are applicable also to the problem of minimizing the differentiable functional (x) in Hilbert space, so long as this problem reduces to the solution of the equation grad (x) = 0.},
  langid = {english},
  file = {/home/luisaam/Zotero/storage/MPHD9U3P/0041555364901375.html}
}

@article{pourchotCEMRLCombiningEvolutionary2019,
  title = {{{CEM-RL}}: {{Combining}} Evolutionary and Gradient-Based Methods for Policy Search},
  shorttitle = {{{CEM-RL}}},
  author = {Pourchot, Alo{\"i}s and Sigaud, Olivier},
  year = {2019},
  month = feb,
  journal = {arXiv:1810.01222 [cs, stat]},
  eprint = {1810.01222},
  primaryclass = {cs, stat},
  urldate = {2021-04-28},
  abstract = {Deep neuroevolution and deep reinforcement learning (deep RL) algorithms are two popular approaches to policy search. The former is widely applicable and rather stable, but suffers from low sample efficiency. By contrast, the latter is more sample efficient, but the most sample efficient variants are also rather unstable and highly sensitive to hyper-parameter setting. So far, these families of methods have mostly been compared as competing tools. However, an emerging approach consists in combining them so as to get the best of both worlds. Two previously existing combinations use either an ad hoc evolutionary algorithm or a goal exploration process together with the Deep Deterministic Policy Gradient (DDPG) algorithm, a sample efficient off-policy deep RL algorithm. In this paper, we propose a different combination scheme using the simple cross-entropy method (CEM) and Twin Delayed Deep Deterministic policy gradient (td3), another off-policy deep RL algorithm which improves over ddpg. We evaluate the resulting method, cem-rl, on a set of benchmarks classically used in deep RL. We show that cem-rl benefits from several advantages over its competitors and offers a satisfactory trade-off between performance and sample efficiency.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/5QUEHIQ8/Pourchot y Sigaud - 2019 - CEM-RL Combining evolutionary and gradient-based .pdf;/home/luisaam/Zotero/storage/EB6TBNIL/1810.html}
}

@book{prisnerGameTheoryExamples2014,
  title = {Game {{Theory}} : {{Through Examples}}},
  author = {Prisner, E.},
  year = {2014},
  publisher = {{Mathematical Association of America}},
  address = {{Washington, D.C.}},
  abstract = {Game Theory Through Examples is a thorough introduction to elementary game theory, covering finite games with complete information. The core philosophy underlying this volume is that abstract concepts are best learned when encountered first (and repeatedly) in concrete settings. Thus, the essential ideas of game theory are here presented in the context of actual games, real games much more complex and rich than the typical toy examples. All the fundamental ideas are here: Nash equilibria, backward induction, elementary probability, imperfect information, extensive and normal form, mixed and behavioral strategies. The active-learning, example-driven approach makes the text suitable for a course taught through problem solving. Students will be thoroughly engaged by the extensive classroom exercises, compelling homework problems and nearly sixty projects in the text. Also available are approximately eighty Java applets and three dozen Excel spreadsheets in which students can play games and organize information in order to acquire a gut feeling to help in the analysis of the games. Mathematical exploration is a deep form of play, that maxim is embodied in this book. Game Theory Through Examples is a lively introduction to this appealing theory. Assuming only high school prerequisites makes the volume especially suitable for a liberal arts or general education spirit-of-mathematics course. It could also serve as the active-learning supplement to a more abstract text in an upper-division game theory course.},
  file = {/home/luisaam/Zotero/storage/UQ7E375P/Prisner - 2014 - Game Theory Through Examples(2).pdf}
}

@article{PruningAlgorithmRelaxed,
  title = {A Pruning Algorithm with Relaxed Conditions for High-Order Neural Networks Based on Smoothing Group {$<$}math{$>$}{{L1}}/2{$<$}/Math{$>$} Regularization and Adaptive Momentum},
  journal = {Knowledge-Based Systems},
  doi = {14.1326.a7b93ece-07d8-4a53-a7ce-be10a8b31fed.1663866252},
  abstract = {To enhance the sparseness of the network, improve its generalization ability and accelerate its training, we propose a novel pruning approach for sigma-pi-sigma neural network (SPSNN) under the relaxed condition by adding smoothing group   L1/2   regularization and adaptive momentum. The main strength of this method is that it can prune both the redundant nodes between groups in the network, and also the redundant weights of the non-redundant nodes within the group, so as to achieve the sparseness of the network. Another strength is that the non-smooth absolute value function in the traditional   L1/2   regularization method is replaced by a smooth function. This reduces the oscillations of learning and enables us to more effectively prove the convergence of the proposed algorithm. Finally, the numerical simulation results demonstrate the effectiveness of the proposed algorithm.},
  keywords = {Pruning,Researcher App,smoothing}
}

@article{pukrittayakameePracticalTrainingFramework2011,
  title = {Practical {{Training Framework}} for {{Fitting}} a {{Function}} and {{Its Derivatives}}},
  author = {Pukrittayakamee, Arjpolson and Hagan, Martin and Raff, Lionel and Bukkapatnam, Satish T. S. and Komanduri, Ranga},
  year = {2011},
  month = jun,
  journal = {IEEE Transactions on Neural Networks},
  volume = {22},
  number = {6},
  pages = {936--947},
  issn = {1941-0093},
  doi = {10.1109/TNN.2011.2128344},
  abstract = {This paper describes a practical framework for using multilayer feedforward neural networks to simultaneously fit both a function and its first derivatives. This framework involves two steps. The first step is to train the network to optimize a performance index, which includes both the error in fitting the function and the error in fitting the derivatives. The second step is to prune the network by removing neurons that cause overfitting and then to retrain it. This paper describes two novel types of overfitting that are only observed when simultaneously fitting both a function and its first derivatives. A new pruning algorithm is proposed to eliminate these types of overfitting. Experimental results show that the pruning algorithm successfully eliminates the overfitting and produces the smoothest responses and the best generalization among all the training algorithms that we have tested.},
  keywords = {Approximation algorithms,Artificial neural networks,Derivative approximation,derivatives learning,function approximation,Function approximation,gradient,multilayer network,Neurons,Performance analysis,Pruning,Sobolev training,Training},
  file = {/home/luisaam/Zotero/storage/K5K4TGHS/Pukrittayakamee et al. - 2011 - Practical Training Framework for Fitting a Functio.pdf;/home/luisaam/Zotero/storage/I9RQSDBZ/5768082.html}
}

@misc{PyCOMPSsParallelComputational,
  title = {{{PyCOMPSs}}: {{Parallel}} Computational Workflows in {{Python}} - {{Enric Tejedor}}, {{Yolanda Becerra}}, {{Guillem Alomar}}, {{Anna Queralt}}, {{Rosa M Badia}}, {{Jordi Torres}}, {{Toni Cortes}}, {{Jes{\'u}s Labarta}}, 2017},
  urldate = {2021-07-09},
  howpublished = {https://journals.sagepub.com/doi/abs/10.1177/1094342015594678},
  keywords = {Parallel progaming},
  file = {/home/luisaam/Zotero/storage/NSCY8A24/1094342015594678.html}
}

@article{qianDerivativeFreeReinforcementLearning2021,
  title = {Derivative-{{Free Reinforcement Learning}}: {{A Review}}},
  shorttitle = {Derivative-{{Free Reinforcement Learning}}},
  author = {Qian, Hong and Yu, Yang},
  year = {2021},
  month = feb,
  journal = {arXiv:2102.05710 [cs]},
  eprint = {2102.05710},
  primaryclass = {cs},
  doi = {10.1007/s11704-020-0241-4},
  urldate = {2021-07-04},
  abstract = {Reinforcement learning is about learning agent models that make the best sequential decisions in unknown environments. In an unknown environment, the agent needs to explore the environment while exploiting the collected information, which usually forms a sophisticated problem to solve. Derivative-free optimization, meanwhile, is capable of solving sophisticated problems. It commonly uses a sampling-and-updating framework to iteratively improve the solution, where exploration and exploitation are also needed to be well balanced. Therefore, derivative-free optimization deals with a similar core issue as reinforcement learning, and has been introduced in reinforcement learning approaches, under the names of learning classifier systems and neuroevolution/evolutionary reinforcement learning. Although such methods have been developed for decades, recently, derivative-free reinforcement learning exhibits attracting increasing attention. However, recent survey on this topic is still lacking. In this article, we summarize methods of derivative-free reinforcement learning to date, and organize the methods in aspects including parameter updating, model selection, exploration, and parallel/distributed methods. Moreover, we discuss some current limitations and possible future directions, hoping that this article could bring more attentions to this topic and serve as a catalyst for developing novel and efficient approaches.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Neuroevolution,Why ES in neural networks},
  file = {/home/luisaam/Zotero/storage/B3TIWMB7/Qian y Yu - 2021 - Derivative-Free Reinforcement Learning A Review.pdf;/home/luisaam/Zotero/storage/P3X29HQK/2102.html}
}

@misc{quProductbasedNeuralNetworks2016,
  title = {Product-Based {{Neural Networks}} for {{User Response Prediction}}},
  author = {Qu, Yanru and Cai, Han and Ren, Kan and Zhang, Weinan and Yu, Yong and Wen, Ying and Wang, Jun},
  year = {2016},
  month = nov,
  number = {arXiv:1611.00144},
  eprint = {1611.00144},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1611.00144},
  urldate = {2022-06-17},
  abstract = {Predicting user responses, such as clicks and conversions, is of great importance and has found its usage in many Web applications including recommender systems, web search and online advertising. The data in those applications is mostly categorical and contains multiple fields; a typical representation is to transform it into a high-dimensional sparse binary feature representation via one-hot encoding. Facing with the extreme sparsity, traditional models may limit their capacity of mining shallow patterns from the data, i.e. low-order feature combinations. Deep models like deep neural networks, on the other hand, cannot be directly applied for the high-dimensional input because of the huge feature space. In this paper, we propose a Product-based Neural Networks (PNN) with an embedding layer to learn a distributed representation of the categorical data, a product layer to capture interactive patterns between inter-field categories, and further fully connected layers to explore high-order feature interactions. Our experimental results on two large-scale real-world ad click datasets demonstrate that PNNs consistently outperform the state-of-the-art models on various metrics.},
  archiveprefix = {arxiv},
  keywords = {click through prediction,Computer Science - Information Retrieval,Computer Science - Machine Learning,feature interaction},
  file = {/home/luisaam/Zotero/storage/LQ8WD836/Qu et al. - 2016 - Product-based Neural Networks for User Response Pr.pdf;/home/luisaam/Zotero/storage/9A6IGZ9U/1611.html}
}

@article{rajbhandariZeROMemoryOptimizations2020,
  title = {{{ZeRO}}: {{Memory Optimizations Toward Training Trillion Parameter Models}}},
  shorttitle = {{{ZeRO}}},
  author = {Rajbhandari, Samyam and Rasley, Jeff and Ruwase, Olatunji and He, Yuxiong},
  year = {2020},
  month = may,
  urldate = {2022-11-07},
  abstract = {Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, [{\ldots}]},
  langid = {american},
  file = {/home/luisaam/Zotero/storage/33UYQAEY/zero-memory-optimizations-toward-training-trillion-parameter-models.html}
}

@misc{rajpurkarSQuAD1000002016,
  title = {{{SQuAD}}: 100,000+ {{Questions}} for {{Machine Comprehension}} of {{Text}}},
  shorttitle = {{{SQuAD}}},
  author = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  year = {2016},
  month = oct,
  number = {arXiv:1606.05250},
  eprint = {1606.05250},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1606.05250},
  urldate = {2022-12-15},
  abstract = {We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0\%, a significant improvement over a simple baseline (20\%). However, human performance (86.8\%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,NLP,question answering},
  file = {/home/luisaam/Zotero/storage/D8IN9967/Rajpurkar et al. - 2016 - SQuAD 100,000+ Questions for Machine Comprehensio.pdf;/home/luisaam/Zotero/storage/PZ79653L/1606.html}
}

@article{ramachandranSearchingActivationFunctions2017,
  title = {Searching for {{Activation Functions}}},
  author = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V.},
  year = {2017},
  month = oct,
  journal = {arXiv:1710.05941 [cs]},
  eprint = {1710.05941},
  primaryclass = {cs},
  urldate = {2021-08-31},
  abstract = {The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, \$f(x) = x {\textbackslash}cdot {\textbackslash}text\{sigmoid\}({\textbackslash}beta x)\$, which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9{\textbackslash}\% for Mobile NASNet-A and 0.6{\textbackslash}\% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/home/luisaam/Zotero/storage/TS3NEDJX/Ramachandran et al. - 2017 - Searching for Activation Functions.pdf;/home/luisaam/Zotero/storage/L8N3HJS9/1710.html}
}

@inproceedings{ramanujanWhatHiddenRandomly2020,
  title = {What's {{Hidden}} in a {{Randomly Weighted Neural Network}}?},
  booktitle = {{{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Ramanujan, Vivek and Wortsman, Mitchell and Kembhavi, Aniruddha and Farhadi, Ali and Rastegari, Mohammad},
  year = {2020},
  month = jun,
  keywords = {Pruning,Random Pruning}
}

@inproceedings{ramanujanWhatHiddenRandomly2020a,
  title = {What's {{Hidden}} in a {{Randomly Weighted Neural Network}}?},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Ramanujan, Vivek and Wortsman, Mitchell and Kembhavi, Aniruddha and Farhadi, Ali and Rastegari, Mohammad},
  year = {2020},
  pages = {11893--11902},
  urldate = {2023-05-31},
  file = {/home/luisaam/Zotero/storage/ZPPMKH53/Ramanujan et al. - 2020 - What's Hidden in a Randomly Weighted Neural Networ.pdf}
}

@article{RansomwareRecognitionBlockchain,
  title = {Ransomware Recognition in Blockchain Network Using Water Moth Flame Optimization-aware {{DRNN}}},
  journal = {Concurrency and Computation: Practice and Experience},
  doi = {10.1002/cpe.7047},
  abstract = {The emergence of networking systems and quick deployment of applications cause huge increase in cybercrimes which involves various applications like phishing, hacking, and malware propagation. However, the Ransomware techniques utilize certain device which may lead to undesirable properties which might shrink the paying-victim pool. This paper devises a new method, namely Water Moth Flame optimization (WMFO) and deep recurrent neural network (Deep RNN) for determining Ransomware. Here, Deep RNN training is done with WMFO, and is developed by combining Moth Flame optimization (MFO) and Water wave optimization (WWO). Moreover, features are mined with opcodes and by finding term frequency-inverse document frequency (TF-IDF) amongst individual features. Moreover, Probabilistic Principal Component Analysis (PPCA) is adapted to choose significant features. These features are adapted in Deep RNN for classification, wherein the proposed WMFO is employed to produce optimum weights. The WMFO offered enhanced performance with elevated accuracy of 95.025\%, sensitivity of 95\%, and specificity of 96\%.},
  keywords = {Researcher App}
}

@article{reedPruningAlgorithmsaSurvey1993,
  title = {Pruning Algorithms-a Survey},
  author = {Reed, R.},
  year = {1993},
  month = sep,
  journal = {IEEE Transactions on Neural Networks},
  volume = {4},
  number = {5},
  pages = {740--747},
  issn = {1941-0093},
  doi = {10.1109/72.248452},
  abstract = {A rule of thumb for obtaining good generalization in systems trained by examples is that one should use the smallest system that will fit the data. Unfortunately, it usually is not obvious what size is best; a system that is too small will not be able to learn the data while one that is just big enough may learn very slowly and be very sensitive to initial conditions and learning parameters. This paper is a survey of neural network pruning algorithms. The approach taken by the methods described here is to train a network that is larger than necessary and then remove the parts that are not needed.{$<>$}},
  keywords = {examples,Multilayer perceptrons,Neural networks,Pruning,Sampling methods,Testing,Thumb,Training data,Tree data structures},
  file = {/home/luisaam/Zotero/storage/5IJENES7/Reed - 1993 - Pruning algorithms-a survey.pdf;/home/luisaam/Zotero/storage/DLK49FHE/authors.html}
}

@article{RelationshipUniversalAdversarial,
  title = {On {{The Relationship Between Universal Adversarial Attacks And Sparse Representations}}. ({{arXiv}}:2311.08265v1 [Cs.{{CV}}])},
  journal = {arXiv Computer Science},
  doi = {arXiv:2311.08265v1},
  abstract = {The prominent success of neural networks, mainly in computer vision tasks, is increasingly shadowed by their sensitivity to small, barely perceivable adversarial perturbations in image input.     In this work, we aim at explaining this vulnerability through the framework of sparsity.     We show the connection between adversarial attacks and sparse representations, with a focus on explaining the universality and transferability of adversarial examples in neural networks.     To this end, we show that sparse coding algorithms, and the neural network-based learned iterative shrinkage thresholding algorithm (LISTA) among them, suffer from this sensitivity, and that common attacks on neural networks can be expressed as attacks on the sparse representation of the input image. The phenomenon that we observe holds true also when the network is agnostic to the sparse representation and dictionary, and thus can provide a possible explanation for the universality and transferability of adversarial attacks.     The code is available at https://github.com/danawr/adversarial\_attacks\_and\_sparse\_representations.},
  keywords = {Researcher App}
}

@inproceedings{rendaComparingRewindingFinetuning2020,
  title = {Comparing Rewinding and Fine-Tuning in Neural Network Pruning},
  booktitle = {International Conference on Learning Representations},
  author = {Renda, Alex and Frankle, Jonathan and Carbin, Michael},
  year = {2020},
  file = {/home/luisaam/Zotero/storage/45UIDC4L/Renda et al. - 2020 - Comparing rewinding and fine-tuning in neural netw.pdf}
}

@article{ResearchHumanSleep,
  title = {Research on Human Sleep Improvement Method Based on {{DQN}}},
  journal = {Journal of Ambient Intelligence and Smart Environments},
  doi = {10.3233/ais-230294},
  abstract = {To solve the problems of sleep disorders such as difficulty in falling asleep and insufficient sleep depth caused by uncomfortable indoor temperature, this paper proposes a deep reinforcement learning method based on deep Q-network (DQN) with human sleep electroencephalogram (EEG) as input to improve human sleep. Firstly, the EEG is subjected to a short-time Fourier transform to construct a time-frequency feature data set, which is used as input to DQN along with temperature. Secondly, the agent performs environmental interaction actions in each time step and returns a reward value. Finally, the optimal strategy for indoor temperature control is formulated by the agent. The simulation results show that this method can dynamically adjust the indoor temperature to the optimal temperature for human sleep, and can alleviate sleep disorders, which has certain practical significance},
  keywords = {Researcher App}
}

@article{retsinasWeightPruningAdaptive,
  title = {Weight {{Pruning}} via {{Adaptive Sparsity Loss}}. ({{arXiv}}:2006.02768v1 [Cs.{{LG}}])},
  author = {Retsinas, George and Elafrou, Athena and Goumas, Georgios and Maragos, Petros},
  journal = {arXiv Computer Science},
  doi = {arXiv:2006.02768v1},
  abstract = {Pruning neural networks has regained interest in recent years as a means to compress state-of-the-art deep neural networks and enable their deployment on resource-constrained devices. In this paper, we propose a robust compressive learning framework that efficiently prunes network parameters during training with minimal computational overhead. We incorporate fast mechanisms to prune individual layers and build upon these to automatically prune the entire network under a user-defined budget constraint. Key to our end-to-end network pruning approach is the formulation of an intuitive and easy-to-implement adaptive sparsity loss that is used to explicitly control sparsity during training, enabling efficient budget-aware optimization. Extensive experiments demonstrate the effectiveness of the proposed framework for image classification on the CIFAR and ImageNet datasets using different architectures, including AlexNet, ResNets and Wide ResNets.},
  keywords = {Pruning,Researcher App,to read}
}

@article{ReviewEvolutionaryAlgorithms,
  title = {A Review of Evolutionary Algorithms in Solving Large Scale Benchmark Optimisation Problems},
  journal = {International Journal of Mathematics in Operational Research},
  doi = {10.1504/IJMOR.2022.120340},
  abstract = {Optimisation problems containing huge total of decision variables are termed as large scale global optimisation problems which are often considered as abundant challenges to the area of optimisation. With presence of large number of decision variables, these problems also used to have the property of nonlinearity, discontinuity and multi-modality. Hence, the nature-inspired optimisation algorithms based on stochastic approaches are termed as great saviours than the deterministic approaches to handle these problems. However, the nature inspired optimisation algorithms also suffer from the jinx of dimensionality in the decision variable space. With increase of dimensions in the decision variable space, the complexity of the problem also increases exponentially. Hence, there is an immense need of proper guidance of choosing capable nature inspired algorithms to solve real-life large scale optimisation problems. In this paper, an attempt has been made to select the elite algorithm with proper justification. Hence, a number of works have been presented to analyse the results and to tackle the difficulty.},
  keywords = {Researcher App}
}

@inproceedings{roAutoLRLayerwisePruning2021,
  title = {{{AutoLR}}: {{Layer-wise}} Pruning and Auto-Tuning of Learning Rates in Fine-Tuning of Deep Networks},
  booktitle = {Proceedings of the {{AAAI}} Conference on Artificial Intelligence},
  author = {Ro, Youngmin and Choi, Jin Young},
  year = {2021},
  volume = {35},
  pages = {2486--2494},
  keywords = {NN feautres,Pruning,Reinforcement Learning},
  file = {/home/luisaam/Zotero/storage/78ST8Z2Y/Ro y Choi - 2021 - AutoLR Layer-wise pruning and auto-tuning of lear.pdf}
}

@article{robbinsStochasticApproximationMethod1951,
  title = {A {{Stochastic Approximation Method}}},
  author = {Robbins, Herbert and Monro, Sutton},
  year = {1951},
  month = sep,
  journal = {The Annals of Mathematical Statistics},
  volume = {22},
  number = {3},
  pages = {400--407},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729586},
  urldate = {2021-08-06},
  abstract = {Let \$M(x)\$ denote the expected value at level \$x\$ of the response to a certain experiment. \$M(x)\$ is assumed to be a monotone function of \$x\$ but is unknown to the experimenter, and it is desired to find the solution \$x = {\textbackslash}theta\$ of the equation \$M(x) = {\textbackslash}alpha\$, where \${\textbackslash}alpha\$ is a given constant. We give a method for making successive experiments at levels \$x\_1,x\_2,{\textbackslash}cdots\$ in such a way that \$x\_n\$ will tend to \${\textbackslash}theta\$ in probability.},
  keywords = {examples},
  file = {/home/luisaam/Zotero/storage/2W4MA39C/Robbins y Monro - 1951 - A Stochastic Approximation Method.pdf;/home/luisaam/Zotero/storage/ZZ8Y9FR8/1177729586.html}
}

@incollection{robertsonUsingGameTheory2015,
  title = {Using {{Game Theory}} for {{Threat Intelligence}}},
  booktitle = {Darkweb {{Cyber Threat Intelligence Mining}}},
  author = {Robertson, John and Diab, Ahmad and Marin, Ericsson and Nunes, Eric and Paliath, Vivin and Shakarian, Jana and Shakarian, Paulo},
  year = {2015},
  pages = {67--95},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/9781316888513.008},
  abstract = {Penetration testing is regarded as the gold-standard for understanding how well an organization can withstand sophisticated cyber-attacks. In a penetration test, a ``red team'' is hired to expose major flaws in the firm's security infrastructure. Recently, however, the market for exploit kits has continued to evolve and what was once a rather hard-to-penetrate and exclusive market{\textemdash}whose buyers were primarily western governments [95], has now become more accessible to a much wider population. In particular, 2015 saw the introduction of darknet markets specializing in zero-day exploit kits{\textemdash}exploits designed to leverage previously undiscovered vulnerabilities. These markets, which were discussed in Chapters 3{\textendash}5, make exploits widely available to potential attackers. These exploit kits are difficult and time consuming to develop{\textemdash}and are often sold at premium prices. The cost associated with these sophisticated kits generally precludes penetration testers from simply obtaining such exploits, meaning an alternative approach is needed to understand what exploits an attacker will most likely purchase and how to defend against them. In this chapter, we introduce a data-driven security game framework to model an attacker and a defender of a specific system, providing system-specific policy recommendations to the defender. In addition to providing a formal framework and algorithms to develop strategies, we present experimental results from applying our framework, for various system configurations, on a subset of the real-world exploit data gathered from the system presented in Chapter 4. This game theoretic framework provides another example of rich cyber threat intelligence that can be derived from the darknet exploit data.},
  isbn = {978-1-316-88851-3},
  file = {/home/luisaam/Zotero/storage/XHXFQPBN/Robertson et al. - 2015 - Using Game Theory for Threat Intelligence.pdf}
}

@article{RobustParetoSet,
  title = {Robust {{Pareto Set Identification}} with {{Contaminated Bandit Feedback}}. ({{arXiv}}:2206.02666v1 [Cs.{{LG}}])},
  journal = {arXiv Machine Learning (Statistics)},
  doi = {arXiv:2206.02666v1},
  abstract = {We consider the Pareto set identification (PSI) problem in multi-objective multi-armed bandits (MO-MAB) with contaminated reward observations. At each arm pull, with some probability, the true reward samples are replaced with the samples from an arbitrary contamination distribution chosen by the adversary. We propose a median-based MO-MAB algorithm for robust PSI that abides by the accuracy requirements set by the user via an accuracy parameter. We prove that the sample complexity of this algorithm depends on the accuracy parameter inverse squarely. We compare the proposed algorithm with a mean-based method from MO-MAB literature on Gaussian reward distributions. Our numerical results verify our theoretical expectations and show the necessity for robust algorithm design in the adversarial setting.},
  keywords = {Researcher App}
}

@inproceedings{rochaEvolutionaryNeuralNetwork2003,
  title = {Evolutionary {{Neural Network Learning}}},
  booktitle = {Progress in {{Artificial Intelligence}}},
  author = {Rocha, Miguel and Cortez, Paulo and Neves, Jos{\'e}},
  editor = {Pires, Fernando Moura and Abreu, Salvador},
  year = {2003},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {24--28},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-24580-3_10},
  abstract = {Several gradient-based methods have been developed for Artificial Neural Network (ANN) training. Still, in some situations, such procedures may lead to local minima, making Evolutionary Algorithms (EAs) a promising alternative. In this work, EAs using direct representations are applied to several classification and regressionANN learning tasks. Furthermore, EAs are also combined with local optimization, under the Lamarckian framework. Both strategies are compared with conventional training methods. The results reveal an enhanced performance by a macro-mutation based Lamarckian approach.},
  isbn = {978-3-540-24580-3},
  langid = {english},
  keywords = {Crossover Operator,Gaussian Mutation,Logistic Activation Function,Neural Population,Root Mean Square Error},
  file = {/home/luisaam/Zotero/storage/NACLX98K/Rocha et al. - 2003 - Evolutionary Neural Network Learning.pdf}
}

@article{romaniukDivideConquerNeural1993,
  title = {Divide and Conquer Neural Networks},
  author = {Romaniuk, Steve G and Hall, Lawrence O},
  year = {1993},
  journal = {Neural Networks},
  volume = {6},
  number = {8},
  pages = {1105--1116},
  publisher = {{Elsevier}}
}

@article{rumelhartLearningRepresentationsBackpropagating1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year = {1986},
  month = oct,
  journal = {Nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/323533a0},
  urldate = {2021-06-24},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
  copyright = {1986 Nature Publishing Group},
  langid = {english},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research},
  file = {/home/luisaam/Zotero/storage/JRIKJQPJ/Rumelhart et al. - 1986 - Learning representations by back-propagating error.pdf;/home/luisaam/Zotero/storage/ZCIZ98IR/323533a0.html}
}

@article{sagunEigenvaluesHessianDeep2016,
  title = {Eigenvalues of the {{Hessian}} in {{Deep Learning}}: {{Singularity}} and {{Beyond}}},
  shorttitle = {Eigenvalues of the {{Hessian}} in {{Deep Learning}}},
  author = {Sagun, Levent and Bottou, Leon and LeCun, Yann},
  year = {2016},
  month = nov,
  urldate = {2022-02-07},
  abstract = {The eigenvalues of the Hessian of loss functions in deep learning have two components: singular bulk at zero that depends on the over-parametrization, and the discrete part that depends on the data.},
  langid = {english},
  keywords = {Hessian,second order},
  file = {/home/luisaam/Zotero/storage/SFBD8BYG/Sagun et al. - 2016 - Eigenvalues of the Hessian in Deep Learning Singu.pdf;/home/luisaam/Zotero/storage/EDJWCGJK/forum.html}
}

@article{sagunEmpiricalAnalysisHessian2018,
  title = {Empirical {{Analysis}} of the {{Hessian}} of {{Over-Parametrized Neural Networks}}},
  author = {Sagun, Levent and Evci, Utku and Guney, V. Ugur and Dauphin, Yann and Bottou, Leon},
  year = {2018},
  month = may,
  journal = {arXiv:1706.04454 [cs]},
  eprint = {1706.04454},
  primaryclass = {cs},
  urldate = {2022-02-07},
  abstract = {We study the properties of common loss surfaces through their Hessian matrix. In particular, in the context of deep learning, we empirically show that the spectrum of the Hessian is composed of two parts: (1) the bulk centered near zero, (2) and outliers away from the bulk. We present numerical evidence and mathematical justifications to the following conjectures laid out by Sagun et al. (2016): Fixing data, increasing the number of parameters merely scales the bulk of the spectrum; fixing the dimension and changing the data (for instance adding more clusters or making the data less separable) only affects the outliers. We believe that our observations have striking implications for non-convex optimization in high dimensions. First, the flatness of such landscapes (which can be measured by the singularity of the Hessian) implies that classical notions of basins of attraction may be quite misleading. And that the discussion of wide/narrow basins may be in need of a new perspective around over-parametrization and redundancy that are able to create large connected components at the bottom of the landscape. Second, the dependence of small number of large eigenvalues to the data distribution can be linked to the spectrum of the covariance matrix of gradients of model outputs. With this in mind, we may reevaluate the connections within the data-architecture-algorithm framework of a model, hoping that it would shed light into the geometry of high-dimensional and non-convex spaces in modern applications. In particular, we present a case that links the two observations: small and large batch gradient descent appear to converge to different basins of attraction but we show that they are in fact connected through their flat region and so belong to the same basin.},
  archiveprefix = {arxiv},
  keywords = {bulk vs outliers,Computer Science - Machine Learning,read},
  file = {/home/luisaam/Zotero/storage/XTLZDAYS/Sagun et al. - 2018 - Empirical Analysis of the Hessian of Over-Parametr.pdf;/home/luisaam/Zotero/storage/QTBRSKCI/1706.html}
}

@article{salimansEvolutionStrategiesScalable2017,
  title = {Evolution {{Strategies}} as a {{Scalable Alternative}} to {{Reinforcement Learning}}},
  author = {Salimans, Tim and Ho, Jonathan and Chen, Xi and Sidor, Szymon and Sutskever, Ilya},
  year = {2017},
  month = sep,
  journal = {arXiv:1703.03864 [cs, stat]},
  eprint = {1703.03864},
  primaryclass = {cs, stat},
  urldate = {2021-07-01},
  abstract = {We explore the use of Evolution Strategies (ES), a class of black box optimization algorithms, as an alternative to popular MDP-based RL techniques such as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show that ES is a viable solution strategy that scales extremely well with the number of CPUs available: By using a novel communication strategy based on common random numbers, our ES implementation only needs to communicate scalars, making it possible to scale to over a thousand parallel workers. This allows us to solve 3D humanoid walking in 10 minutes and obtain competitive results on most Atari games after one hour of training. In addition, we highlight several advantages of ES as a black box optimization technique: it is invariant to action frequency and delayed rewards, tolerant of extremely long horizons, and does not need temporal discounting or value function approximation.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,RL and neuroevolution,Statistics - Machine Learning,Why ES in neural networks},
  file = {/home/luisaam/Zotero/storage/WLUFA8NB/Salimans et al. - 2017 - Evolution Strategies as a Scalable Alternative to .pdf;/home/luisaam/Zotero/storage/ZSWHBRA3/1703.html}
}

@article{sandlerMobileNetV2InvertedResiduals2019,
  title = {{{MobileNetV2}}: {{Inverted Residuals}} and {{Linear Bottlenecks}}},
  shorttitle = {{{MobileNetV2}}},
  author = {Sandler, Mark and Howard, Andrew and Zhu, Menglong and Zhmoginov, Andrey and Chen, Liang-Chieh},
  year = {2019},
  month = mar,
  journal = {arXiv:1801.04381 [cs]},
  eprint = {1801.04381},
  primaryclass = {cs},
  urldate = {2021-08-05},
  abstract = {In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input an MobileNetV2 uses lightweight depthwise convolutions to filter features in the intermediate expansion layer. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on Imagenet classification, COCO object detection, VOC image segmentation. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as the number of parameters},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Inverted Conv Block,small networks},
  file = {/home/luisaam/Zotero/storage/QPB394TG/Sandler et al. - 2019 - MobileNetV2 Inverted Residuals and Linear Bottlen.pdf;/home/luisaam/Zotero/storage/QDJPWGEU/1801.html}
}

@article{saxeExactSolutionsNonlinear2014,
  title = {Exact Solutions to the Nonlinear Dynamics of Learning in Deep Linear Neural Networks},
  author = {Saxe, Andrew M. and McClelland, James L. and Ganguli, Surya},
  year = {2014},
  month = feb,
  journal = {arXiv:1312.6120 [cond-mat, q-bio, stat]},
  eprint = {1312.6120},
  primaryclass = {cond-mat, q-bio, stat},
  urldate = {2021-08-05},
  abstract = {Despite the widespread practical success of deep learning methods, our theoretical understanding of the dynamics of learning in deep neural networks remains quite sparse. We attempt to bridge the gap between the theory and practice of deep learning by systematically analyzing learning dynamics for the restricted case of deep linear neural networks. Despite the linearity of their input-output map, such networks have nonlinear gradient descent dynamics on weights that change with the addition of each new hidden layer. We show that deep linear networks exhibit nonlinear learning phenomena similar to those seen in simulations of nonlinear networks, including long plateaus followed by rapid transitions to lower error solutions, and faster convergence from greedy unsupervised pretraining initial conditions than from random initial conditions. We provide an analytical description of these phenomena by finding new exact solutions to the nonlinear dynamics of deep learning. Our theoretical analysis also reveals the surprising finding that as the depth of a network approaches infinity, learning speed can nevertheless remain finite: for a special class of initial conditions on the weights, very deep networks incur only a finite, depth independent, delay in learning speed relative to shallow networks. We show that, under certain conditions on the training data, unsupervised pretraining can find this special class of initial conditions, while scaled random Gaussian initializations cannot. We further exhibit a new class of random orthogonal initial conditions on weights that, like unsupervised pre-training, enjoys depth independent learning times. We further show that these initial conditions also lead to faithful propagation of gradients even in deep nonlinear networks, as long as they operate in a special regime known as the edge of chaos.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks,Dynamics of Learning,Quantitative Biology - Neurons and Cognition,small networks,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/CYDCQTZG/Saxe et al. - 2014 - Exact solutions to the nonlinear dynamics of learn.pdf;/home/luisaam/Zotero/storage/NIG5I49V/1312.html}
}

@article{schurholtHyperRepresentationsPreTrainingTransfer2022,
  title = {Hyper-{{Representations}} for {{Pre-Training}} and {{Transfer Learning}}},
  author = {Sch{\"u}rholt, Konstantin and Knyazev, Boris and {Gir{\'o}-i-Nieto}, Xavier and Borth, Damian},
  year = {2022},
  journal = {arXiv preprint arXiv:2207.10951},
  eprint = {2207.10951},
  archiveprefix = {arxiv},
  file = {/home/luisaam/Zotero/storage/AZDQZDUN/SchÃ¼rholt et al. - 2022 - Hyper-Representations for Pre-Training and Transfe.pdf}
}

@article{schwartzGreenAI2019,
  title = {Green {{AI}}},
  author = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},
  year = {2019},
  month = aug,
  journal = {arXiv:1907.10597 [cs, stat]},
  eprint = {1907.10597},
  primaryclass = {cs, stat},
  urldate = {2022-01-30},
  abstract = {The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2]. These computations have a surprisingly large carbon footprint [38]. Ironically, deep learning was inspired by the human brain, which is remarkably energy efficient. Moreover, the financial cost of the computations can make it difficult for academics, students, and researchers, in particular those from emerging economies, to engage in deep learning research. This position paper advocates a practical solution by making efficiency an evaluation criterion for research alongside accuracy and related measures. In addition, we propose reporting the financial cost or "price tag" of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods. Our goal is to make AI both greener and more inclusive---enabling any inspired undergraduate with a laptop to write high-quality research papers. Green AI is an emerging focus at the Allen Institute for AI.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Efficiency,green ai,Statistics - Methodology},
  file = {/home/luisaam/Zotero/storage/W3I4U2TL/Schwartz et al. - 2019 - Green AI.pdf;/home/luisaam/Zotero/storage/4LZP3WEN/1907.html}
}

@inproceedings{semenovaExistenceSimplerMachine2022,
  title = {On the {{Existence}} of {{Simpler Machine Learning Models}}},
  booktitle = {2022 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Semenova, Lesia and Rudin, Cynthia and Parr, Ronald},
  year = {2022},
  month = jun,
  series = {{{FAccT}} '22},
  pages = {1827--1858},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3531146.3533232},
  urldate = {2023-03-08},
  abstract = {It is almost always easier to find an accurate-but-complex model than an accurate-yet-simple model. Finding optimal, sparse, accurate models of various forms (linear models with integer coefficients, decision sets, rule lists, decision trees) is generally NP-hard. We often do not know whether the search for a simpler model will be worthwhile, and thus we do not go to the trouble of searching for one. In this work, we ask an important practical question: can accurate-yet-simple models be proven to exist, or shown likely to exist, before explicitly searching for them? We hypothesize that there is an important reason that simple-yet-accurate models often do exist. This hypothesis is that the size of the Rashomon set is often large, where the Rashomon set is the set of almost-equally-accurate models from a function class. If the Rashomon set is large, it contains numerous accurate models, and perhaps at least one of them is the simple model we desire. In this work, we formally present the Rashomon ratio as a new gauge of simplicity for a learning problem, depending on a function class and a data set. The Rashomon ratio is the ratio of the volume of the set of accurate models to the volume of the hypothesis space, and it is different from standard complexity measures from statistical learning theory. Insight from studying the Rashomon ratio provides an easy way to check whether a simpler model might exist for a problem before finding it, namely whether several different machine learning methods achieve similar performance on the data. In that sense, the Rashomon ratio is a powerful tool for understanding why and when an accurate-yet-simple model might exist. If, as we hypothesize in this work, many real-world data sets admit large Rashomon sets, the implications are vast: it means that simple or interpretable models may often be used for high-stakes decisions without losing accuracy.},
  isbn = {978-1-4503-9352-2},
  keywords = {Generalization,Interpretable Machine Learning,Model Multiplicity,Pruning,Rashomon Set,Simplicity},
  file = {/home/luisaam/Zotero/storage/T9NA5MGZ/Semenova et al. - 2022 - On the Existence of Simpler Machine Learning Model.pdf}
}

@article{sener2020,
  title = {L g r s},
  author = {Sener, Ozan and Labs, Intel and Koltun, Vladlen and Labs, Intel},
  year = {2020},
  pages = {1--23}
}

@inproceedings{senerLearningGuideRandom2020,
  title = {Learning to {{Guide Random Search}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Sener, Ozan and Koltun, Vladlen},
  year = {2020},
  month = mar,
  urldate = {2022-07-27},
  abstract = {We improve the sample-efficiency of the random search for functions defined on low-dimensional manifolds. Our method jointly learns the underlying manifold and optimizes the function.},
  langid = {english},
  keywords = {Evolution strategy,Neuroevolution},
  file = {/home/luisaam/Zotero/storage/RKHHTFPN/learning_to_guide_random_search.pdf;/home/luisaam/Zotero/storage/7TVNF4TL/forum.html}
}

@article{seniorImprovedProteinStructure2020,
  title = {Improved Protein Structure Prediction Using Potentials from Deep Learning},
  author = {Senior, Andrew W. and Evans, Richard and Jumper, John and Kirkpatrick, James and Sifre, Laurent and Green, Tim and Qin, Chongli and {\v Z}{\'i}dek, Augustin and Nelson, Alexander W. R. and Bridgland, Alex and Penedones, Hugo and Petersen, Stig and Simonyan, Karen and Crossan, Steve and Kohli, Pushmeet and Jones, David T. and Silver, David and Kavukcuoglu, Koray and Hassabis, Demis},
  year = {2020},
  month = jan,
  journal = {Nature},
  volume = {577},
  number = {7792},
  pages = {706--710},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-019-1923-7},
  urldate = {2021-07-02},
  abstract = {Protein structure prediction can be used to determine the three-dimensional shape of a protein from its amino acid sequence1. This problem is of fundamental importance as the structure of a protein largely determines its function2; however, protein structures can be difficult to determine experimentally. Considerable progress has recently been made by leveraging genetic information. It is possible to infer which amino acid residues are in contact by analysing covariation in homologous sequences, which aids in the prediction of protein structures3. Here we show that we can train a neural network to make accurate predictions of the distances between pairs of residues, which convey more information about the structure than contact predictions. Using this information, we construct a potential of mean force4 that can accurately describe the shape of a protein. We find that the resulting potential can be optimized by a simple gradient descent algorithm to generate structures without complex sampling procedures. The resulting system, named AlphaFold, achieves high accuracy, even for sequences with fewer homologous sequences. In the recent Critical Assessment of Protein Structure Prediction5 (CASP13){\textemdash}a blind assessment of the state of the field{\textemdash}AlphaFold created high-accuracy structures (with template modelling (TM) scores6 of 0.7 or higher) for 24 out of 43 free modelling domains, whereas the next best method, which used sampling and contact information, achieved such accuracy for only 14 out of 43 domains. AlphaFold represents a considerable advance in protein-structure prediction. We expect this increased accuracy to enable insights into the function and malfunction of proteins, especially in cases for which no structures for homologous proteins have been experimentally determined7. AlphaFold predicts the distances between pairs of residues, is used to construct potentials of mean force that accurately describe the shape of a protein and can be optimized with gradient descent to predict protein structures.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {examples},
  annotation = {Primary\_atype: Research Subject\_term: Machine learning;Protein structure predictions Subject\_term\_id: machine-learning;protein-structure-predictions},
  file = {/home/luisaam/Zotero/storage/7VA64XW5/Senior et al. - 2020 - Improved protein structure prediction using potent.pdf;/home/luisaam/Zotero/storage/X5SL5YGD/s41586-019-1923-7.html}
}

@inproceedings{shenWhenPrunePolicy2022,
  title = {When to Prune? A Policy towards Early Structural Pruning},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Shen, Maying and Molchanov, Pavlo and Yin, Hongxu and Alvarez, Jose M},
  year = {2022},
  pages = {12247--12256},
  keywords = {Pruning,Random Pruning}
}

@inproceedings{shepanskiTeachingArtificialNeural1988,
  title = {Teaching {{Artificial Neural Systems}} to {{Drive}}: {{Manual Training Techniques}} for {{Autonomous Systems}}},
  shorttitle = {Teaching {{Artificial Neural Systems}} to {{Drive}}},
  booktitle = {Neural {{Information Processing Systems}}},
  author = {Shepanski, J. F. and Macy, S. A.},
  editor = {Anderson, D.},
  year = {1988},
  publisher = {{American Institute of Physics}},
  urldate = {2021-08-04},
  file = {/home/luisaam/Zotero/storage/C7NFN9QH/46a4378f835dc8040c8057beb6a2da52-Abstract.html}
}

@misc{shresthaOccamNetsMitigatingDataset2022,
  title = {{{OccamNets}}: {{Mitigating Dataset Bias}} by {{Favoring Simpler Hypotheses}}},
  shorttitle = {{{OccamNets}}},
  author = {Shrestha, Robik and Kafle, Kushal and Kanan, Christopher},
  year = {2022},
  month = jul,
  number = {arXiv:2204.02426},
  eprint = {2204.02426},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.02426},
  urldate = {2022-12-21},
  abstract = {Dataset bias and spurious correlations can significantly impair generalization in deep neural networks. Many prior efforts have addressed this problem using either alternative loss functions or sampling strategies that focus on rare patterns. We propose a new direction: modifying the network architecture to impose inductive biases that make the network robust to dataset bias. Specifically, we propose OccamNets, which are biased to favor simpler solutions by design. OccamNets have two inductive biases. First, they are biased to use as little network depth as needed for an individual example. Second, they are biased toward using fewer image locations for prediction. While OccamNets are biased toward simpler hypotheses, they can learn more complex hypotheses if necessary. In experiments, OccamNets outperform or rival state-of-the-art methods run on architectures that do not incorporate these inductive biases. Furthermore, we demonstrate that when the state-of-the-art debiasing methods are combined with OccamNets results further improve.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Pruning,small networks,System1-System2},
  file = {/home/luisaam/Zotero/storage/MWC2EDQ6/Shrestha et al. - 2022 - OccamNets Mitigating Dataset Bias by Favoring Sim.pdf;/home/luisaam/Zotero/storage/4GU9IUI7/2204.html}
}

@inproceedings{siebelEfficientNeuralNetwork2009,
  title = {Efficient Neural Network Pruning during Neuro-Evolution},
  booktitle = {2009 {{International Joint Conference}} on {{Neural Networks}}},
  author = {Siebel, Nils T. and Botel, Jonas and Sommer, Gerald},
  year = {2009},
  month = jun,
  pages = {2920--2927},
  issn = {2161-4407},
  doi = {10.1109/IJCNN.2009.5179035},
  abstract = {In this article we present a new method for the pruning of unnecessary connections from neural networks created by an evolutionary algorithm (neuro-evolution). Pruning not only decreases the complexity of the network but also improves the numerical stability of the parameter optimisation process. We show results from experiments where connection pruning is incorporated into EANT2, an evolutionary reinforcement learning algorithm for both the topology and parameters of neural networks. By analysing data from the evolutionary optimisation process that determines the network's parameters, candidate connections for removal are identified without the need for extensive additional calculations.},
  keywords = {Artificial neural networks,Data mining,Economic forecasting,Load forecasting,Neural networks,Particle swarm optimization,Power industry,Power system modeling,Power system security,Predictive models,Pruning,Reinforcement Learning},
  file = {/home/luisaam/Zotero/storage/KYKXH4FP/5179035.html}
}

@article{silverMasteringGameGo2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and {van den Driessche}, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  journal = {Nature},
  volume = {529},
  number = {7587},
  pages = {484--489},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature16961},
  urldate = {2021-07-02},
  abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses `value networks' to evaluate board positions and `policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
  copyright = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  langid = {english},
  keywords = {examples},
  annotation = {Bandiera\_abtest: a Cg\_type: Nature Research Journals Primary\_atype: Research Subject\_term: Computational science;Computer science;Reward Subject\_term\_id: computational-science;computer-science;reward},
  file = {/home/luisaam/Zotero/storage/RQER3SBP/Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf;/home/luisaam/Zotero/storage/6223L2GS/nature16961.html}
}

@misc{simonsinstituteStochasticSecondOrder,
  title = {Stochastic {{Second Order Optimization Methods I}}},
  author = {{Simons Institute}},
  urldate = {2021-07-12},
  abstract = {Fred Roosta, University of Queensland https://simons.berkeley.edu/talks/clo... Foundations of Data Science Boot Camp}
}

@misc{simonsinstituteStochasticSecondOrdera,
  title = {Stochastic {{Second Order Optimization Methods II}}},
  author = {{Simons Institute}},
  urldate = {2021-07-12},
  abstract = {Fred Roosta, University of Queensland https://simons.berkeley.edu/talks/sec... Foundations of Data Science Boot Camp}
}

@article{simonyanVeryDeepConvolutional2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large-Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = {2015},
  month = apr,
  journal = {arXiv:1409.1556 [cs]},
  eprint = {1409.1556},
  primaryclass = {cs},
  urldate = {2022-01-31},
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  file = {/home/luisaam/Zotero/storage/NZT3G39R/Simonyan y Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf;/home/luisaam/Zotero/storage/XGTEN8S6/1409.html}
}

@inproceedings{singhWoodFisherEfficientSecondOrder2020,
  title = {{{WoodFisher}}: {{Efficient Second-Order Approximation}} for {{Neural Network Compression}}},
  shorttitle = {{{WoodFisher}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Singh, Sidak Pal and Alistarh, Dan},
  year = {2020},
  volume = {33},
  pages = {18098--18109},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-08-03},
  abstract = {Second-order information, in the form of Hessian- or Inverse-Hessian-vector products, is a fundamental tool for solving optimization problems. Recently, there has been significant interest in utilizing this information in the context of deep neural networks; however, relatively little is known about the quality of existing approximations in this context. Our work considers this question, examines the accuracy of existing approaches, and proposes a method called WoodFisher to compute a faithful and efficient estimate of the inverse Hessian.},
  keywords = {Neural Magic,Pruning},
  file = {/home/luisaam/Zotero/storage/EPNHPNAE/Singh y Alistarh - 2020 - WoodFisher Efficient Second-Order Approximation f.pdf}
}

@misc{skorokhodovLossLandscapeSightseeing2019,
  title = {Loss {{Landscape Sightseeing}} with {{Multi-Point Optimization}}},
  author = {Skorokhodov, Ivan and Burtsev, Mikhail},
  year = {2019},
  month = oct,
  number = {arXiv:1910.03867},
  eprint = {1910.03867},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1910.03867},
  urldate = {2023-12-03},
  abstract = {We present multi-point optimization: an optimization technique that allows to train several models simultaneously without the need to keep the parameters of each one individually. The proposed method is used for a thorough empirical analysis of the loss landscape of neural networks. By extensive experiments on FashionMNIST and CIFAR10 datasets we demonstrate two things: 1) loss surface is surprisingly diverse and intricate in terms of landscape patterns it contains, and 2) adding batch normalization makes it more smooth. Source code to reproduce all the reported results is available on GitHub: https://github.com/universome/loss-patterns.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Second Order,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/SGWUCMDT/Skorokhodov and Burtsev - 2019 - Loss Landscape Sightseeing with Multi-Point Optimi.pdf;/home/luisaam/Zotero/storage/XW6VHRUX/1910.html}
}

@inproceedings{soEvolvedTransformer2019,
  title = {The {{Evolved Transformer}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {So, David and Le, Quoc and Liang, Chen},
  year = {2019},
  month = may,
  pages = {5877--5886},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2021-08-15},
  langid = {english},
  keywords = {Interactions,small networks},
  file = {/home/luisaam/Zotero/storage/TLIIZWQ2/So et al. - 2019 - The Evolved Transformer.pdf;/home/luisaam/Zotero/storage/X9Y267UL/So et al. - 2019 - The Evolved Transformer.pdf}
}

@misc{sokarDynamicSparseTraining2022,
  title = {Dynamic {{Sparse Training}} for {{Deep Reinforcement Learning}}},
  author = {Sokar, Ghada and Mocanu, Elena and Mocanu, Decebal Constantin and Pechenizkiy, Mykola and Stone, Peter},
  year = {2022},
  month = may,
  number = {arXiv:2106.04217},
  eprint = {2106.04217},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2106.04217},
  urldate = {2022-12-14},
  abstract = {Deep reinforcement learning (DRL) agents are trained through trial-and-error interactions with the environment. This leads to a long training time for dense neural networks to achieve good performance. Hence, prohibitive computation and memory resources are consumed. Recently, learning efficient DRL agents has received increasing attention. Yet, current methods focus on accelerating inference time. In this paper, we introduce for the first time a dynamic sparse training approach for deep reinforcement learning to accelerate the training process. The proposed approach trains a sparse neural network from scratch and dynamically adapts its topology to the changing data distribution during training. Experiments on continuous control tasks show that our dynamic sparse agents achieve higher performance than the equivalent dense methods, reduce the parameter count and floating-point operations (FLOPs) by 50\%, and have a faster learning speed that enables reaching the performance of dense agents with 40-50\% reduction in the training steps.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,dynamical sparse training,Pruning,reinforcement learning},
  file = {/home/luisaam/Zotero/storage/UMC54QCW/Sokar et al. - 2022 - Dynamic Sparse Training for Deep Reinforcement Lea.pdf;/home/luisaam/Zotero/storage/BD2WBXQR/2106.html}
}

@article{sokolicRobustLargeMargin2017,
  title = {Robust Large Margin Deep Neural Networks},
  author = {Sokoli{\'c}, Jure and Giryes, Raja and Sapiro, Guillermo and Rodrigues, Miguel RD},
  year = {2017},
  journal = {IEEE Transactions on Signal Processing},
  volume = {65},
  number = {16},
  pages = {4265--4280},
  publisher = {{IEEE}},
  keywords = {large margin neural networks,Sobolev training}
}

@article{soltanolkotabiTheoreticalInsightsOptimization2019,
  title = {Theoretical {{Insights Into}} the {{Optimization Landscape}} of {{Over-Parameterized Shallow Neural Networks}}},
  author = {Soltanolkotabi, Mahdi and Javanmard, Adel and Lee, Jason D.},
  year = {2019},
  month = feb,
  journal = {IEEE Transactions on Information Theory},
  volume = {65},
  number = {2},
  pages = {742--769},
  issn = {1557-9654},
  doi = {10.1109/TIT.2018.2854560},
  abstract = {In this paper, we study the problem of learning a shallow artificial neural network that best fits a training data set. We study this problem in the over-parameterized regime where the numbers of observations are fewer than the number of parameters in the model. We show that with the quadratic activations, the optimization landscape of training, such shallow neural networks, has certain favorable characteristics that allow globally optimal models to be found efficiently using a variety of local search heuristics. This result holds for an arbitrary training data of input/output pairs. For differentiable activation functions, we also show that gradient descent, when suitably initialized, converges at a linear rate to a globally optimal model. This result focuses on a realizable model where the inputs are chosen i.i.d. from a Gaussian distribution and the labels are generated according to planted weight coefficients.},
  keywords = {Biological neural networks,Convergence,Data models,Loss Landscape,Nonconvex optimization,Numerical models,Optimization,over-parametrized neural networks,random matrix theory,small networks,Training},
  file = {/home/luisaam/Zotero/storage/AD2D3X7Y/Soltanolkotabi et al. - 2019 - Theoretical Insights Into the Optimization Landsca.pdf;/home/luisaam/Zotero/storage/ZWFGW7X3/8409482.html}
}

@inproceedings{songAutoIntAutomaticFeature2019,
  title = {{{AutoInt}}: {{Automatic Feature Interaction Learning}} via {{Self-Attentive Neural Networks}}},
  shorttitle = {{{AutoInt}}},
  booktitle = {Proceedings of the 28th {{ACM International Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Song, Weiping and Shi, Chence and Xiao, Zhiping and Duan, Zhijian and Xu, Yewen and Zhang, Ming and Tang, Jian},
  year = {2019},
  month = nov,
  eprint = {1810.11921},
  primaryclass = {cs},
  pages = {1161--1170},
  doi = {10.1145/3357384.3357925},
  urldate = {2022-06-17},
  abstract = {Click-through rate (CTR) prediction, which aims to predict the probability of a user clicking on an ad or an item, is critical to many online applications such as online advertising and recommender systems. The problem is very challenging since (1) the input features (e.g., the user id, user age, item id, item category) are usually sparse and high-dimensional, and (2) an effective prediction relies on high-order combinatorial features ({\textbackslash}textit\{a.k.a.\} cross features), which are very time-consuming to hand-craft by domain experts and are impossible to be enumerated. Therefore, there have been efforts in finding low-dimensional representations of the sparse and high-dimensional raw features and their meaningful combinations. In this paper, we propose an effective and efficient method called the {\textbackslash}emph\{AutoInt\} to automatically learn the high-order feature interactions of input features. Our proposed algorithm is very general, which can be applied to both numerical and categorical input features. Specifically, we map both the numerical and categorical features into the same low-dimensional space. Afterwards, a multi-head self-attentive neural network with residual connections is proposed to explicitly model the feature interactions in the low-dimensional space. With different layers of the multi-head self-attentive neural networks, different orders of feature combinations of input features can be modeled. The whole model can be efficiently fit on large-scale raw data in an end-to-end fashion. Experimental results on four real-world datasets show that our proposed approach not only outperforms existing state-of-the-art approaches for prediction but also offers good explainability. Code is available at: {\textbackslash}url\{https://github.com/DeepGraphLearning/RecommenderSystems\}.},
  archiveprefix = {arxiv},
  keywords = {click through prediction,Computer Science - Artificial Intelligence,Computer Science - Information Retrieval,Computer Science - Machine Learning,feature interaction},
  file = {/home/luisaam/Zotero/storage/3Z4MGU7A/Song et al. - 2019 - AutoInt Automatic Feature Interaction Learning vi.pdf;/home/luisaam/Zotero/storage/TAPWW3SJ/1810.html}
}

@article{SonoMyoNetConvolutionalNeural,
  title = {{{SonoMyoNet}}: {{A Convolutional Neural Network}} for {{Predicting Isometric Force From Highly Sparse Ultrasound Images}}},
  journal = {bioRxiv},
  doi = {10.1101/2022.06.10.495590},
  abstract = {Ultrasound imaging or sonomyography has been found to be a robust modality for sensing muscle activity due to its ability to directly image deep-seated muscles while providing superior spatiotemporal specificity compared to surface electromyography- based techniques. Quantifying the morphological changes during muscle activity involves computationally expensive approaches to track muscle anatomical structures or extracting features from B-mode images and A-mode signals. In this paper an offline regression convolutional neural network (CNN) called SonoMyoNet for estimating continuous isometric force from sparse ultrasound scanlines has been presented. SonoMyoNet learns features from a few equispaced scanlines selected from B-mode images and utilizes the learned features to accurately estimate continuous isometric force. The performance of SonoMyoNet was evaluated by varying the number of scanlines to simulate the placement of multiple single element ultrasound transducers in a wearable system. Results showed that SonoMyoNet could accurately predict isometric force with just four scanlines and is immune to speckle noise and shifts in the scanline location. Thus, the proposed network reduces the computational load involved in feature tracking algorithms and estimates muscle force from global features of sparse ultrasound images.},
  keywords = {Researcher App}
}

@article{SpanishDatasetReproducible,
  title = {A {{Spanish}} Dataset for Reproducible Benchmarked Offline Handwriting Recognition},
  journal = {Language Resources and Evaluation},
  doi = {10.1007/s10579-022-09587-3},
  abstract = {In this paper, a public dataset for Offline Handwriting Recognition, along with an appropriate evaluation method to provide benchmark indicators at sentence level, is presented. This dataset, called SPA-Sentences, consists of offline handwritten Spanish sentences extracted from 1617 forms produced by the same number of writers. A total of 13,691 sentences comprising around 100,000 word instances out of a vocabulary of 3288 words occur in the collection. Careful attention has been paid to make the baseline experiments both reproducible and competitive. To this end, experiments are based on state-of-the-art recognition techniques combining convolutional blocks with one-dimensional Bidirectional Long Short Term Memory (LSTM) networks using Connectionist Temporal Classification (CTC) decoding. The scripts with the entire experimental setting have been made available. The SPA-Sentences dataset and its baseline evaluation are freely available for research purposes via the institutional University repository. We expect the research community to include this corpus, as is usually done with English IAM and French RIMES datasets, in their battery of experiments when reporting novel handwriting recognition techniques.},
  keywords = {Researcher App}
}

@article{SparseDeepNeural,
  title = {Sparse {{Deep Neural Network}} for {{Nonlinear Partial Differential Equations}}. ({{arXiv}}:2207.13266v1 [Math.{{NA}}])},
  journal = {arXiv Computer Science},
  doi = {arXiv:2207.13266v1},
  abstract = {More competent learning models are demanded for data processing due to increasingly greater amounts of data available in applications. Data that we encounter often have certain embedded sparsity structures. That is, if they are represented in an appropriate basis, their energies can concentrate on a small number of basis functions. This paper is devoted to a numerical study of adaptive approximation of solutions of nonlinear partial differential equations whose solutions may have singularities, by deep neural networks (DNNs) with a sparse regularization with multiple parameters. Noting that DNNs have an intrinsic multi-scale structure which is favorable for adaptive representation of functions, by employing a penalty with multiple parameters, we develop DNNs with a multi-scale sparse regularization (SDNN) for effectively representing functions having certain singularities. We then apply the proposed SDNN to numerical solutions of the Burgers equation and the Schr{\textbackslash}"odinger equation. Numerical examples confirm that solutions generated by the proposed SDNN are sparse and accurate.},
  keywords = {Researcher App}
}

@misc{SpectralPruningFully,
  title = {Spectral Pruning of Fully Connected Layers | {{Scientific Reports}}},
  urldate = {2022-10-28},
  howpublished = {https://www.nature.com/articles/s41598-022-14805-7},
  keywords = {Pruning,second order},
  file = {/home/luisaam/Zotero/storage/XTRZRP2P/s41598-022-14805-7.html}
}

@article{srivastavaDropoutSimpleWay2014,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  shorttitle = {Dropout},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  year = {2014},
  journal = {Journal of Machine Learning Research},
  volume = {15},
  number = {56},
  pages = {1929--1958},
  issn = {1533-7928},
  urldate = {2021-08-18},
  abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different {\^a}ÂÂthinned{\^a}ÂÂ networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
  keywords = {examples},
  file = {/home/luisaam/Zotero/storage/NWQIXLAM/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks f.pdf}
}

@article{stanleyDesigningNeuralNetworks2019,
  title = {Designing Neural Networks through Neuroevolution},
  author = {Stanley, Kenneth O. and Clune, Jeff and Lehman, Joel and Miikkulainen, Risto},
  year = {2019},
  month = jan,
  journal = {Nature Machine Intelligence},
  volume = {1},
  number = {1},
  pages = {24--35},
  publisher = {{Nature Publishing Group}},
  issn = {2522-5839},
  doi = {10.1038/s42256-018-0006-z},
  urldate = {2021-05-04},
  abstract = {Much of recent machine learning has focused on deep learning, in which neural network weights are trained through variants of stochastic gradient descent. An alternative approach comes from the field of neuroevolution, which harnesses evolutionary algorithms to optimize neural networks, inspired by the fact that natural brains themselves are the products of an evolutionary process. Neuroevolution enables important capabilities that are typically unavailable to gradient-based approaches, including learning neural network building blocks (for example activation functions), hyperparameters, architectures and even the algorithms for learning themselves. Neuroevolution also differs from deep learning (and deep reinforcement learning) by maintaining a population of solutions during search, enabling extreme exploration and massive parallelization. Finally, because neuroevolution research has (until recently) developed largely in isolation from gradient-based neural network research, it has developed many unique and effective techniques that should be effective in other machine learning areas too. This Review looks at several key aspects of modern neuroevolution, including large-scale computing, the benefits of novelty and diversity, the power of indirect encoding, and the field's contributions to meta-learning and architecture search. Our hope is to inspire renewed interest in the field as it meets the potential of the increasing computation available today, to highlight how many of its ideas can provide an exciting resource for inspiration and hybridization to the deep learning, deep reinforcement learning and machine learning communities, and to explain how neuroevolution could prove to be a critical tool in the long-term pursuit of artificial general intelligence.},
  copyright = {2019 Springer Nature Limited},
  langid = {english},
  keywords = {Neuroevolution},
  file = {/home/luisaam/Zotero/storage/2KIZIBTR/Stanley et al. - 2019 - Designing neural networks through neuroevolution.pdf;/home/luisaam/Zotero/storage/XAL7D4J7/s42256-018-0006-z.html}
}

@article{StochasticMaximumPrinciple,
  title = {A {{Stochastic Maximum Principle Approach}} for {{Reinforcement Learning}} with {{Parameterized Environment}}},
  journal = {Journal of Computational Physics},
  doi = {10.1016/j.jcp.2023.112238},
  abstract = {In this work, we introduce a stochastic maximum principle (SMP) approach for solving the reinforcement learning problem with the assumption that the unknowns in the environment can be parameterized based on physics knowledge. For the development of numerical algorithms, we apply an effective online parameter estimation method as our exploration technique to estimate the environment parameter during the training procedure, and the exploitation for the optimal policy is achieved by an efficient backward action learning method for policy improvement under the SMP framework. Numerical experiments are presented to demonstrate that the SMP approach for reinforcement learning can produce reliable control policy, and the gradient descent type optimization in the SMP solver requires less training episodes compared with the standard dynamic programming principle based methods.},
  keywords = {Researcher App}
}

@article{strubellEnergyPolicyConsiderations2020,
  title = {Energy and {{Policy Considerations}} for {{Modern Deep Learning Research}}},
  author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {09},
  pages = {13693--13696},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i09.7123},
  urldate = {2022-12-17},
  abstract = {The field of artificial intelligence has experienced a dramatic methodological shift towards large neural networks trained on plentiful data. This shift has been fueled by recent advances in hardware and techniques enabling remarkable levels of computation, resulting in impressive advances in AI across many applications. However, the massive computation required to obtain these exciting results is costly both financially, due to the price of specialized hardware and electricity or cloud compute time, and to the environment, as a result of non-renewable energy used to fuel modern tensor processing hardware. In a paper published this year at ACL, we brought this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training and tuning neural network models for NLP (Strubell, Ganesh, and McCallum 2019). In this extended abstract, we briefly summarize our findings in NLP, incorporating updated estimates and broader information from recent related publications, and provide actionable recommendations to reduce costs and improve equity in the machine learning and artificial intelligence community.},
  copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {AI,Pruning},
  file = {/home/luisaam/Zotero/storage/KM6XWX3S/Strubell et al. - 2020 - Energy and Policy Considerations for Modern Deep L.pdf}
}

@article{strubellEnergyPolicyConsiderations2020a,
  title = {Energy and {{Policy Considerations}} for {{Modern Deep Learning Research}}},
  author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  year = {2020},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {34},
  number = {09},
  pages = {13693--13696},
  issn = {2374-3468},
  doi = {10.1609/aaai.v34i09.7123},
  urldate = {2022-11-07},
  abstract = {The field of artificial intelligence has experienced a dramatic methodological shift towards large neural networks trained on plentiful data. This shift has been fueled by recent advances in hardware and techniques enabling remarkable levels of computation, resulting in impressive advances in AI across many applications. However, the massive computation required to obtain these exciting results is costly both financially, due to the price of specialized hardware and electricity or cloud compute time, and to the environment, as a result of non-renewable energy used to fuel modern tensor processing hardware. In a paper published this year at ACL, we brought this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training and tuning neural network models for NLP (Strubell, Ganesh, and McCallum 2019). In this extended abstract, we briefly summarize our findings in NLP, incorporating updated estimates and broader information from recent related publications, and provide actionable recommendations to reduce costs and improve equity in the machine learning and artificial intelligence community.},
  copyright = {Copyright (c) 2020 Association for the Advancement of Artificial Intelligence},
  langid = {english},
  keywords = {green ai},
  file = {/home/luisaam/Zotero/storage/X48BJP32/Strubell et al. - 2020 - Energy and Policy Considerations for Modern Deep L.pdf}
}

@article{suchDeepNeuroevolutionGenetic2018,
  title = {Deep {{Neuroevolution}}: {{Genetic Algorithms Are}} a {{Competitive Alternative}} for {{Training Deep Neural Networks}} for {{Reinforcement Learning}}},
  shorttitle = {Deep {{Neuroevolution}}},
  author = {Such, Felipe Petroski and Madhavan, Vashisht and Conti, Edoardo and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
  year = {2018},
  month = apr,
  journal = {arXiv:1712.06567 [cs]},
  eprint = {1712.06567},
  primaryclass = {cs},
  urldate = {2021-07-01},
  abstract = {Deep artificial neural networks (DNNs) are typically trained via gradient-based learning algorithms, namely backpropagation. Evolution strategies (ES) can rival backprop-based algorithms such as Q-learning and policy gradients on challenging deep reinforcement learning (RL) problems. However, ES can be considered a gradient-based algorithm because it performs stochastic gradient descent via an operation similar to a finite-difference approximation of the gradient. That raises the question of whether non-gradient-based evolutionary algorithms can work at DNN scales. Here we demonstrate they can: we evolve the weights of a DNN with a simple, gradient-free, population-based genetic algorithm (GA) and it performs well on hard deep RL problems, including Atari and humanoid locomotion. The Deep GA successfully evolves networks with over four million free parameters, the largest neural networks ever evolved with a traditional evolutionary algorithm. These results (1) expand our sense of the scale at which GAs can operate, (2) suggest intriguingly that in some cases following the gradient is not the best choice for optimizing performance, and (3) make immediately available the multitude of neuroevolution techniques that improve performance. We demonstrate the latter by showing that combining DNNs with novelty search, which encourages exploration on tasks with deceptive or sparse reward functions, can solve a high-dimensional problem on which reward-maximizing algorithms (e.g.{\textbackslash} DQN, A3C, ES, and the GA) fail. Additionally, the Deep GA is faster than ES, A3C, and DQN (it can train Atari in \$\{{\textbackslash}raise.17ex{\textbackslash}hbox\{\${\textbackslash}scriptstyle{\textbackslash}sim\$\}\}\$4 hours on one desktop or \$\{{\textbackslash}raise.17ex{\textbackslash}hbox\{\${\textbackslash}scriptstyle{\textbackslash}sim\$\}\}\$1 hour distributed on 720 cores), and enables a state-of-the-art, up to 10,000-fold compact encoding technique.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,RL and neuroevolution,Why ES in neural networks},
  file = {/home/luisaam/Zotero/storage/3VPRSDU4/Such et al. - 2018 - Deep Neuroevolution Genetic Algorithms Are a Comp.pdf;/home/luisaam/Zotero/storage/I3YQ3U7W/1712.html}
}

@article{sunBoostingAntColony2022,
  title = {Boosting Ant Colony Optimization via Solution Prediction and Machine Learning},
  author = {Sun, Yuan and Wang, Sheng and Shen, Yunzhuang and Li, Xiaodong and Ernst, Andreas T and Kirley, Michael},
  year = {2022},
  journal = {Computers \& Operations Research},
  volume = {143},
  pages = {105769},
  publisher = {{Elsevier}},
  keywords = {learning in optimization}
}

@article{sunUsingStatisticalMeasures2021,
  title = {Using {{Statistical Measures}} and {{Machine Learning}} for {{Graph Reduction}} to {{Solve Maximum Weight Clique Problems}}},
  author = {Sun, Yuan and Li, Xiaodong and Ernst, Andreas},
  year = {2021},
  month = may,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {43},
  number = {5},
  pages = {1746--1760},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2019.2954827},
  abstract = {In this article, we investigate problem reduction techniques using stochastic sampling and machine learning to tackle large-scale optimization problems. These techniques heuristically remove decision variables from the problem instance, that are not expected to be part of an optimal solution. First we investigate the use of statistical measures computed from stochastic sampling of feasible solutions compared with features computed directly from the instance data. Two measures are particularly useful for this: 1) a ranking-based measure, favoring decision variables that frequently appear in high-quality solutions; and 2) a correlation-based measure, favoring decision variables that are highly correlated with the objective values. To take this further we develop a machine learning approach, called Machine Learning for Problem Reduction (MLPR), that trains a supervised learning model on easy problem instances for which the optimal solution is known. This gives us a combination of features enabling us to better predict the decision variables that belong to the optimal solution for a given hard problem. We evaluate our approaches using a typical optimization problem on graphs-the maximum weight clique problem. The experimental results show our problem reduction techniques are very effective and can be used to boost the performance of existing solution methods.},
  keywords = {Atmospheric measurements,Combinatorial optimization,data mining,Heuristic algorithms,machine learning,Machine learning,Machine learning algorithms,Optimization,Particle measurements,problem reduction,Search problems,statistics},
  file = {/home/luisaam/Zotero/storage/R9BQ7QQ9/Sun et al. - 2021 - Using Statistical Measures and Machine Learning fo.pdf;/home/luisaam/Zotero/storage/A9HHFDFG/8908752.html}
}

@article{SuperposingManyTickets,
  title = {Superposing {{Many Tickets}} into {{One}}: {{A Performance Booster}} for {{Sparse Neural Network Training}}. ({{arXiv}}:2205.15322v1 [Cs.{{LG}}])},
  journal = {arXiv Computer Science},
  doi = {arXiv:2205.15322v1},
  abstract = {Recent works on sparse neural network training (sparse training) have shown that a compelling trade-off between performance and efficiency can be achieved by training intrinsically sparse neural networks from scratch. Existing sparse training methods usually strive to find the best sparse subnetwork possible in one single run, without involving any expensive dense or pre-training steps. For instance, dynamic sparse training (DST), as one of the most prominent directions, is capable of reaching a competitive performance of dense training by iteratively evolving the sparse topology during the course of training. In this paper, we argue that it is better to allocate the limited resources to create multiple low-loss sparse subnetworks and superpose them into a stronger one, instead of allocating all resources entirely to find an individual subnetwork. To achieve this, two desiderata are required: (1) efficiently producing many low-loss subnetworks, the so-called cheap tickets, within one training process limited to the standard training time used in dense training; (2) effectively superposing these cheap tickets into one stronger subnetwork without going over the constrained parameter budget. To corroborate our conjecture, we present a novel sparse training approach, termed {\textbackslash}textbf\{Sup-tickets\}, which can satisfy the above two desiderata concurrently in a single sparse-to-sparse training process. Across various modern architectures on CIFAR-10/100 and ImageNet, we show that Sup-tickets integrates seamlessly with the existing sparse training methods and demonstrates consistent performance improvement.},
  keywords = {Researcher App}
}

@article{SurpriseSavioursGut,
  title = {The Surprise Saviours of the Gut: The Neurons That Sense Pain},
  journal = {Nature},
  doi = {10.1038/d41586-022-03320-4},
  abstract = {Nerve cells that carry pain signals play a part in suppressing inflammation and tissue damage in the gut. Nerve cells that carry pain signals play a part in suppressing inflammation and tissue damage in the gut.},
  keywords = {Researcher App}
}

@article{SurveyEfficientConvolutional,
  title = {A {{Survey}} on {{Efficient Convolutional Neural Networks}} and {{Hardware Acceleration}}},
  journal = {Electronics},
  doi = {10.3390/electronics11060945},
  abstract = {Over the past decade, deep-learning-based representations have demonstrated remarkable performance in academia and industry. The learning capability of convolutional neural networks (CNNs) originates from a combination of various feature extraction layers that fully utilize a large amount of data. However, they often require substantial computation and memory resources while replacing traditional hand-engineered features in existing systems. In this review, to improve the efficiency of deep learning research, we focus on three aspects: quantized/binarized models, optimized architectures, and resource-constrained systems. Recent advances in light-weight deep learning models and network architecture search (NAS) algorithms are reviewed, starting with simplified layers and efficient convolution and including new architectural design and optimization. In addition, several practical applications of efficient CNNs have been investigated using various types of hardware architectures and platforms.},
  keywords = {Researcher App}
}

@inproceedings{sutskeverImportanceInitializationMomentum2013,
  title = {On the Importance of Initialization and Momentum in Deep Learning},
  booktitle = {International Conference on Machine Learning},
  author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  year = {2013},
  pages = {1139--1147},
  publisher = {{PMLR}}
}

@inproceedings{tanakaPruningNeuralNetworks2020,
  title = {Pruning Neural Networks without Any Data by Iteratively Conserving Synaptic Flow},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Tanaka, Hidenori and Kunin, Daniel and Yamins, Daniel L and Ganguli, Surya},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
  year = {2020},
  volume = {33},
  pages = {6377--6389},
  publisher = {{Curran Associates, Inc.}},
  keywords = {Interactions,Pruning,small networks},
  file = {/home/luisaam/Zotero/storage/H49BYK65/Tanaka et al. - 2020 - Pruning neural networks without any data by iterat.pdf}
}

@inproceedings{tanEfficientNetRethinkingModel2019,
  title = {{{EfficientNet}}: {{Rethinking Model Scaling}} for {{Convolutional Neural Networks}}},
  shorttitle = {{{EfficientNet}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Tan, Mingxing and Le, Quoc},
  year = {2019},
  month = may,
  pages = {6105--6114},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-12-18},
  abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are given. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on MobileNets and ResNet. To go even further, we use neural architecture search to design a new baseline network and scale it up to obtain a family of models, called EfficientNets, which achieve much better accuracy and efficiency than previous ConvNets. In particular, our EfficientNet-B7 achieves stateof-the-art 84.4\% top-1 / 97.1\% top-5 accuracy on ImageNet, while being 8.4x smaller and 6.1x faster on inference than the best existing ConvNet (Huang et al., 2018). Our EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7\%), Flower (98.8\%), and 3 other transfer learning datasets, with an order of magnitude fewer parameters.},
  langid = {english},
  keywords = {example,small networks},
  file = {/home/luisaam/Zotero/storage/I8NWGRBT/Tan y Le - 2019 - EfficientNet Rethinking Model Scaling for Convolu.pdf}
}

@article{tanReviewSecondorderOptimization2019,
  title = {Review of Second-Order Optimization Techniques in Artificial Neural Networks Backpropagation},
  author = {Tan, Hong Hui and Lim, King Hann},
  year = {2019},
  month = jun,
  journal = {IOP Conference Series: Materials Science and Engineering},
  volume = {495},
  pages = {012003},
  publisher = {{IOP Publishing}},
  issn = {1757-899X},
  doi = {10.1088/1757-899X/495/1/012003},
  urldate = {2021-06-22},
  abstract = {Second-order optimization technique is the advances of first-order optimization in neural networks. It provides an addition curvature information of an objective function that adaptively estimate the step-length of optimization trajectory in training phase of neural network. With the additional information, it reduces training iteration and achieves fast convergence with less tuning of hyper-parameter. The current improved memory allocation and computing power further motivates machine learning practitioners to revisit the benefits of second-order optimization techniques. This paper covers the review on second-order optimization techniques that involve Hessian calculation for neural network training. It reviews the basic theory of Newton method, quasi-Newton, Gauss-Newton, Levenberg-Marquardt, Approximate Greatest Descent and Hessian-Free optimization. This paper summarizes the feasibility and performance of optimization techniques in deep neural network training. Comments and suggestions are highlighted for second-order optimization techniques in artificial neural network training in term of advantages and limitations.},
  langid = {english},
  file = {/home/luisaam/Zotero/storage/T4VGJ4LP/Tan y Lim - 2019 - Review of second-order optimization techniques in .pdf}
}

@article{tesseraKeepGradientsFlowing2021,
  title = {Keep the Gradients Flowing: {{Using}} Gradient Flow to Study Sparse Network Optimization},
  author = {Tessera, Kale-ab and Hooker, Sara and Rosman, Benjamin},
  year = {2021},
  journal = {arXiv preprint arXiv:2102.01670},
  eprint = {2102.01670},
  archiveprefix = {arxiv},
  keywords = {Pruning}
}

@article{thompsonComputationalLimitsDeep2020,
  title = {The {{Computational Limits}} of {{Deep Learning}}},
  author = {Thompson, Neil C. and Greenewald, Kristjan and Lee, Keeheon and Manso, Gabriel F.},
  year = {2020},
  month = jul,
  journal = {arXiv:2007.05558 [cs, stat]},
  eprint = {2007.05558},
  primaryclass = {cs, stat},
  urldate = {2021-08-15},
  abstract = {Deep learning's recent history has been one of achievement: from triumphing over humans in the game of Go to world-leading performance in image recognition, voice recognition, translation, and other tasks. But this progress has come with a voracious appetite for computing power. This article reports on the computational demands of Deep Learning applications in five prominent application areas and shows that progress in all five is strongly reliant on increases in computing power. Extrapolating forward this reliance reveals that progress along current lines is rapidly becoming economically, technically, and environmentally unsustainable. Thus, continued progress in these applications will require dramatically more computationally-efficient methods, which will either have to come from changes to deep learning or from moving to other machine learning methods.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,small networks,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/TGMEK6JR/Thompson et al. - 2020 - The Computational Limits of Deep Learning.pdf;/home/luisaam/Zotero/storage/YIDFS5ZF/2007.html}
}

@techreport{thompsonDeclineComputersGeneral2018,
  type = {{{SSRN Scholarly Paper}}},
  title = {The {{Decline}} of {{Computers As}} a {{General Purpose Technology}}: {{Why Deep Learning}} and the {{End}} of {{Moore}}'s {{Law}} Are {{Fragmenting Computing}}},
  shorttitle = {The {{Decline}} of {{Computers As}} a {{General Purpose Technology}}},
  author = {Thompson, Neil and Spanuth, Svenja},
  year = {2018},
  month = nov,
  number = {ID 3287769},
  address = {{Rochester, NY}},
  institution = {{Social Science Research Network}},
  doi = {10.2139/ssrn.3287769},
  urldate = {2021-12-14},
  abstract = {It is a triumph of technology and of economics that our computer chips are so universal. Countless applications are only possible because of the staggering variety of calculations that modern chips can compute. But, this was not always the case. Computers used to be specialized, doing only narrow sets of calculations. Their rise as a 'general purpose technology (GPT)' only happened because of the technical breakthroughs by computer scientists like von Neumann and Turing, and the mutually-reinforcing economic cycle of general purpose technologies, where product improvement and market growth fuel each other. This paper argues that technological and economic forces are now pushing computing in the opposite direction, making computer processors less general purpose and more specialized. This process has already begun, driven by the slow down in Moore's Law and the algorithmic success of Deep Learning. This trend towards specialization threatens to fragment computing into 'fast lane' applications that get powerful customized chips and 'slow lane' applications that get stuck using general purpose chips whose progress fades.The rise of general purpose computer chips has been remarkable. So, too, could be their fall. This paper outlines the forces already starting to fragment this general purpose technology.},
  langid = {english},
  keywords = {Computer Chips,CPU,Deep Learning,Economics of I.T.,Efficiency,General Purpose Technology,GPU,green ai,Information Technology,Moore's Law,Processors},
  file = {/home/luisaam/Zotero/storage/WEQV7K5G/Thompson y Spanuth - 2018 - The Decline of Computers As a General Purpose Tech.pdf;/home/luisaam/Zotero/storage/VH56ZYTB/papers.html}
}

@article{tolstikhinMLPMixerAllMLPArchitecture2021,
  title = {{{MLP-Mixer}}: {{An}} All-{{MLP Architecture}} for {{Vision}}},
  shorttitle = {{{MLP-Mixer}}},
  author = {Tolstikhin, Ilya and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},
  year = {2021},
  month = jun,
  journal = {arXiv:2105.01601 [cs]},
  eprint = {2105.01601},
  primaryclass = {cs},
  urldate = {2021-08-22},
  abstract = {Convolutional Neural Networks (CNNs) are the go-to model for computer vision. Recently, attention-based networks, such as the Vision Transformer, have also become popular. In this paper we show that while convolutions and attention are both sufficient for good performance, neither of them are necessary. We present MLP-Mixer, an architecture based exclusively on multi-layer perceptrons (MLPs). MLP-Mixer contains two types of layers: one with MLPs applied independently to image patches (i.e. "mixing" the per-location features), and one with MLPs applied across patches (i.e. "mixing" spatial information). When trained on large datasets, or with modern regularization schemes, MLP-Mixer attains competitive scores on image classification benchmarks, with pre-training and inference cost comparable to state-of-the-art models. We hope that these results spark further research beyond the realms of well established CNNs and Transformers.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Interactions},
  file = {/home/luisaam/Zotero/storage/IPMP3ZF3/Tolstikhin et al. - 2021 - MLP-Mixer An all-MLP Architecture for Vision.pdf;/home/luisaam/Zotero/storage/K4UVCVIL/2105.html}
}

@inproceedings{touvronGoingDeeperImage2021,
  title = {Going {{Deeper With Image Transformers}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Touvron, Hugo and Cord, Matthieu and Sablayrolles, Alexandre and Synnaeve, Gabriel and J{\'e}gou, Herv{\'e}},
  year = {2021},
  pages = {32--42},
  urldate = {2023-03-31},
  langid = {english},
  keywords = {Efficiency,Pruning},
  file = {/home/luisaam/Zotero/storage/YB4MEQHX/Touvron et al. - 2021 - Going Deeper With Image Transformers.pdf}
}

@misc{tripathiRSOGradientFree2020,
  title = {{{RSO}}: {{A Gradient Free Sampling Based Approach For Training Deep Neural Networks}}},
  shorttitle = {{{RSO}}},
  author = {Tripathi, Rohun and Singh, Bharat},
  year = {2020},
  month = may,
  number = {arXiv:2005.05955},
  eprint = {2005.05955},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2005.05955},
  urldate = {2022-07-22},
  abstract = {We propose RSO (random search optimization), a gradient free Markov Chain Monte Carlo search based approach for training deep neural networks. To this end, RSO adds a perturbation to a weight in a deep neural network and tests if it reduces the loss on a mini-batch. If this reduces the loss, the weight is updated, otherwise the existing weight is retained. Surprisingly, we find that repeating this process a few times for each weight is sufficient to train a deep neural network. The number of weight updates for RSO is an order of magnitude lesser when compared to backpropagation with SGD. RSO can make aggressive weight updates in each step as there is no concept of learning rate. The weight update step for individual layers is also not coupled with the magnitude of the loss. RSO is evaluated on classification tasks on MNIST and CIFAR-10 datasets with deep neural networks of 6 to 10 layers where it achieves an accuracy of 99.1\% and 81.8\% respectively. We also find that after updating the weights just 5 times, the algorithm obtains a classification accuracy of 98\% on MNIST.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,possible improvement,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/ATQ3REMF/Tripathi y Singh - 2020 - RSO A Gradient Free Sampling Based Approach For T.pdf;/home/luisaam/Zotero/storage/KJB6YPVA/2005.html}
}

@article{TrulySparseNeural,
  title = {Truly {{Sparse Neural Networks}} at {{Scale}}. ({{arXiv}}:2102.01732v2 [Cs.{{LG}}] {{UPDATED}})},
  journal = {arXiv Computer Science},
  doi = {arXiv:2102.01732v2},
  abstract = {Recently, sparse training methods have started to be established as a de facto approach for training and inference efficiency in artificial neural networks. Yet, this efficiency is just in theory. In practice, everyone uses a binary mask to simulate sparsity since the typical deep learning software and hardware are optimized for dense matrix operations. In this paper, we take an orthogonal approach, and we show that we can train truly sparse neural networks to harvest their full potential. To achieve this goal, we introduce three novel contributions, specially designed for sparse neural networks: (1) a parallel training algorithm and its corresponding sparse implementation from scratch, (2) an activation function with non-trainable parameters to favour the gradient flow, and (3) a hidden neurons importance metric to eliminate redundancies. All in one, we are able to break the record and to train the largest neural network ever trained in terms of representational power -- reaching the bat brain size. The results show that our approach has state-of-the-art performance while opening the path for an environmentally friendly artificial intelligence era.},
  keywords = {Researcher App}
}

@article{TwothirdsScientistsWant,
  title = {Two-Thirds of Scientists Want to Keep Alcohol at Conferences},
  journal = {Nature},
  doi = {10.1038/d41586-022-00703-5},
  abstract = {Nature, Published online: 11 March 2022; doi:10.1038/d41586-022-00703-5  Alcohol creates a relaxed, social atmosphere at conferences, say survey respondents, but more care could be taken to ensure everyone feels safe and included.},
  keywords = {Researcher App}
}

@article{UnderstandingWhyMaskReconstruction,
  title = {Towards {{Understanding Why Mask-Reconstruction Pretraining Helps}} in {{Downstream Tasks}}. ({{arXiv}}:2206.03826v2 [Cs.{{LG}}] {{UPDATED}})},
  journal = {arXiv Computer Vision and Pattern Recognition},
  doi = {arXiv:2206.03826v2},
  abstract = {For unsupervised pretraining, mask-reconstruction pretraining (MRP) approaches randomly mask input patches and then reconstruct pixels or semantic features of these masked patches via an auto-encoder. Then for a downstream task, supervised fine-tuning the pretrained encoder remarkably surpasses the conventional supervised learning (SL) trained from scratch. However, it is still unclear 1) how MRP performs semantic learning in the pretraining phase and 2) why it helps in downstream tasks. To solve these problems, we theoretically show that on an auto-encoder of a two/one-layered convolution encoder/decoder, MRP can capture all discriminative semantics in the pretraining dataset, and accordingly show its provable improvement over SL on the classification downstream task. Specifically, we assume that pretraining dataset contains multi-view samples of ratio  and single-view samples of ratio , where multi/single-view samples has multiple/single discriminative semantics. Then for pretraining, we prove that 1) the convolution kernels of the MRP encoder captures all discriminative semantics in the pretraining data; and 2) a convolution kernel captures at most one semantic. Accordingly, in the downstream supervised fine-tuning, most semantics would be captured and different semantics would not be fused together. This helps the downstream fine-tuned network to easily establish the relation between kernels and semantic class labels. In this way, the fine-tuned encoder in MRP provably achieves zero test error with high probability for both multi-view and single-view test data. In contrast, as proved by{\textasciitilde}[3], conventional SL can only obtain a test accuracy between around  for single-view test data. These results together explain the benefits of MRP in downstream tasks. Experimental results testify to multi-view data assumptions and our theoretical implications.},
  keywords = {Researcher App}
}

@article{UsingNeuralNetworks,
  title = {Using Neural Networks with Reinforcement in the Tasks of Forming User Recommendations},
  journal = {Journal of Physics: Conference Series},
  doi = {10.1088/1742-6596/2291/1/012005},
  abstract = {The article discusses the design of an algorithm for the formation of personalized user recommendations. The basis of the algorithm is a recurrent neural network with reinforcement. The use of neural networks in the recommendation algorithm will help to increase persistence as it becomes possible to identify weak relationships of implicit factors.},
  keywords = {Researcher App}
}

@article{vaderaMethodsPruningDeep2022,
  title = {Methods for {{Pruning Deep Neural Networks}}},
  author = {Vadera, Sunil and Ameen, Salem},
  year = {2022},
  journal = {IEEE Access},
  volume = {10},
  pages = {63280--63300},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2022.3182659},
  abstract = {This paper presents a survey of methods for pruning deep neural networks. It begins by categorising over 150 studies based on the underlying approach used and then focuses on three categories: methods that use magnitude based pruning, methods that utilise clustering to identify redundancy, and methods that use sensitivity analysis to assess the effect of pruning. Some of the key influencing studies within these categories are presented to highlight the underlying approaches and results achieved. Most studies present results which are distributed in the literature as new architectures, algorithms and data sets have developed with time, making comparison across different studied difficult. The paper therefore provides a resource for the community that can be used to quickly compare the results from many different methods on a variety of data sets, and a range of architectures, including AlexNet, ResNet, DenseNet and VGG. The resource is illustrated by comparing the results published for pruning AlexNet and ResNet50 on ImageNet and ResNet56 and VGG16 on the CIFAR10 data to reveal which pruning methods work well in terms of retaining accuracy whilst achieving good compression rates. The paper concludes by identifying some research gaps and promising directions for future research.},
  keywords = {Computer architecture,Deep learning,filter similarity,neural networks,Neural networks,Prediction algorithms,Pruning,pruning deep networks,Quantization (signal),Sensitivity analysis,survey,Weight measurement},
  file = {/home/luisaam/Zotero/storage/F8JNAHZM/Vadera and Ameen - 2022 - Methods for Pruning Deep Neural Networks.pdf;/home/luisaam/Zotero/storage/ER3VY5QP/9795013.html}
}

@article{vanniekerkComposingValueFunctions2019,
  title = {Composing Value Functions in Reinforcement Learning},
  author = {VanNiekerk, B. and James, S. and Earle, A. and Rosman, B.},
  year = {2019},
  journal = {36th International Conference on Machine Learning, ICML 2019},
  volume = {2019-},
  publisher = {{International Machine Learning Society (IMLS) rasmussen@ptd.net}},
  abstract = {An important property for lifelong-learning agents is the ability to combine existing skills to solve new unseen tasks. In general, however, it is unclear how to compose existing skills in a principled manner. Under the assumption of deterministic dynamics, we prove that optimal value function composition can be achieved in entropyregularised reinforcement learning (RL), and extend this result to the standard RL setting. Composition is demonstrated in a high-dimensional video game, where an agent with an existing library of skills is immediately able to solve new tasks without the need for further learning.}
}

@article{vargaGradientRegularizationImproves2018,
  title = {Gradient {{Regularization Improves Accuracy}} of {{Discriminative Models}}},
  author = {Varga, D{\'a}niel and Csisz{\'a}rik, Adri{\'a}n and Zombori, Zsolt},
  year = {2018},
  month = jun,
  journal = {Schedae Informaticae},
  volume = {2018},
  number = {Volume 27},
  pages = {31--45},
  publisher = {{Portal Czasopism Naukowych Ejournals.eu}},
  issn = {2083-8476},
  doi = {10.4467/20838476SI.18.003.10408},
  urldate = {2022-05-16},
  abstract = {{$<$}p{$>$} 	Gradient Regularization Improves Accuracy of Discriminative Models{$<$}/p{$>$}},
  langid = {english},
  keywords = {gradients training,Sobolev training},
  file = {/home/luisaam/Zotero/storage/WFNZMU68/Varga et al. - 2018 - Gradient Regularization Improves Accuracy of Discr.pdf;/home/luisaam/Zotero/storage/XYUEAZ2L/13926.html}
}

@inproceedings{vaswaniAttentionAllYou2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2021-08-05},
  keywords = {Fundational,Transformers},
  file = {/home/luisaam/Zotero/storage/9ZWKWNTJ/Vaswani et al. - 2017 - Attention is All you Need.pdf}
}

@article{vinyalsGrandmasterLevelStarCraft2019,
  title = {Grandmaster Level in {{StarCraft II}} Using Multi-Agent Reinforcement Learning},
  author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, R{\'e}mi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and W{\"u}nsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
  year = {2019},
  month = nov,
  journal = {Nature},
  volume = {575},
  number = {7782},
  pages = {350--354},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-019-1724-z},
  urldate = {2021-07-20},
  abstract = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1{\textendash}3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8\% of officially ranked human players. AlphaStar uses a multi-agent reinforcement learning algorithm and has reached Grandmaster level, ranking among the top 0.2\% of human players for the real-time strategy game StarCraft II.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  annotation = {Primary\_atype: Research Subject\_term: Computer science;Statistics Subject\_term\_id: computer-science;statistics},
  file = {/home/luisaam/Zotero/storage/PW9NTG76/Vinyals et al. - 2019 - Grandmaster level in StarCraft II using multi-agen.pdf;/home/luisaam/Zotero/storage/RQ7F52VB/s41586-019-1724-z.html}
}

@article{vinyalsGrandmasterLevelStarCraft2019a,
  title = {Grandmaster Level in {{StarCraft II}} Using Multi-Agent Reinforcement Learning},
  author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, R{\'e}mi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and W{\"u}nsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
  year = {2019},
  month = nov,
  journal = {Nature},
  volume = {575},
  number = {7782},
  pages = {350--354},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/s41586-019-1724-z},
  urldate = {2021-07-02},
  abstract = {Many real-world applications require artificial agents to compete and coordinate with other agents in complex environments. As a stepping stone to this goal, the domain of StarCraft has emerged as an important challenge for artificial intelligence research, owing to its iconic and enduring status among the most difficult professional esports and its relevance to the real world in terms of its raw complexity and multi-agent challenges. Over the course of a decade and numerous competitions1{\textendash}3, the strongest agents have simplified important aspects of the game, utilized superhuman capabilities, or employed hand-crafted sub-systems4. Despite these advantages, no previous agent has come close to matching the overall skill of top StarCraft players. We chose to address the challenge of StarCraft using general-purpose learning methods that are in principle applicable to other complex domains: a multi-agent reinforcement learning algorithm that uses data from both human and agent games within a diverse league of continually adapting strategies and counter-strategies, each represented by deep neural networks5,6. We evaluated our agent, AlphaStar, in the full game of StarCraft II, through a series of online games against human players. AlphaStar was rated at Grandmaster level for all three StarCraft races and above 99.8\% of officially ranked human players. AlphaStar uses a multi-agent reinforcement learning algorithm and has reached Grandmaster level, ranking among the top 0.2\% of human players for the real-time strategy game StarCraft II.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {examples},
  annotation = {Primary\_atype: Research Subject\_term: Computer science;Statistics Subject\_term\_id: computer-science;statistics},
  file = {/home/luisaam/Zotero/storage/7VK8HGMW/Vinyals et al. - 2019 - Grandmaster level in StarCraft II using multi-agen.pdf;/home/luisaam/Zotero/storage/I49IZ7EH/s41586-019-1724-z.html}
}

@article{VisionModelsAre,
  title = {Vision {{Models Are More Robust And Fair When Pretrained On Uncurated Images Without Supervision}}. ({{arXiv}}:2202.08360v1 [Cs.{{CV}}])},
  journal = {arXiv Computer Science},
  doi = {arXiv:2202.08360v1},
  abstract = {Discriminative self-supervised learning allows training models on any random group of internet images, and possibly recover salient information that helps differentiate between the images. Applied to ImageNet, this leads to object centric features that perform on par with supervised features on most object-centric downstream tasks. In this work, we question if using this ability, we can learn any salient and more representative information present in diverse unbounded set of images from across the globe. To do so, we train models on billions of random images without any data pre-processing or prior assumptions about what we want the model to learn. We scale our model size to dense 10 billion parameters to avoid underfitting on a large data size. We extensively study and validate our model performance on over 50 benchmarks including fairness, robustness to distribution shift, geographical diversity, fine grained recognition, image copy detection and many image classification datasets. The resulting model, not only captures well semantic information, it also captures information about artistic style and learns salient information such as geolocations and multilingual word embeddings based on visual content only. More importantly, we discover that such model is more robust, more fair, less harmful and less biased than supervised models or models trained on object centric datasets such as ImageNet.},
  keywords = {Researcher App}
}

@article{vlaarWhatCanLinear2022,
  title = {What Can Linear Interpolation of Neural Network Loss Landscapes Tell Us?},
  author = {Vlaar, Tiffany and Frankle, Jonathan},
  year = {2022},
  month = feb,
  journal = {arXiv:2106.16004 [cs, stat]},
  eprint = {2106.16004},
  primaryclass = {cs, stat},
  urldate = {2022-02-17},
  abstract = {Studying neural network loss landscapes provides insights into the nature of the underlying optimization problems. Unfortunately, loss landscapes are notoriously difficult to visualize in a human-comprehensible fashion. One common way to address this problem is to plot linear slices of the landscape, for example from the initial state of the network to the final state after optimization. On the basis of this analysis, prior work has drawn broader conclusions about the difficulty of the optimization problem. In this paper, we put inferences of this kind to the test, systematically evaluating how linear interpolation and final performance vary when altering the data, choice of initialization, and other optimizer and architecture design choices. Further, we use linear interpolation to study the role played by individual layers and substructures of the network. We find that certain layers are more sensitive to the choice of initialization, but that the shape of the linear path is not indicative of the changes in test accuracy of the model. Our results cast doubt on the broader intuition that the presence or absence of barriers when interpolating necessarily relates to the success of optimization.},
  archiveprefix = {arxiv},
  keywords = {barriers,basin of attraction,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,linear interpolation,Loss Landscape,read,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/YZLL3XL6/Vlaar y Frankle - 2022 - What can linear interpolation of neural network lo.pdf;/home/luisaam/Zotero/storage/IFZVC9ER/2106.html}
}

@article{vysogoretsConnectivityMattersNeural2023,
  title = {Connectivity {{Matters}}: {{Neural Network Pruning Through}} the {{Lens}} of {{Effective Sparsity}}},
  author = {Vysogorets, Artem and Kempe, Julia},
  year = {2023},
  journal = {Journal of Machine Learning Research},
  volume = {24},
  number = {99},
  pages = {1--23},
  abstract = {Neural network pruning is a fruitful area of research with surging interest in high sparsity regimes. Benchmarking in this domain heavily relies on faithful representation of the sparsity of subnetworks, which has been traditionally computed as the fraction of removed connections (direct sparsity). This definition, however, fails to recognize unpruned parameters that detached from input or output layers of the underlying subnetworks, potentially underestimating actual effective sparsity: the fraction of inactivated connections. While this effect might be negligible for moderately pruned networks (up to 10{\texttimes}{\textendash}100{\texttimes} compression rates), we find that it plays an increasing role for sparser subnetworks, greatly distorting comparison between different pruning algorithms. For example, we show that effective compression of a randomly pruned LeNet-300-100 can be orders of magnitude larger than its direct counterpart, while no discrepancy is ever observed when using SynFlow for pruning (Tanaka et al., 2020). In this work, we adopt the lens of effective sparsity to reevaluate several recent pruning algorithms on common benchmark architectures (e.g., LeNet-300-100, VGG-19, ResNet-18) and discover that their absolute and relative performance changes dramatically in this new, and as we argue, more appropriate framework. To aim for effective, rather than direct, sparsity, we develop a low-cost extension to most pruning algorithms. Further, equipped with effective sparsity as a reference frame, we partially reconfirm that random pruning with appropriate sparsity allocation across layers performs as well or better than more sophisticated algorithms for pruning at initialization (Su et al., 2020). In response to this observation, using an analogy of pressure distribution in coupled cylinders from thermodynamics, we design novel layerwise sparsity quotas that outperform all existing baselines in the context of random pruning.},
  langid = {english},
  keywords = {Pruning},
  file = {/home/luisaam/Zotero/storage/7F8C92G9/Vysogorets and Kempe - Connectivity Matters Neural Network Pruning Throu.pdf}
}

@inproceedings{wanEnhancingGeneralizationAbility2001,
  title = {Enhancing the Generalization Ability of Neural Networks by Using {{Gram-Schmidt}} Orthogonalization Algorithm},
  booktitle = {{{IJCNN}}'01. {{International Joint Conference}} on {{Neural Networks}}. {{Proceedings}} ({{Cat}}. {{No}}.{{01CH37222}})},
  author = {Wan, W. and Hirasawa, K. and Hu, J. and Murata, J.},
  year = {2001},
  month = jul,
  volume = {3},
  pages = {1721-1726 vol.3},
  issn = {1098-7576},
  doi = {10.1109/IJCNN.2001.938421},
  abstract = {The generalization ability of neural networks is an important criterion when determining whether one algorithm is powerful or not. Many new algorithms have been devised to enhance the generalization ability of neural networks. In this paper an algorithm using the Gram-Schmidt orthogonalization algorithm on the outputs of nodes in the hidden layers is proposed with the aim to reduce the interference among the nodes in the hidden layers, which is much more efficient than the regularizers methods. Simulation results confirm the above assertion.},
  keywords = {alternatives to SGD,Backpropagation algorithms,Error correction,Information science,Intelligent control,Interference,Laboratories,Neural networks,Neurons,performance gain without computation,Sparse matrices,Weight control},
  file = {/home/luisaam/Zotero/storage/9ZC79W5Y/938421.html}
}

@inproceedings{wangConvolutionalNeuralNetwork2021,
  title = {Convolutional {{Neural Network Pruning With Structural Redundancy Reduction}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Wang, Zi and Li, Chengcheng and Wang, Xiangyang},
  year = {2021},
  pages = {14913--14922},
  urldate = {2023-07-18},
  langid = {english},
  keywords = {filter similarity,Pruning,redundancy},
  file = {/home/luisaam/Zotero/storage/9C5YSQ4B/Wang et al. - 2021 - Convolutional Neural Network Pruning With Structur.pdf}
}

@article{wangDynamicalIsometryMissing2021,
  title = {Dynamical {{Isometry}}: {{The Missing Ingredient}} for {{Neural Network Pruning}}},
  shorttitle = {Dynamical {{Isometry}}},
  author = {Wang, Huan and Qin, Can and Bai, Yue and Fu, Yun},
  year = {2021},
  month = may,
  journal = {arXiv:2105.05916 [cs]},
  eprint = {2105.05916},
  primaryclass = {cs},
  urldate = {2021-08-05},
  abstract = {Several recent works [40, 24] observed an interesting phenomenon in neural network pruning: A larger finetuning learning rate can improve the final performance significantly. Unfortunately, the reason behind it remains elusive up to date. This paper is meant to explain it through the lens of dynamical isometry [42]. Specifically, we examine neural network pruning from an unusual perspective: pruning as initialization for finetuning, and ask whether the inherited weights serve as a good initialization for the finetuning? The insights from dynamical isometry suggest a negative answer. Despite its critical role, this issue has not been well-recognized by the community so far. In this paper, we will show the understanding of this problem is very important -- on top of explaining the aforementioned mystery about the larger finetuning rate, it also unveils the mystery about the value of pruning [5, 30]. Besides a clearer theoretical understanding of pruning, resolving the problem can also bring us considerable performance benefits in practice.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Dynamical Isomerty,Pruning},
  file = {/home/luisaam/Zotero/storage/38SIVNUC/Wang et al. - 2021 - Dynamical Isometry The Missing Ingredient for Neu.pdf;/home/luisaam/Zotero/storage/B4D3SETD/2105.html}
}

@inproceedings{wangEigenDamageStructuredPruning2019,
  title = {{{EigenDamage}}: {{Structured Pruning}} in the {{Kronecker-Factored Eigenbasis}}},
  shorttitle = {{{EigenDamage}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Wang, Chaoqi and Grosse, Roger and Fidler, Sanja and Zhang, Guodong},
  year = {2019},
  month = may,
  pages = {6566--6575},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-12-05},
  abstract = {Reducing the test time resource requirements of a neural network while preserving test accuracy is crucial for running inference on resource-constrained devices. To achieve this goal, we introduce a novel network reparameterization based on the Kronecker-factored eigenbasis (KFE), and then apply Hessian-based structured pruning methods in this basis. As opposed to existing Hessian-based pruning algorithms which do pruning in parameter coordinates, our method works in the KFE where different weights are approximately independent, enabling accurate pruning and fast computation. We demonstrate empirically the effectiveness of the proposed method through extensive experiments. In particular, we highlight that the improvements are especially significant for more challenging datasets and networks. With negligible loss of accuracy, an iterative-pruning version gives a 10x reduction in model size and a 8x reduction in FLOPs on wide ResNet32.},
  langid = {english},
  keywords = {Hessian,Pruning},
  file = {/home/luisaam/Zotero/storage/FUUUKUZH/Wang et al. - 2019 - EigenDamage Structured Pruning in the Kronecker-F.pdf;/home/luisaam/Zotero/storage/PD2PRFX2/Wang et al. - 2019 - EigenDamage Structured Pruning in the Kronecker-F.pdf}
}

@article{wangModelPruningBased2022a,
  title = {Model {{Pruning Based}} on {{Quantified Similarity}} of {{Feature Maps}}},
  author = {Wang, Zidu and Liu, Xuexin and Huang, Long and Chen, Yunqing and Zhang, Yufei and Lin, Zhikang and Wang, Rui},
  year = {2022},
  month = dec,
  journal = {IEEE Internet of Things Journal},
  volume = {9},
  number = {23},
  eprint = {2105.06052},
  primaryclass = {cs},
  pages = {24506--24515},
  issn = {2327-4662, 2372-2541},
  doi = {10.1109/JIOT.2022.3190873},
  urldate = {2023-07-18},
  abstract = {Convolutional Neural Networks (CNNs) has been applied in numerous Internet of Things (IoT) devices for multifarious downstream tasks. However, with the increasing amount of data on edge devices, CNNs can hardly complete some tasks in time with limited computing and storage resources. Recently, filter pruning has been regarded as an effective technique to compress and accelerate CNNs, but existing methods rarely prune CNNs from the perspective of compressing high-dimensional tensors. In this paper, we propose a novel theory to find redundant information in three-dimensional tensors, namely Quantified Similarity between Feature Maps (QSFM), and utilize this theory to guide the filter pruning procedure. We perform QSFM on datasets (CIFAR-10, CIFAR-100 and ILSVRC-12) and edge devices, demonstrate that the proposed method can find the redundant information in the neural networks effectively with comparable compression and tolerable drop of accuracy. Without any fine-tuning operation, QSFM can compress ResNet-56 on CIFAR-10 significantly (48.7\% FLOPs and 57.9\% parameters are reduced) with only a loss of 0.54\% in the top-1 accuracy. For the practical application of edge devices, QSFM can accelerate MobileNet-V2 inference speed by 1.53 times with only a loss of 1.23\% in the ILSVRC-12 top-1 accuracy.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,filter similarity,Pruning},
  file = {/home/luisaam/Zotero/storage/VLYEKUHH/Wang et al. - 2022 - Model Pruning Based on Quantified Similarity of Fe.pdf;/home/luisaam/Zotero/storage/6LU2JLBZ/2105.html}
}

@article{wangModelPruningBased2022b,
  title = {Model {{Pruning Based}} on {{Quantified Similarity}} of {{Feature Maps}}},
  author = {Wang, Zidu and Liu, Xuexin and Huang, Long and Chen, Yunqing and Zhang, Yufei and Lin, Zhikang and Wang, Rui},
  year = {2022},
  month = dec,
  journal = {IEEE Internet of Things Journal},
  volume = {9},
  number = {23},
  eprint = {2105.06052},
  primaryclass = {cs},
  pages = {24506--24515},
  issn = {2327-4662, 2372-2541},
  doi = {10.1109/JIOT.2022.3190873},
  urldate = {2023-07-18},
  abstract = {Convolutional Neural Networks (CNNs) has been applied in numerous Internet of Things (IoT) devices for multifarious downstream tasks. However, with the increasing amount of data on edge devices, CNNs can hardly complete some tasks in time with limited computing and storage resources. Recently, filter pruning has been regarded as an effective technique to compress and accelerate CNNs, but existing methods rarely prune CNNs from the perspective of compressing high-dimensional tensors. In this paper, we propose a novel theory to find redundant information in three-dimensional tensors, namely Quantified Similarity between Feature Maps (QSFM), and utilize this theory to guide the filter pruning procedure. We perform QSFM on datasets (CIFAR-10, CIFAR-100 and ILSVRC-12) and edge devices, demonstrate that the proposed method can find the redundant information in the neural networks effectively with comparable compression and tolerable drop of accuracy. Without any fine-tuning operation, QSFM can compress ResNet-56 on CIFAR-10 significantly (48.7\% FLOPs and 57.9\% parameters are reduced) with only a loss of 0.54\% in the top-1 accuracy. For the practical application of edge devices, QSFM can accelerate MobileNet-V2 inference speed by 1.53 times with only a loss of 1.23\% in the ILSVRC-12 top-1 accuracy.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,filter similarity,Pruning},
  file = {/home/luisaam/Zotero/storage/ZFLQUIRD/Wang et al. - 2022 - Model Pruning Based on Quantified Similarity of Fe.pdf;/home/luisaam/Zotero/storage/ZUP35XX8/2105.html}
}

@inproceedings{wangNeuralPruningGrowing2022,
  title = {Neural {{Pruning}} via {{Growing Regularization}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Wang, Huan and Qin, Can and Zhang, Yulun and Fu, Yun},
  year = {2022},
  month = feb,
  urldate = {2022-12-05},
  abstract = {Regularization has long been utilized to learn sparsity in deep neural network pruning. However, its role is mainly explored in the small penalty strength regime. In this work, we extend its application to a new scenario where the regularization grows large gradually to tackle two central problems of pruning: pruning schedule and weight importance scoring. (1) The former topic is newly brought up in this work, which we find critical to the pruning performance while receives little research attention. Specifically, we propose an L2 regularization variant with rising penalty factors and show it can bring significant accuracy gains compared with its one-shot counterpart, even when the same weights are removed. (2) The growing penalty scheme also brings us an approach to exploit the Hessian information for more accurate pruning without knowing their specific values, thus not bothered by the common Hessian approximation problems. Empirically, the proposed algorithms are easy to implement and scalable to large datasets and networks in both structured and unstructured pruning. Their effectiveness is demonstrated with modern deep neural networks on the CIFAR and ImageNet datasets, achieving competitive results compared to many state-of-the-art algorithms. Our code and trained models are publicly available at https://github.com/mingsun-tse/regularization-pruning.},
  langid = {english},
  keywords = {Pruning},
  file = {/home/luisaam/Zotero/storage/FIIJCKB3/Wang et al. - 2022 - Neural Pruning via Growing Regularization.pdf;/home/luisaam/Zotero/storage/KV3R6AGC/forum.html}
}

@article{wangPickingWinningTickets2020,
  title = {Picking {{Winning Tickets Before Training}} by {{Preserving Gradient Flow}}},
  author = {Wang, Chaoqi and Zhang, Guodong and Grosse, Roger},
  year = {2020},
  month = aug,
  journal = {arXiv:2002.07376 [cs, stat]},
  eprint = {2002.07376},
  primaryclass = {cs, stat},
  urldate = {2021-08-05},
  abstract = {Overparameterization has been shown to benefit both the optimization and generalization of neural networks, but large networks are resource hungry at both training and test time. Network pruning can reduce test-time resource requirements, but is typically applied to trained networks and therefore cannot avoid the expensive training process. We aim to prune networks at initialization, thereby saving resources at training time as well. Specifically, we argue that efficient training requires preserving the gradient flow through the network. This leads to a simple but effective pruning criterion we term Gradient Signal Preservation (GraSP). We empirically investigate the effectiveness of the proposed method with extensive experiments on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet, using VGGNet and ResNet architectures. Our method can prune 80\% of the weights of a VGG-16 network on ImageNet at initialization, with only a 1.6\% drop in top-1 accuracy. Moreover, our method achieves significantly better performance than the baseline at extreme sparsity levels.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Pruning,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/374UX62B/Wang et al. - 2020 - Picking Winning Tickets Before Training by Preserv.pdf;/home/luisaam/Zotero/storage/5MQRI6U5/2002.html}
}

@inproceedings{wangRecentAdvancesNeural2022,
  title = {Recent {{Advances}} on {{Neural Network Pruning}} at {{Initialization}}},
  booktitle = {Thirty-{{First International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Wang, Huan and Qin, Can and Bai, Yue and Zhang, Yulun and Fu, Yun},
  year = {2022},
  month = jul,
  volume = {6},
  pages = {5638--5645},
  issn = {1045-0823},
  doi = {10.24963/ijcai.2022/786},
  urldate = {2022-12-05},
  abstract = {Electronic proceedings of IJCAI 2022},
  langid = {english},
  keywords = {Pruning},
  file = {/home/luisaam/Zotero/storage/X8WWVZ5F/Wang et al. - 2022 - Recent Advances on Neural Network Pruning at Initi.pdf;/home/luisaam/Zotero/storage/SKDYTV4F/786.html}
}

@misc{wangStructuredProbabilisticPruning2018,
  title = {Structured {{Probabilistic Pruning}} for {{Convolutional Neural Network Acceleration}}},
  author = {Wang, Huan and Zhang, Qiming and Wang, Yuehai and Hu, Haoji},
  year = {2018},
  month = sep,
  number = {arXiv:1709.06994},
  eprint = {1709.06994},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1709.06994},
  urldate = {2023-08-11},
  abstract = {In this paper, we propose a novel progressive parameter pruning method for Convolutional Neural Network acceleration, named Structured Probabilistic Pruning (SPP), which effectively prunes weights of convolutional layers in a probabilistic manner. Unlike existing deterministic pruning approaches, where unimportant weights are permanently eliminated, SPP introduces a pruning probability for each weight, and pruning is guided by sampling from the pruning probabilities. A mechanism is designed to increase and decrease pruning probabilities based on importance criteria in the training process. Experiments show that, with 4x speedup, SPP can accelerate AlexNet with only 0.3\% loss of top-5 accuracy and VGG-16 with 0.8\% loss of top-5 accuracy in ImageNet classification. Moreover, SPP can be directly applied to accelerate multi-branch CNN networks, such as ResNet, without specific adaptations. Our 2x speedup ResNet-50 only suffers 0.8\% loss of top-5 accuracy on ImageNet. We further show the effectiveness of SPP on transfer learning tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Pruning,Reviewers Stochastic Pruning,Statistics - Machine Learning},
  file = {/home/luisaam/Zotero/storage/X9PYCHLG/Wang et al. - 2018 - Structured Probabilistic Pruning for Convolutional.pdf;/home/luisaam/Zotero/storage/DEA4R9TJ/1709.html}
}

@inproceedings{wanRegularizationNeuralNetworks2013,
  title = {Regularization of {{Neural Networks}} Using {{DropConnect}}},
  booktitle = {Proceedings of the 30th {{International Conference}} on {{Machine Learning}}},
  author = {Wan, Li and Zeiler, Matthew and Zhang, Sixin and Cun, Yann Le and Fergus, Rob},
  year = {2013},
  month = may,
  pages = {1058--1066},
  publisher = {{PMLR}},
  issn = {1938-7228},
  urldate = {2023-08-11},
  abstract = {We introduce DropConnect, a generalization of DropOut, for regularizing large fully-connected layers within neural networks. When training with Dropout, a randomly selected subset of activations are set to zero within each layer. DropConnect instead sets a randomly selected subset of weights within the network to zero. Each unit thus receives input from a random subset of units in the previous layer. We derive a bound on the generalization performance of both Dropout and DropConnect. We then evaluate DropConnect on a range of datasets, comparing to Dropout, and show state-of-the-art results on several image recoginition benchmarks can be obtained by aggregating multiple DropConnect-trained models.},
  langid = {english},
  keywords = {Regularisation,Reviewers Stochastic Pruning},
  file = {/home/luisaam/Zotero/storage/N48JPU8U/Wan et al. - 2013 - Regularization of Neural Networks using DropConnec.pdf}
}

@article{WeActuallyNeed,
  title = {Do {{We Actually Need Dense Over-Parameterization}}? {{In-Time Over-Parameterization}} in {{Sparse Training}}. ({{arXiv}}:2102.02887v3 [Cs.{{LG}}] {{UPDATED}})},
  journal = {arXiv Computer Science},
  doi = {arXiv:2102.02887v3},
  abstract = {In this paper, we introduce a new perspective on training deep neural networks capable of state-of-the-art performance without the need for the expensive over-parameterization by proposing the concept of In-Time Over-Parameterization (ITOP) in sparse training. By starting from a random sparse network and continuously exploring sparse connectivities during training, we can perform an Over-Parameterization in the space-time manifold, closing the gap in the expressibility between sparse training and dense training. We further use ITOP to understand the underlying mechanism of Dynamic Sparse Training (DST) and indicate that the benefits of DST come from its ability to consider across time all possible parameters when searching for the optimal sparse connectivity. As long as there are sufficient parameters that have been reliably explored during training, DST can outperform the dense neural network by a large margin. We present a series of experiments to support our conjecture and achieve the state-of-the-art sparse training performance with ResNet-50 on ImageNet. More impressively, our method achieves dominant performance over the overparameterization-based sparse methods at extreme sparsity levels. When trained on CIFAR-100, our method can match the performance of the dense model even at an extreme sparsity (98\%). Code can be found https://github.com/Shiweiliuiiiiiii/In-Time-Over-Parameterization.},
  keywords = {Pruning,Researcher App,sparsity}
}

@inproceedings{wieczorekSelectionSchemesEvolutionary2002,
  title = {Selection {{Schemes}} in {{Evolutionary Algorithms}}},
  booktitle = {Intelligent {{Information Systems}} 2002},
  author = {Wieczorek, Wojciech and Czech, Zbigniew J.},
  editor = {K{\l}opotek, Mieczys{\l}aw A. and Wierzcho{\'n}, S{\l}awomir T. and Michalewicz, Maciej},
  year = {2002},
  series = {Advances in {{Soft Computing}}},
  pages = {185--194},
  publisher = {{Physica-Verlag HD}},
  address = {{Heidelberg}},
  doi = {10.1007/978-3-7908-1777-5_19},
  abstract = {One of the steps of an evolutionary algorithm is selection which chooses some individuals from a current population as parents to the individuals of the next population. This work focuses on loss of population diversity defined as the proportion of population individuals which are not chosen during selection. Quantifying loss of population diversity is crucial for designing evolutionary algorithms in which the search process is directed through controlling population diversity via parameters of a selection scheme. The aim of this work is to derive closed, approximate formulas which enable to determine loss of population diversity for some selection schemes. The selection schemes under consideration are also compared with respect to effectiveness and ease of controlling population diversity.},
  isbn = {978-3-7908-1777-5},
  langid = {english},
  keywords = {Evolutionary algorithms,exponential ranking selection,linear ranking selection,loss of diversity,selection schemes,selective pressure,tournament selection,truncation selection}
}

@inproceedings{wortsmanDiscoveringNeuralWirings2019,
  title = {Discovering {{Neural Wirings}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Wortsman, Mitchell and Farhadi, Ali and Rastegari, Mohammad},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-04-10},
  abstract = {The success of neural networks has driven a shift in focus from feature engineering to architecture engineering. However, successful networks today are constructed using a small and manually defined set of building blocks. Even in methods of neural architecture search (NAS) the network connectivity patterns are largely constrained. In this work we propose a method for discovering neural wirings. We relax the typical notion of layers and instead enable channels to form connections independent of each other. This allows for a much larger space of possible networks. The wiring of our network is not fixed during training -- as we learn the network parameters we also learn the structure itself. Our experiments demonstrate that our learned connectivity outperforms hand engineered and randomly wired networks. By learning the connectivity of MobileNetV1we boost the ImageNet accuracy by 10\% at {\textasciitilde}41M FLOPs. Moreover, we show that our method generalizes to recurrent and continuous time networks. Our work may also be regarded as unifying core aspects of the neural architecture search problem with sparse neural network learning. As NAS becomes more fine grained, finding a good architecture is akin to finding a sparse subnetwork of the complete graph. Accordingly, DNW provides an effective mechanism for discovering sparse subnetworks of predefined architectures in a single training run. Though we only ever use a small percentage of the weights during the forward pass, we still play the so-called initialization lottery with a combinatorial number of subnetworks. Code and pretrained models are available at https://github.com/allenai/dnw while additional visualizations may be found at https://mitchellnw.github.io/blog/2019/dnw/.},
  keywords = {dynamical sparse training,NAS,neural architecture search},
  file = {/home/luisaam/Zotero/storage/9VFFCUAQ/Wortsman et al. - 2019 - Discovering Neural Wirings.pdf}
}

@inproceedings{xiaoParallelLearningEvolutionary2006,
  title = {Parallel {{Learning Evolutionary Algorithm Based}} on {{Neural Network Ensemble}}},
  booktitle = {2006 {{IEEE International Conference}} on {{Information Acquisition}}},
  author = {Xiao, Sha and Yu, Dong and Li, Yibin},
  year = {2006},
  month = aug,
  pages = {70--74},
  doi = {10.1109/ICIA.2006.305824},
  abstract = {A new parallel evolutionary algorithm based on neural network (NN) ensemble is proposed for mobile robots. The robot controller has a reconfigurable structure with user- defined sensor suite, which corresponds to a typical network. NN ensemble is evolved to optimize the optimal sub-networks including both learning rules and combining weights dynamically. The experiment results show that the genetic algorithm based NN ensemble has stronger evolutionary capacity and higher learning efficiency of parallelism compared with the general genetic algorithm.},
  keywords = {Automatic control,evolutionary algorithm,Evolutionary computation,Genetic algorithms,Mobile robots,Neural network ensemble,Neural networks,parallel,Parallel robots,Robot sensing systems,Sensor arrays,Sensor systems,Sonar},
  file = {/home/luisaam/Zotero/storage/3TD6FZ5U/4097757.html}
}

@article{xieUnsupervisedDataAugmentation2020,
  title = {Unsupervised {{Data Augmentation}} for {{Consistency Training}}},
  author = {Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V.},
  year = {2020},
  month = nov,
  journal = {arXiv:1904.12848 [cs, stat]},
  eprint = {1904.12848},
  primaryclass = {cs, stat},
  urldate = {2021-08-04},
  abstract = {Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods such as RandAugment and back-translation, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 5.43 with only 250 examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10\% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at https://github.com/google-research/uda.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Data Augmentation,Statistics - Machine Learning,Unsupervised learning},
  file = {/home/luisaam/Zotero/storage/Y569Y25Q/Xie et al. - 2020 - Unsupervised Data Augmentation for Consistency Tra.pdf;/home/luisaam/Zotero/storage/TSHYBMTX/1904.html}
}

@inproceedings{xinNeuroLKHCombiningDeep2021,
  title = {{{NeuroLKH}}: {{Combining Deep Learning Model}} with {{Lin-Kernighan-Helsgaun Heuristic}} for {{Solving}} the {{Traveling Salesman Problem}}},
  shorttitle = {{{NeuroLKH}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Xin, Liang and Song, Wen and Cao, Zhiguang and Zhang, Jie},
  year = {2021},
  volume = {34},
  pages = {7472--7483},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-08-11},
  abstract = {We present NeuroLKH, a novel algorithm that combines deep learning with the strong traditional heuristic Lin-Kernighan-Helsgaun (LKH) for solving Traveling Salesman Problem. Specifically, we train a Sparse Graph Network (SGN) with supervised learning for edge scores and unsupervised learning for node penalties, both of which are critical for improving the performance of LKH. Based on the output of SGN, NeuroLKH creates the edge candidate set and transforms edge distances to guide the searching process of LKH. Extensive experiments firmly demonstrate that, by training one model on a wide range of problem sizes, NeuroLKH significantly outperforms LKH and generalizes well to much larger sizes. Also, we show that NeuroLKH can be applied to other routing problems such as Capacitated Vehicle Routing Problem (CVRP), Pickup and Delivery Problem (PDP), and CVRP with Time Windows (CVRPTW).},
  file = {/home/luisaam/Zotero/storage/3ANU6ZFA/Xin et al. - 2021 - NeuroLKH Combining Deep Learning Model with Lin-K.pdf}
}

@article{xuNewtontypeMethodsNonconvex2020,
  title = {Newton-Type Methods for Non-Convex Optimization under Inexact {{Hessian}} Information},
  author = {Xu, Peng and Roosta, Fred and Mahoney, Michael W.},
  year = {2020},
  month = nov,
  journal = {Mathematical Programming},
  volume = {184},
  number = {1-2},
  pages = {35--70},
  issn = {0025-5610, 1436-4646},
  doi = {10.1007/s10107-019-01405-z},
  urldate = {2021-08-21},
  langid = {english},
  keywords = {second order},
  file = {/home/luisaam/Zotero/storage/IM67JBT8/Xu et al. - 2020 - Newton-type methods for non-convex optimization un.pdf}
}

@inproceedings{xuOptimizedComputationCombining2021,
  title = {Optimized {{Computation Combining Classification}} and {{Detection Networks}} with {{Distillation}}},
  booktitle = {Proceedings of the {{International Joint Conference}} on {{Neural Networks}}},
  author = {Xu, M. and Gu, Y. and Poslad, S. and Xue, S.},
  year = {2021},
  volume = {2021-July},
  doi = {10.1109/IJCNN52387.2021.9534331},
  abstract = {A Convolutional neural network (CNN) has emerged as a widely used approach to computer vision tasks, including object classification and detection tasks. The high requirement for the model to be more computationally efficient on lower information and communication technology (ICT) resource, e.g., mobile terminals can benefit from model distillation. However, most existing distillation methods suffer from a significant accuracy reduction, which requires a large number of pre-training models or doesn't make good use of the more of the network information, e.g., in the middle layers, during the distillation. In this paper, we study how knowledge about traffic signs recognition could be transferred to smaller models by distillation while cutting channels. We present an optimized object detection network, which uses a Region Proposal Network (RPN) weighted loss and hard-soft distribution-wise distillation loss for structural differences between teacher and student networks. We validate the network on multiple real-world datasets, the experiments demonstrate that the classification accuracy can be improved by 9 \% with about 16 times parameter reduction while the detection network performance could be increased by 10.6\% using an optimized object detection network. {\textcopyright} 2021 IEEE.},
  isbn = {978-0-7381-3366-9},
  langid = {english},
  keywords = {knowledge distilation,knowledge distillation,object detection,Pruning,Sobolev training,student network,teacher network},
  file = {/home/luisaam/Zotero/storage/JJAQ5389/Xu et al. - 2021 - Optimized Computation Combining Classification and.pdf;/home/luisaam/Zotero/storage/5Y68IDNN/display.html}
}

@article{xuPersonalityYoungAdult2015,
  title = {Personality and Young Adult Financial Distress},
  author = {Xu, Yilan and Beller, Andrea H. and Roberts, Brent W. and Brown, Jeffrey R.},
  year = {2015},
  month = dec,
  journal = {Journal of Economic Psychology},
  volume = {51},
  pages = {90--100},
  issn = {0167-4870},
  doi = {10.1016/j.joep.2015.08.010},
  urldate = {2023-06-23},
  abstract = {Researchers have become increasingly interested in understanding the sources of heterogeneity in individual financial behaviors. In this paper, we examine how the Big Five personality traits are related to measures of young adults' financial distress. Using data from the National Longitudinal Study of Adolescent to Adult Health in the United States, we find that conscientiousness is negatively correlated, and neuroticism positively correlated with financial distress. These correlations are robust to controlling for early life background and other demographic and socioeconomic factors. Young adulthood sets the stage for financial security in later life; as such, this study provides insight for lifelong financial wellbeing. Based on the empirical results, we discuss potential behavioral and policy interventions that can be used to improve financial wellbeing.},
  langid = {english},
  keywords = {Conscientiousness,Financial distress,Financial vulnerability,Neuroticism,Personality,Personality traits},
  file = {/home/luisaam/Zotero/storage/LZSFCITS/Xu et al. - 2015 - Personality and young adult financial distress.pdf;/home/luisaam/Zotero/storage/AR9IQKRR/S0167487015001142.html}
}

@inproceedings{xuSecondorderOptimizationNonconvex2020,
  title = {Second-Order Optimization for Non-Convex Machine Learning: {{An}} Empirical Study},
  booktitle = {Proceedings of the 2020 {{SIAM International Conference}} on {{Data Mining}}, {{SDM}} 2020},
  author = {Xu, Peng and Roosta, Fred and Mahoney, Michael W.},
  year = {2020},
  doi = {10.1137/1.9781611976236.23},
  abstract = {While first-order optimization methods such as SGD are popular in machine learning (ML), they come with well-known deficiencies, including relatively-slow convergence, sensitivity to the settings of hyper-parameters such as learning rate, stagnation at high training errors, and difficulty in escaping flat regions and saddle points. These issues are particularly acute in highly non-convex settings such as those arising in neural networks. Motivated by this, there has been recent interest in second-order methods that aim to alleviate these shortcomings by capturing curvature information. In this paper, we report detailed empirical evaluations of a class of Newton-type methods, namely sub-sampled variants of trust region (TR) and adaptive regularization with cubics (ARC) algorithms, for non-convex ML problems. In doing so, we demonstrate that these methods not only can be computationally competitive with hand-tuned SGD with momentum, obtaining comparable or better generalization performance, but also they are highly robust to hyper-parameter settings. Further, we show that the manner in which these Newton-type methods employ curvature information allows them to seamlessly escape flat regions and saddle points.},
  isbn = {978-1-61197-623-6},
  file = {/home/luisaam/Zotero/storage/XY3JUKN7/Xu et al. - 2020 - Second-order optimization for non-convex machine l.pdf}
}

@misc{yangDepthEstimationSimplified2022,
  title = {Depth {{Estimation}} with {{Simplified Transformer}}},
  author = {Yang, John and An, Le and Dixit, Anurag and Koo, Jinkyu and Park, Su Inn},
  year = {2022},
  month = may,
  number = {arXiv:2204.13791},
  eprint = {2204.13791},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2204.13791},
  urldate = {2022-12-15},
  abstract = {Transformer and its variants have shown state-of-the-art results in many vision tasks recently, ranging from image classification to dense prediction. Despite of their success, limited work has been reported on improving the model efficiency for deployment in latency-critical applications, such as autonomous driving and robotic navigation. In this paper, we aim at improving upon the existing transformers in vision, and propose a method for self-supervised monocular Depth Estimation with Simplified Transformer (DEST), which is efficient and particularly suitable for deployment on GPU-based platforms. Through strategic design choices, our model leads to significant reduction in model size, complexity, as well as inference latency, while achieving superior accuracy as compared to state-of-the-art. We also show that our design generalize well to other dense prediction task without bells and whistles.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,latency-critical applications,Transformers,Visual Transformers},
  file = {/home/luisaam/Zotero/storage/DATAAATZ/Yang et al. - 2022 - Depth Estimation with Simplified Transformer.pdf;/home/luisaam/Zotero/storage/XZJPL2LC/2204.html}
}

@article{yaoEvolvingArtificialNeural1999,
  title = {Evolving Artificial Neural Networks},
  author = {Yao, Xin},
  year = {1999},
  month = sep,
  journal = {Proceedings of the IEEE},
  volume = {87},
  number = {9},
  pages = {1423--1447},
  issn = {1558-2256},
  doi = {10.1109/5.784219},
  abstract = {Learning and evolution are two fundamental forms of adaptation. There has been a great interest in combining learning and evolution with artificial neural networks (ANNs) in recent years. This paper: 1) reviews different combinations between ANNs and evolutionary algorithms (EAs), including using EAs to evolve ANN connection weights, architectures, learning rules, and input features; 2) discusses different search operators which have been used in various EAs; and 3) points out possible future research directions. It is shown, through a considerably large literature review, that combinations between ANNs and EAs can lead to significantly better intelligent systems than relying on ANNs or EAs alone.},
  keywords = {Adaptive systems,Algorithm design and analysis,Artificial intelligence,Artificial neural networks,Competitive intelligence,Computer networks,Evolutionary computation,Intelligent networks,Intelligent systems,Transfer functions},
  file = {/home/luisaam/Zotero/storage/4VHTM535/Yao - 1999 - Evolving artificial neural networks.pdf;/home/luisaam/Zotero/storage/9UJPC7ND/784219.html}
}

@inproceedings{yaoPyHessianNeuralNetworks2020,
  title = {{{PyHessian}}: {{Neural Networks Through}} the {{Lens}} of the {{Hessian}}},
  shorttitle = {{{PyHessian}}},
  booktitle = {2020 {{IEEE International Conference}} on {{Big Data}} ({{Big Data}})},
  author = {Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael W.},
  year = {2020},
  month = dec,
  pages = {581--590},
  doi = {10.1109/BigData50022.2020.9378171},
  abstract = {We present PYHESSIAN, a new scalable framework that enables fast computation of Hessian (i.e., second-order derivative) information for deep neural networks. PYHESSIAN enables fast computations of the top Hessian eigenvalues, the Hessian trace, and the full Hessian eigenvalue/spectral density; it supports distributed-memory execution on cloud/supercomputer systems; and it is available as open source [1]. This general framework can be used to analyze neural network models, including the topology of the loss landscape (i.e., curvature information) to gain insight into the behavior of different models/optimizers. As an example, we analyze the effect of residual connections and Batch Normalization layers on the trainability of neural networks. One recent claim, based on simpler first-order analysis, is that residual connections and Batch Normalization make the loss landscape "smoother," thus making it easier for Stochastic Gradient Descent to converge to a good solution. Our second-order analysis, easily enabled by PYHESSIAN, shows new finer-scale insights, demonstrating that while conventional wisdom is sometimes validated, in other cases it is simply incorrect. In particular, we find that Batch Normalization does not necessarily make the loss landscape smoother, especially for shallow networks.},
  keywords = {Analytical models,Artificial neural networks,Big Data,Eigenvalues and eigenfunctions,Electrostatic discharges,framework,Hessian,Lenses,second order,Training},
  file = {/home/luisaam/Zotero/storage/QBIQ3G6Z/Yao et al. - 2020 - PyHessian Neural Networks Through the Lens of the.pdf;/home/luisaam/Zotero/storage/ZGL9JU6E/9378171.html}
}

@article{yarkoniPersonality1000002010,
  title = {Personality in 100,000 {{Words}}: {{A}} Large-Scale Analysis of Personality and Word Use among Bloggers},
  shorttitle = {Personality in 100,000 {{Words}}},
  author = {Yarkoni, Tal},
  year = {2010},
  month = jun,
  journal = {Journal of research in personality},
  volume = {44},
  number = {3},
  pages = {363--373},
  issn = {0092-6566},
  doi = {10.1016/j.jrp.2010.04.001},
  urldate = {2023-07-07},
  abstract = {Previous studies have found systematic associations between personality and individual differences in word use. Such studies have typically focused on broad associations between major personality domains and aggregate word categories, potentially masking more specific associations. Here I report the results of a large-scale analysis of personality and word use in a large sample of blogs (N=694). The size of the dataset enabled pervasive correlations with personality to be identified for a broad range of lexical variables, including both aggregate word categories and individual English words. The results replicated category-level findings from previous offline studies, identified numerous novel associations at both a categorical and single-word level, and underscored the value of complementary approaches to the study of personality and word use.},
  pmcid = {PMC2885844},
  pmid = {20563301},
  keywords = {Big 5,NLP,Psycholinguistics},
  file = {/home/luisaam/Zotero/storage/IRQNDKHB/Yarkoni - 2010 - Personality in 100,000 Words A large-scale analys.pdf}
}

@misc{yinSuperposingManyTickets2022,
  title = {Superposing {{Many Tickets}} into {{One}}: {{A Performance Booster}} for {{Sparse Neural Network Training}}},
  shorttitle = {Superposing {{Many Tickets}} into {{One}}},
  author = {Yin, Lu and Menkovski, Vlado and Fang, Meng and Huang, Tianjin and Pei, Yulong and Pechenizkiy, Mykola and Mocanu, Decebal Constantin and Liu, Shiwei},
  year = {2022},
  month = may,
  number = {arXiv:2205.15322},
  eprint = {2205.15322},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2205.15322},
  urldate = {2022-06-02},
  abstract = {Recent works on sparse neural network training (sparse training) have shown that a compelling trade-off between performance and efficiency can be achieved by training intrinsically sparse neural networks from scratch. Existing sparse training methods usually strive to find the best sparse subnetwork possible in one single run, without involving any expensive dense or pre-training steps. For instance, dynamic sparse training (DST), as one of the most prominent directions, is capable of reaching a competitive performance of dense training by iteratively evolving the sparse topology during the course of training. In this paper, we argue that it is better to allocate the limited resources to create multiple low-loss sparse subnetworks and superpose them into a stronger one, instead of allocating all resources entirely to find an individual subnetwork. To achieve this, two desiderata are required: (1) efficiently producing many low-loss subnetworks, the so-called cheap tickets, within one training process limited to the standard training time used in dense training; (2) effectively superposing these cheap tickets into one stronger subnetwork without going over the constrained parameter budget. To corroborate our conjecture, we present a novel sparse training approach, termed {\textbackslash}textbf\{Sup-tickets\}, which can satisfy the above two desiderata concurrently in a single sparse-to-sparse training process. Across various modern architectures on CIFAR-10/100 and ImageNet, we show that Sup-tickets integrates seamlessly with the existing sparse training methods and demonstrates consistent performance improvement.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,dynamical sparse training,Pruning},
  file = {/home/luisaam/Zotero/storage/HED9ACJF/Yin et al. - 2022 - Superposing Many Tickets into One A Performance B.pdf;/home/luisaam/Zotero/storage/FIMVPGM8/2205.html}
}

@inproceedings{yoonLifelongLearningDynamically2018,
  title = {Lifelong {{Learning}} with {{Dynamically Expandable Networks}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Yoon, Jaehong and Yang, Eunho and Lee, Jeongtae and Hwang, Sung Ju},
  year = {2018},
  month = feb,
  urldate = {2021-11-20},
  abstract = {We propose a novel deep network architecture that can dynamically decide its network capacity as it trains on a lifelong learning scenario.},
  langid = {english},
  file = {/home/luisaam/Zotero/storage/CUAPVMAQ/Yoon et al. - 2018 - Lifelong Learning with Dynamically Expandable Netw.pdf;/home/luisaam/Zotero/storage/6HG2ZKDE/forum.html}
}

@inproceedings{yosinskiHowTransferableAre2014,
  title = {How Transferable Are Features in Deep Neural Networks?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  year = {2014},
  volume = {27},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2022-10-24},
  abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
  keywords = {layer importance,layer role},
  file = {/home/luisaam/Zotero/storage/4V8WZ4YJ/Yosinski et al. - 2014 - How transferable are features in deep neural netwo.pdf}
}

@inproceedings{yosinskiHowTransferableAre2014a,
  title = {How Transferable Are Features in Deep Neural Networks?},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  year = {2014},
  volume = {27},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-11-21},
  abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
  keywords = {NN features,Receptive Field},
  file = {/home/luisaam/Zotero/storage/9FKU2RSC/Yosinski et al. - 2014 - How transferable are features in deep neural netwo.pdf}
}

@article{youyouComputerbasedPersonalityJudgments2015,
  title = {Computer-Based Personality Judgments Are More Accurate than Those Made by Humans},
  author = {Youyou, Wu and Kosinski, Michal and Stillwell, David},
  year = {2015},
  month = jan,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {112},
  number = {4},
  pages = {1036--1040},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1418680112},
  urldate = {2023-07-07},
  abstract = {Judging others' personalities is an essential skill in successful social living, as personality is a key driver behind people's interactions, behaviors, and emotions. Although accurate personality judgments stem from social-cognitive skills, developments in machine learning show that computer models can also make valid judgments. This study compares the accuracy of human and computer-based personality judgments, using a sample of 86,220 volunteers who completed a 100-item personality questionnaire. We show that (i) computer predictions based on a generic digital footprint (Facebook Likes) are more accurate (r = 0.56) than those made by the participants' Facebook friends using a personality questionnaire (r = 0.49); (ii) computer models show higher interjudge agreement; and (iii) computer personality judgments have higher external validity when predicting life outcomes such as substance use, political attitudes, and physical health; for some outcomes, they even outperform the self-rated personality scores. Computers outpacing humans in personality judgment presents significant opportunities and challenges in the areas of psychological assessment, marketing, and privacy.},
  keywords = {Big 5,Psycholinguistics},
  file = {/home/luisaam/Zotero/storage/WHQCVVYQ/Youyou et al. - 2015 - Computer-based personality judgments are more accu.pdf}
}

@inproceedings{yuHessianAwarePruningOptimal2022,
  title = {Hessian-{{Aware Pruning}} and {{Optimal Neural Implant}}},
  booktitle = {2022 {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision}} ({{WACV}})},
  author = {Yu, Shixing and Yao, Zhewei and Gholami, Amir and Dong, Zhen and Kim, Sehoon and Mahoney, Michael W. and Keutzer, Kurt},
  year = {2022},
  month = jan,
  pages = {3665--3676},
  issn = {2642-9381},
  doi = {10.1109/WACV51458.2022.00372},
  abstract = {Pruning is an effective method to reduce the memory footprint and FLOPs associated with neural network models. However, existing structured-pruning methods often result in significant accuracy degradation for moderate pruning levels. To address this problem, we introduce a new Hessian Aware Pruning (HAP) method coupled with a Neural Implant approach that uses second-order sensitivity as a metric for structured pruning. The basic idea is to prune insensitive components and to use a Neural Implant for moderately sensitive components, instead of completely pruning them. For the latter approach, the moderately sensitive components are replaced with a low rank implant that is smaller and less computationally expensive than the original component. We use the relative Hessian trace to measure sensitivity, as opposed to the magnitude based sensitivity metric commonly used in the literature. We test HAP for both computer vision tasks and natural language tasks, and we achieve new state-of-the-art results. Specifically, HAP achieves less than 0.1\%/0.5\% degradation on PreResNet29/ResNet50 (CIFAR-10/ImageNet) with more than 70\%/50\% of parameters pruned. Meanwhile, HAP also achieves significantly better performance (up to 0.8\% with 60\% of parameters pruned) as compared to gradient based method for head pruning on transformer-based models. The framework has been open sourced and available online: https://github.com/yaozhewei/HAP.},
  keywords = {Computer vision,Deep Learning -{$>$} Efficient Training and Inference Methods for Networks,Degradation,Head,Hessian,Natural languages,Neural implants,Pruning,Sensitivity,Transformers},
  file = {/home/luisaam/Zotero/storage/MNXTBANI/Yu et al. - 2022 - Hessian-Aware Pruning and Optimal Neural Implant.pdf;/home/luisaam/Zotero/storage/DVPNATC3/9707021.html}
}

@article{yuMetaFormerActuallyWhat2021,
  title = {{{MetaFormer}} Is {{Actually What You Need}} for {{Vision}}},
  author = {Yu, Weihao and Luo, Mi and Zhou, Pan and Si, Chenyang and Zhou, Yichen and Wang, Xinchao and Feng, Jiashi and Yan, Shuicheng},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.11418 [cs]},
  eprint = {2111.11418},
  primaryclass = {cs},
  urldate = {2021-12-13},
  abstract = {Transformers have shown great potential in computer vision tasks. A common belief is their attention-based token mixer module contributes most to their competence. However, recent works show the attention-based module in transformers can be replaced by spatial MLPs and the resulted models still perform quite well. Based on this observation, we hypothesize that the general architecture of the transformers, instead of the specific token mixer module, is more essential to the model's performance. To verify this, we deliberately replace the attention module in transformers with an embarrassingly simple spatial pooling operator to conduct only the most basic token mixing. Surprisingly, we observe that the derived model, termed as PoolFormer, achieves competitive performance on multiple computer vision tasks. For example, on ImageNet-1K, PoolFormer achieves 82.1\% top-1 accuracy, surpassing well-tuned vision transformer/MLP-like baselines DeiT-B/ResMLP-B24 by 0.3\%/1.1\% accuracy with 35\%/52\% fewer parameters and 48\%/60\% fewer MACs. The effectiveness of PoolFormer verifies our hypothesis and urges us to initiate the concept of "MetaFormer", a general architecture abstracted from transformers without specifying the token mixer. Based on the extensive experiments, we argue that MetaFormer is the key player in achieving superior results for recent transformer and MLP-like models on vision tasks. This work calls for more future research dedicated to improving MetaFormer instead of focusing on the token mixer modules. Additionally, our proposed PoolFormer could serve as a starting baseline for future MetaFormer architecture design. Code is available at https://github.com/sail-sg/poolformer},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/luisaam/Zotero/storage/ZCLVP3C8/Yu et al. - 2021 - MetaFormer is Actually What You Need for Vision.pdf;/home/luisaam/Zotero/storage/3H27YYJ2/2111.html}
}

@inproceedings{yung-chinEvolutionaryNeuralNetworks2010,
  title = {Evolutionary {{Neural Networks}} for {{Time Series Prediction}}},
  booktitle = {2010 {{Fourth International Conference}} on {{Genetic}} and {{Evolutionary Computing}}},
  author = {{Yung-Chin} and {Yung-Chien} and Su, Kuo-Lan},
  year = {2010},
  month = dec,
  pages = {219--223},
  doi = {10.1109/ICGEC.2010.61},
  abstract = {A novel application to the optimization of neural networks is presented in this paper. Here, the weight and architecture optimization of neural networks can be formulated as a mixed-integer optimization problem. And then a mixed-integer evolutionary algorithm (Mixed-Integer Hybrid Differential Evolution, MIHDE) is used to optimize the neural network. Finally, the optimized neural network is applied to the prediction of chaotic time series. The satisfactory results are achieved, and demonstrate that the neural network optimized by MIHDE can effectively predict the chaotic time series.},
  keywords = {Applications,Artificial neural networks,Computer architecture,evolutionary algorithm,Evolutionary computation,mixed-integer optimization,neural networks,Optimization,Time series analysis,Training,Transfer functions},
  file = {/home/luisaam/Zotero/storage/TR7AIWSN/Yung-Chin et al. - 2010 - Evolutionary Neural Networks for Time Series Predi.pdf;/home/luisaam/Zotero/storage/C4XX84D8/5715409.html}
}

@article{yuZeroshotLearningSimultaneous2019,
  title = {Zero-Shot {{Learning}} via {{Simultaneous Generating}} and {{Learning}}},
  author = {Yu, Hyeonwoo and Lee, Beomhee},
  year = {2019},
  number = {NeurIPS},
  issn = {10495258},
  abstract = {To overcome the absence of training data for unseen classes, conventional zero-shot learning approaches mainly train their model on seen datapoints and leverage the semantic descriptions for both seen and unseen classes. Beyond exploiting relations between classes of seen and unseen, we present a deep generative model to provide the model with experience about both seen and unseen classes. Based on the variational auto-encoder with class-specific multi-modal prior, the proposed method learns the conditional distribution of seen and unseen classes. In order to circumvent the need for samples of unseen classes, we treat the non-existing data as missing examples. That is, our network aims to find optimal unseen datapoints and model parameters, by iteratively following the generating and learning strategy. Since we obtain the conditional generative model for both seen and unseen classes, classification as well as generation can be performed directly without any off-the-shell classifiers. In experimental results, we demonstrate that the proposed generating and learning strategy makes the model achieve the outperforming results compared to that trained only on the seen classes, and also to the several state-of-the-art methods.},
  file = {/home/luisaam/Zotero/storage/YM5KBSFX/8300-zero-shot-learning-via-simultaneous-generating-and-learning.pdf}
}

@article{zagoruykoWideResidualNetworks2017,
  title = {Wide {{Residual Networks}}},
  author = {Zagoruyko, Sergey and Komodakis, Nikos},
  year = {2017},
  month = jun,
  journal = {arXiv:1605.07146 [cs]},
  eprint = {1605.07146},
  primaryclass = {cs},
  urldate = {2021-07-14},
  abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at https://github.com/szagoruyko/wide-residual-networks},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,examples},
  file = {/home/luisaam/Zotero/storage/NHLKQJSH/Zagoruyko y Komodakis - 2017 - Wide Residual Networks.pdf;/home/luisaam/Zotero/storage/8ZHBQBIU/1605.html}
}

@article{zeilerADADELTAAdaptiveLearning2012,
  title = {{{ADADELTA}}: {{An Adaptive Learning Rate Method}}},
  shorttitle = {{{ADADELTA}}},
  author = {Zeiler, Matthew D.},
  year = {2012},
  month = dec,
  journal = {arXiv:1212.5701 [cs]},
  eprint = {1212.5701},
  primaryclass = {cs},
  urldate = {2021-08-06},
  abstract = {We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Optimizers},
  file = {/home/luisaam/Zotero/storage/FWNW7HLX/Zeiler - 2012 - ADADELTA An Adaptive Learning Rate Method.pdf;/home/luisaam/Zotero/storage/W424JR6S/1212.html}
}

@article{zhaiScalingVisionTransformers2021,
  title = {Scaling {{Vision Transformers}}},
  author = {Zhai, Xiaohua and Kolesnikov, Alexander and Houlsby, Neil and Beyer, Lucas},
  year = {2021},
  month = jun,
  journal = {arXiv:2106.04560 [cs]},
  eprint = {2106.04560},
  primaryclass = {cs},
  urldate = {2021-07-19},
  abstract = {Attention-based neural networks such as the Vision Transformer (ViT) have recently attained state-of-the-art results on many computer vision benchmarks. Scale is a primary ingredient in attaining excellent results, therefore, understanding a model's scaling properties is a key to designing future generations effectively. While the laws for scaling Transformer language models have been studied, it is unknown how Vision Transformers scale. To address this, we scale ViT models and data, both up and down, and characterize the relationships between error rate, data, and compute. Along the way, we refine the architecture and training of ViT, reducing memory consumption and increasing accuracy the resulting models. As a result, we successfully train a ViT model with two billion parameters, which attains a new state-of-the-art on ImageNet of 90.45\% top-1 accuracy. The model also performs well on few-shot learning, for example, attaining 84.86\% top-1 accuracy on ImageNet with only 10 examples per class.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/home/luisaam/Zotero/storage/332W5QGE/Zhai et al. - 2021 - Scaling Vision Transformers.pdf;/home/luisaam/Zotero/storage/BBAVJ49L/2106.html}
}

@inproceedings{zhangCanSubnetworkStructure2021,
  title = {Can {{Subnetwork Structure Be}} the {{Key}} to {{Out-of-Distribution Generalization}}?},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Zhang, Dinghuai and Ahuja, Kartik and Xu, Yilun and Wang, Yisen and Courville, Aaron},
  year = {2021},
  month = jul,
  pages = {12356--12367},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2022-03-24},
  abstract = {Can models with particular structure avoid being biased towards spurious correlation in out-of-distribution (OOD) generalization? Peters et al. (2016) provides a positive answer for linear cases. In this paper, we use a functional modular probing method to analyze deep model structures under OOD setting. We demonstrate that even in biased models (which focus on spurious correlation) there still exist unbiased functional subnetworks. Furthermore, we articulate and confirm the functional lottery ticket hypothesis: the full network contains a subnetwork with proper structure that can achieve better OOD performance. We then propose Modular Risk Minimization to solve the subnetwork selection problem. Our algorithm learns the functional structure from a given dataset, and can be combined with any other OOD regularization methods. Experiments on various OOD generalization tasks corroborate the effectiveness of our method.},
  langid = {english},
  keywords = {functional LTH,subnetwork structure},
  file = {/home/luisaam/Zotero/storage/8ZBJZC94/Zhang et al. - 2021 - Can Subnetwork Structure Be the Key to Out-of-Dist.pdf;/home/luisaam/Zotero/storage/MSTUS7TG/Zhang et al. - 2021 - Can Subnetwork Structure Be the Key to Out-of-Dist.pdf}
}

@article{zhangFilterPruningUniqueness2023a,
  title = {Filter Pruning with Uniqueness Mechanism in the Frequency Domain for Efficient Neural Networks},
  author = {Zhang, Shuo and Gao, Mingqi and Ni, Qiang and Han, Jungong},
  year = {2023},
  month = apr,
  journal = {Neurocomputing},
  volume = {530},
  pages = {116--124},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2023.02.004},
  urldate = {2023-06-09},
  abstract = {Filter pruning has drawn extensive attention due to its advantage in reducing computational costs and memory requirements of deep convolutional neural networks. However, most existing methods only prune filters based on their intrinsic properties or spatial feature maps, ignoring the correlation between filters. In this paper, we suggest the correlation is valuable and consider it from a novel view: the frequency domain. Specifically, we first transfer features to the frequency domain by Discrete Cosine Transform (DCT). Then, for each feature map, we compute a uniqueness score, which measures its probability of being replaced by others. This way allows to prune the filters corresponding to the low-uniqueness maps without significant performance degradation. Compared to the methods focusing on intrinsic properties, our proposed method introduces a more comprehensive criterion to prune filters, further improving the network compactness while preserving good performance. In addition, our method is more robust against noise than the spatial ones since the critical clues for pruning are more concentrated after DCT. Experimental results demonstrate the superiority of our method. To be specific, our method outperforms the baseline ResNet-56 by 0.38\% on CIFAR-10 while reducing the floating-point operations (FLOPs) by 47.4\%. In addition, a consistent improvement can be observed when pruning the baseline ResNet-110: 0.23\% performance increase and up to 71\% FLOPs drop. Finally, on ImageNet, our method reduces the FLOPs of the baseline ResNet-50 by 48.7\% with only 0.32\% accuracy loss.},
  langid = {english},
  keywords = {Computer vision,Deep learning,filter similarity,Frequency-domain transformation,Image classification,Model compression,Pruning},
  file = {/home/luisaam/Zotero/storage/SXVU38F8/Zhang et al. - 2023 - Filter pruning with uniqueness mechanism in the fr.pdf;/home/luisaam/Zotero/storage/RM4U2E8A/S0925231223001364.html}
}

@article{zhangFPFSFilterlevelPruning2022,
  title = {{{FPFS}}: {{Filter-level}} Pruning via Distance Weight Measuring Filter Similarity},
  shorttitle = {{{FPFS}}},
  author = {Zhang, Wei and Wang, Zhiming},
  year = {2022},
  month = nov,
  journal = {Neurocomputing},
  volume = {512},
  pages = {40--51},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2022.09.049},
  urldate = {2023-06-09},
  abstract = {Deep Neural Networks (DNNs) enjoy the welfare of convolution, while also bearing huge computational pressure. Therefore, model compression techniques are used to alleviate this problem, where filter-based neural network has received extensive attention as the research object of this paper. Common approaches treat filters as independent individuals and choose retrained filters by evaluating their performance, while more complex macro methods consider relationship between filters. Therefore, we propose a facile distance-based filter selection method, called FPFS, to visualize the similarity between filters from a global perspective. We calculate and sum the distance between filters to get filters' ``Distance Weight'' which is applied as a metric to assess filters. We use four common and appropriate distances for filters evaluation. To verify the performance of our algorithm, we introduce FPFS to classical DCNNs and test it on general classification datasets CIFAR-10, CIFAR-100 and mageNet. For example, FPFS reduces Parameters and FLOPs of the lightweight model DenseNet-40 to about half of the original while maintain accuracy on CIFAR-10 by 94.40\% (the original model is 94.80\%). To ResNet-56 on CIFAR-100, FPFS compresses FLOPs to less than half of the original, while model accuracy reaches 71.46\% (the original model is 71.44\%). About ResNet-50 on ImageNet, FPFS achieves 60.3\% FLOPs pruning rate accompanied by 0.96\% top-1 accuracy loss. We also compare the experimental results with state-of-the-art filter pruning algorithms to highlight the effectiveness of FPFS.},
  langid = {english},
  keywords = {Deep convolutional neural network (DCNN),filter similarity,Model compression,Neural network pruning,Pruning},
  file = {/home/luisaam/Zotero/storage/BRWLAHQI/Zhang and Wang - 2022 - FPFS Filter-level pruning via distance weight mea.pdf;/home/luisaam/Zotero/storage/YVYEPZ4X/S092523122201164X.html}
}

@inproceedings{zhangGraphHyperNetworksNeural2018,
  title = {Graph {{HyperNetworks}} for {{Neural Architecture Search}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Zhang, Chris and Ren, Mengye and Urtasun, Raquel},
  year = {2018},
  month = sep,
  urldate = {2021-07-24},
  abstract = {Neural architecture search (NAS) automatically finds the best task-specific neural network topology, outperforming many manual architecture designs. However, it can be prohibitively expensive as...},
  langid = {english},
  keywords = {small networks},
  file = {/home/luisaam/Zotero/storage/3J26C7W6/Zhang et al. - 2018 - Graph HyperNetworks for Neural Architecture Search.pdf;/home/luisaam/Zotero/storage/WGVZSMJN/forum.html}
}

@misc{zhangLotteryJackpotsExist2022,
  title = {Lottery {{Jackpots Exist}} in {{Pre-trained Models}}},
  author = {Zhang, Yuxin and Lin, Mingbao and Zhong, Yunshan and Chao, Fei and Ji, Rongrong},
  year = {2022},
  month = dec,
  number = {arXiv:2104.08700},
  eprint = {2104.08700},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.08700},
  urldate = {2023-05-31},
  abstract = {Network pruning is an effective approach to reduce network complexity with acceptable performance compromise. Existing studies achieve the sparsity of neural networks via time-consuming weight training or complex searching on networks with expanded width, which greatly limits the applications of network pruning. In this paper, we show that high-performing and sparse sub-networks without the involvement of weight training, termed "lottery jackpots", exist in pre-trained models with unexpanded width. Furthermore, we improve the efficiency for searching lottery jackpots from two perspectives. Firstly, we observe that the sparse masks derived from many existing pruning criteria have a high overlap with the searched mask of our lottery jackpot, among which, the magnitude-based pruning results in the most similar mask with ours. Consequently, our searched lottery jackpot removes 90\% weights in ResNet-50, while it easily obtains more than 70\% top-1 accuracy using only 5 searching epochs on ImageNet. In compliance with this insight, we initialize our sparse mask using the magnitude-based pruning, resulting in at least 3x cost reduction on the lottery jackpot searching while achieving comparable or even better performance. Secondly, we conduct an in-depth analysis of the searching process for lottery jackpots. Our theoretical result suggests that the decrease in training loss during weight searching can be disturbed by the dependency between weights in modern networks. To mitigate this, we propose a novel short restriction method to restrict change of masks that may have potential negative impacts on the training loss. Our code is available at https://github.com/zyxxmu/lottery-jackpots.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Mask optimiztion,Pruning,Stochastic Pruning},
  file = {/home/luisaam/Zotero/storage/J2I7Y85W/Zhang et al. - 2022 - Lottery Jackpots Exist in Pre-trained Models.pdf;/home/luisaam/Zotero/storage/7XL5LFDL/2104.html}
}

@article{zhangOneShotPruningRecurrent2020,
  title = {One-{{Shot Pruning}} of {{Recurrent Neural Networks}} by {{Jacobian Spectrum Evaluation}}},
  author = {Zhang, Matthew Shunshi and Stadie, Bradly C.},
  year = {2020},
  journal = {ICLR},
  abstract = {A new recurrent pruning objective derived from the spectrum of the recurrent Jacobian is introduced, which is data efficient, easy to implement, and produces 95\% sparse GRUs that significantly improve on existing baselines. Recent advances in the sparse neural network literature have made it possible to prune many large feed forward and convolutional networks with only a small quantity of data. Yet, these same techniques often falter when applied to the problem of recovering sparse recurrent networks. These failures are quantitative: when pruned with recent techniques, RNNs typically obtain worse performance than they do under a simple random pruning scheme. The failures are also qualitative: the distribution of active weights in a pruned LSTM or GRU network tend to be concentrated in specific neurons and gates, and not well dispersed across the entire architecture. We seek to rectify both the quantitative and qualitative issues with recurrent network pruning by introducing a new recurrent pruning objective derived from the spectrum of the recurrent Jacobian. Our objective is data efficient (requiring only 64 data points to prune the network), easy to implement, and produces 95\% sparse GRUs that significantly improve on existing baselines. We evaluate on sequential MNIST, Billion Words, and Wikitext.},
  keywords = {Jacobian regularisation,Jacobian spectrum evaluation,Pruning},
  file = {/home/luisaam/Zotero/storage/CCKAPK9S/Zhang y Stadie - 2020 - One-Shot Pruning of Recurrent Neural Networks by J.pdf}
}

@article{zhangUnderstandingDeepLearning2017,
  title = {Understanding Deep Learning Requires Rethinking Generalization},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  year = {2017},
  month = feb,
  journal = {arXiv:1611.03530 [cs]},
  eprint = {1611.03530},
  primaryclass = {cs},
  urldate = {2021-06-19},
  abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,generalization},
  file = {/home/luisaam/Zotero/storage/ZN8HV7F2/Zhang et al. - 2017 - Understanding deep learning requires rethinking ge.pdf;/home/luisaam/Zotero/storage/UNVYWX9E/1611.html}
}

@inproceedings{zhouDeconstructingLotteryTickets2019,
  title = {Deconstructing {{Lottery Tickets}}: {{Zeros}}, {{Signs}}, and the {{Supermask}}},
  shorttitle = {Deconstructing {{Lottery Tickets}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhou, Hattie and Lan, Janice and Liu, Rosanne and Yosinski, Jason},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2021-08-26},
  keywords = {mask training,Overparametrization,small networks,subnetwork structure},
  file = {/home/luisaam/Zotero/storage/PZI6LADV/Zhou et al. - 2019 - Deconstructing Lottery Tickets Zeros, Signs, and .pdf}
}

@misc{zhouObjectDetectorsEmerge2015,
  title = {Object {{Detectors Emerge}} in {{Deep Scene CNNs}}},
  author = {Zhou, Bolei and Khosla, Aditya and Lapedriza, Agata and Oliva, Aude and Torralba, Antonio},
  year = {2015},
  month = apr,
  number = {arXiv:1412.6856},
  eprint = {1412.6856},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1412.6856},
  urldate = {2023-11-20},
  abstract = {With the success of new computational architectures for visual processing, such as convolutional neural networks (CNN) and access to image databases with millions of labeled examples (e.g., ImageNet, Places), the state of the art in computer vision is advancing rapidly. One important factor for continued progress is to understand the representations that are learned by the inner layers of these deep architectures. Here we show that object detectors emerge from training CNNs to perform scene classification. As scenes are composed of objects, the CNN for scene classification automatically discovers meaningful objects detectors, representative of the learned scene categories. With object detectors emerging as a result of learning to recognize scenes, our work demonstrates that the same network can perform both scene recognition and object localization in a single forward-pass, without ever having been explicitly taught the notion of objects.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing,NN feautres,Receptive Field},
  file = {/home/luisaam/Zotero/storage/5DIV5NYL/Zhou et al. - 2015 - Object Detectors Emerge in Deep Scene CNNs.pdf;/home/luisaam/Zotero/storage/HR6D74XF/1412.html}
}

@inproceedings{zhouSGDConvergesGlobal2018,
  title = {{{SGD Converges}} to {{Global Minimum}} in {{Deep Learning}} via {{Star-convex Path}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Zhou, Yi and Yang, Junjie and Zhang, Huishuai and Liang, Yingbin and Tarokh, Vahid},
  year = {2018},
  month = sep,
  urldate = {2021-06-28},
  abstract = {Stochastic gradient descent (SGD) has been found to be surprisingly effective in training a variety of deep neural networks. However, there is still a lack of understanding on how and why SGD can...},
  langid = {english},
  keywords = {Why no global second order},
  file = {/home/luisaam/Zotero/storage/GHLYUIU8/Zhou et al. - 2018 - SGD Converges to Global Minimum in Deep Learning v.pdf;/home/luisaam/Zotero/storage/X9SV5QNN/forum.html}
}

@inproceedings{zhouUnderstandingImportanceNoise2019,
  title = {Toward {{Understanding}} the {{Importance}} of {{Noise}} in {{Training Neural Networks}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Zhou, Mo and Liu, Tianyi and Li, Yan and Lin, Dachao and Zhou, Enlu and Zhao, Tuo},
  year = {2019},
  month = may,
  pages = {7594--7602},
  publisher = {{PMLR}},
  issn = {2640-3498},
  urldate = {2023-08-11},
  abstract = {Numerous empirical evidence has corroborated that the noise plays a crucial rule in effective and efficient training of deep neural networks. The theory behind, however, is still largely unknown. This paper studies this fundamental problem through training a simple two-layer convolutional neural network model. Although training such a network requires to solve a non-convex optimization problem with a spurious local optimum and a global optimum, we prove that a perturbed gradient descent algorithm in conjunction with noise annealing is guaranteed to converge to a global optimum in polynomial time with arbitrary initialization. This implies that the noise enables the algorithm to efficiently escape from the spurious local optimum. Numerical experiments are provided to support our theory.},
  langid = {english},
  keywords = {Noise,Pruning,Regularisation},
  file = {/home/luisaam/Zotero/storage/DT8FEDGG/Zhou et al. - 2019 - Toward Understanding the Importance of Noise in Tr.pdf;/home/luisaam/Zotero/storage/F6S5PNI9/Zhou et al. - 2019 - Toward Understanding the Importance of Noise in Tr.pdf}
}

@inproceedings{zhuangDiscriminationawareChannelPruning2018,
  title = {Discrimination-Aware {{Channel Pruning}} for {{Deep Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhuang, Zhuangwei and Tan, Mingkui and Zhuang, Bohan and Liu, Jing and Guo, Yong and Wu, Qingyao and Huang, Junzhou and Zhu, Jinhui},
  year = {2018},
  volume = {31},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-03-23},
  abstract = {Channel pruning is one of the predominant approaches for deep model compression. Existing pruning methods either train from scratch with sparsity constraints on channels, or  minimize the reconstruction error between the pre-trained feature maps and the compressed ones. Both strategies suffer from some limitations: the former kind is computationally expensive and difficult to converge, whilst the latter kind optimizes the reconstruction error but ignores the discriminative power of channels. To overcome these drawbacks, we investigate a simple-yet-effective method, called discrimination-aware channel pruning, to choose those channels that really contribute to discriminative power. To this end, we introduce additional losses into the network to increase the discriminative power of intermediate layers and then select the most discriminative channels for each layer by considering the additional loss and the reconstruction error. Last, we propose a greedy algorithm to conduct channel selection and parameter optimization in an iterative way. Extensive experiments demonstrate the effectiveness of our method. For example, on ILSVRC-12, our pruned ResNet-50 with 30\% reduction of channels even outperforms the original model by 0.39\% in top-1 accuracy.},
  keywords = {Channel pruning,Pruning},
  file = {/home/luisaam/Zotero/storage/KWZPFKHJ/Zhuang et al. - 2018 - Discrimination-aware Channel Pruning for Deep Neur.pdf}
}

@inproceedings{zhuGradInitLearningInitialize2021,
  title = {{{GradInit}}: {{Learning}} to {{Initialize Neural Networks}} for {{Stable}} and {{Efficient Training}}},
  shorttitle = {{{GradInit}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhu, Chen and Ni, Renkun and Xu, Zheng and Kong, Kezhi and Huang, W. Ronny and Goldstein, Tom},
  year = {2021},
  volume = {34},
  pages = {16410--16422},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-03-31},
  abstract = {Innovations in neural architectures have fostered significant breakthroughs in language modeling and computer vision. Unfortunately, novel architectures often result in challenging hyper-parameter choices and training instability if the network parameters are not properly initialized. A number of architecture-specific initialization schemes have been proposed, but these schemes are not always portable to new architectures. This paper presents GradInit, an automated and architecture agnostic method for initializing neural networks. GradInit is based on a simple heuristic; the norm of each network layer is adjusted so that a single step of SGD or Adam with prescribed hyperparameters results in the smallest possible loss value. This adjustment is done by introducing a scalar multiplier variable in front of each parameter block, and then optimizing these variables using a simple numerical scheme. GradInit accelerates the convergence and test performance of many convolutional architectures, both with or without skip connections, and even without normalization layers. It also improves the stability of the original Transformer architecture for machine translation, enabling training it without learning rate warmup using either Adam or SGD under a wide range of learning rates and momentum coefficients. Code is available at https://github.com/zhuchen03/gradinit.},
  keywords = {Initialisation},
  file = {/home/luisaam/Zotero/storage/2GRAKRT4/Zhu et al. - 2021 - GradInit Learning to Initialize Neural Networks f.pdf}
}

@inproceedings{zhuGradInitLearningInitialize2021a,
  title = {{{GradInit}}: {{Learning}} to {{Initialize Neural Networks}} for {{Stable}} and {{Efficient Training}}},
  shorttitle = {{{GradInit}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zhu, Chen and Ni, Renkun and Xu, Zheng and Kong, Kezhi and Huang, W. Ronny and Goldstein, Tom},
  year = {2021},
  volume = {34},
  pages = {16410--16422},
  publisher = {{Curran Associates, Inc.}},
  urldate = {2023-03-31},
  abstract = {Innovations in neural architectures have fostered significant breakthroughs in language modeling and computer vision. Unfortunately, novel architectures often result in challenging hyper-parameter choices and training instability if the network parameters are not properly initialized. A number of architecture-specific initialization schemes have been proposed, but these schemes are not always portable to new architectures. This paper presents GradInit, an automated and architecture agnostic method for initializing neural networks. GradInit is based on a simple heuristic; the norm of each network layer is adjusted so that a single step of SGD or Adam with prescribed hyperparameters results in the smallest possible loss value. This adjustment is done by introducing a scalar multiplier variable in front of each parameter block, and then optimizing these variables using a simple numerical scheme. GradInit accelerates the convergence and test performance of many convolutional architectures, both with or without skip connections, and even without normalization layers. It also improves the stability of the original Transformer architecture for machine translation, enabling training it without learning rate warmup using either Adam or SGD under a wide range of learning rates and momentum coefficients. Code is available at https://github.com/zhuchen03/gradinit.},
  file = {/home/luisaam/Zotero/storage/JCJSYGVY/Zhu et al. - 2021 - GradInit Learning to Initialize Neural Networks f.pdf}
}

@article{zhuVisualTransformerPruning2021,
  title = {Visual {{Transformer Pruning}}},
  author = {Zhu, Mingjian and Han, Kai and Tang, Yehui and Wang, Yunhe},
  year = {2021},
  month = jul,
  journal = {arXiv:2104.08500 [cs]},
  eprint = {2104.08500},
  primaryclass = {cs},
  urldate = {2021-07-30},
  abstract = {Vision transformer has achieved competitive performance on a variety of computer vision applications. However, their storage, run-time memory, and computational demands are hindering the deployment to mobile devices. Here we present a vision transformer pruning approach, which identifies the impacts of dimensions in each layer of transformer and then executes pruning accordingly. By encouraging dimension-wise sparsity in the transformer, important dimensions automatically emerge. A great number of dimensions with small importance scores can be discarded to achieve a high pruning ratio without significantly compromising accuracy. The pipeline for vision transformer pruning is as follows: 1) training with sparsity regularization; 2) pruning dimensions of linear projections; 3) fine-tuning. The reduced parameters and FLOPs ratios of the proposed algorithm are well evaluated and analyzed on ImageNet dataset to demonstrate the effectiveness of our proposed method.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,small networks},
  file = {/home/luisaam/Zotero/storage/7366CGJJ/Zhu et al. - 2021 - Visual Transformer Pruning.pdf;/home/luisaam/Zotero/storage/C2T32DIK/2104.html}
}

@article{ziyinSGDMayNever2021,
  title = {{{SGD May Never Escape Saddle Points}}},
  author = {Ziyin, Liu and Li, Botao and Simon, James B. and Ueda, Masahito},
  year = {2021},
  month = sep,
  journal = {arXiv:2107.11774 [cs, math, stat]},
  eprint = {2107.11774},
  primaryclass = {cs, math, stat},
  urldate = {2021-12-30},
  abstract = {Stochastic gradient descent (SGD) has been deployed to solve highly non-linear and non-convex machine learning problems such as the training of deep neural networks. However, previous works on SGD often rely on restrictive and unrealistic assumptions about the nature of noise in SGD. In this work, we mathematically construct examples that defy previous understandings of SGD. For example, our constructions show that: (1) SGD may converge to a local maximum; (2) SGD may escape a saddle point arbitrarily slowly; (3) SGD may prefer sharp minima over the flat ones; and (4) AMSGrad may converge to a local maximum. We also show the relevance of our result to deep learning by presenting a minimal neural network example. Our result suggests that the noise structure of SGD might be more important than the loss landscape in neural network training and that future research should focus on deriving the actual noise structure in deep learning.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Saddle Points,Statistics - Machine Learning,to read},
  file = {/home/luisaam/Zotero/storage/J5BJT9K5/Ziyin et al. - 2021 - SGD May Never Escape Saddle Points.pdf;/home/luisaam/Zotero/storage/CESXUPEW/2107.html}
}

@misc{zotero-1243,
  urldate = {2023-07-20},
  howpublished = {https://cka-similarity.github.io/},
  file = {/home/luisaam/Zotero/storage/V3VZHCB3/cka-similarity.github.io.html}
}

@article{zouDatadrivenEffectiveModel2021,
  title = {Data-Driven Effective Model Shows a Liquid-like Deep Learning},
  author = {Zou, Wenxuan and Huang, Haiping},
  year = {2021},
  month = sep,
  journal = {Physical Review Research},
  volume = {3},
  number = {3},
  pages = {033290},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevResearch.3.033290},
  urldate = {2022-03-04},
  abstract = {The geometric structure of an optimization landscape is argued to be fundamentally important to support the success of deep neural network learning. A direct computation of the landscape beyond two layers is hard. Therefore, to capture the global view of the landscape, an interpretable model of the network-parameter (or weight) space must be established. However, the model is lacking so far. Furthermore, it remains unknown what the landscape looks like for deep networks of binary synapses, which plays a key role in robust and energy efficient neuromorphic computation. Here, we propose a statistical mechanics framework by directly building a least structured model of the high-dimensional weight space, considering realistic structured data, stochastic gradient descent training, and the computational depth of neural networks. We also consider whether the number of network parameters outnumbers the number of supplied training data, namely, over- or under-parametrization. Our least structured model reveals that the weight spaces of the under-parametrization and over-parameterization cases belong to the same class, in the sense that these weight spaces are well connected without any hierarchical clustering structure. In contrast, the shallow-network has a broken weight space, characterized by a discontinuous phase transition, thereby clarifying the benefit of depth in deep learning from the angle of high-dimensional geometry. Our effective model also reveals that inside a deep network, there exists a liquid-like central part of the architecture in the sense that the weights in this part behave as randomly as possible, providing algorithmic implications. Our data-driven model thus provides a statistical mechanics insight about why deep learning is unreasonably effective in terms of the high-dimensional weight space, and how deep networks are different from shallow ones.},
  file = {/home/luisaam/Zotero/storage/6GW5PYAZ/Zou y Huang - 2021 - Data-driven effective model shows a liquid-like de.pdf;/home/luisaam/Zotero/storage/BVW7FT2S/PhysRevResearch.3.html}
}
